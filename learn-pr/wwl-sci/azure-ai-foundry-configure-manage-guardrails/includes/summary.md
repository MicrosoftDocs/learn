As AI adoption grows, balancing innovation with safety becomes a central challenge. Without consistent guardrails, prompts could expose sensitive data or generate inappropriate responses that undermine trust and compliance.

You learned to:

- Explain how Azure AI Foundry guardrails protect against unsafe or policy-violating interactions
- Describe how safety controls such as content filters, blocklists, and Prompt Shields work together
- Configure and validate guardrails for both input and output protection
- Review observability data to refine configurations and maintain continuous assurance

By applying guardrails that align with organizational policies and reviewing their performance over time, you have learned how to create a safe, adaptable foundation for AI workloads. These practices help ensure every deployment operates securely, transparently, and in line with your organizationâ€™s risk posture.

## Resources

- [Trustworthy AI for Azure AI Foundry](/azure/ai-foundry/responsible-use-of-ai-overview?azure-portal=true)
- [Content Safety in the Azure AI Foundry portal](/azure/ai-foundry/ai-services/content-safety-overview?azure-portal=true)
- [Guardrails & controls for Models Sold Directly by Azure](/azure/ai-foundry/concepts/model-catalog-content-safety?azure-portal=true)
- [Content filtering in Azure AI Foundry portal](/azure/ai-foundry/concepts/content-filtering?azure-portal=true)
- [How to use blocklists with Foundry Models in Azure AI Foundry services](/azure/ai-foundry/foundry-models/how-to/use-blocklist?azure-portal=true)
