{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup chunk to install and load required packages\n",
        "knitr::opts_chunk$set(warning = FALSE, message = FALSE)\n",
        "suppressWarnings(if(!require(\"pacman\")) install.packages(\"pacman\"))\n",
        "\n",
        "pacman::p_load('tidyverse', 'tidymodels', 'glmnet',\n",
        "               'randomForest', 'xgboost','patchwork',\n",
        "               'paletteer', 'here', 'doParallel', 'summarytools')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regression - Optimize and save models\n",
        "\n",
        "In the previous notebook, you used complex regression models to look at the relationship between features of a bike rentals dataset. In this notebook, you'll see if you can improve the performance of these models even further.\n",
        "\n",
        "Let's load the bicycle-sharing data as a tibble and view the first few rows. You'll also split the data into training and test datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the required packages and make them available in your current R session\n",
        "suppressPackageStartupMessages({\n",
        "  library(tidyverse)\n",
        "  library(tidymodels)\n",
        "  library(lubridate)\n",
        "  library(paletteer)\n",
        "})\n",
        "\n",
        "# Import the data into the R session\n",
        "bike_data <- read_csv(file = \"https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/data/daily-bike-share.csv\", show_col_types = FALSE)\n",
        "\n",
        "# Parse dates then extract days\n",
        "bike_data <- bike_data %>%\n",
        "  mutate(dteday = mdy(dteday)) %>% \n",
        "  mutate(day = day(dteday))\n",
        "\n",
        "# Select desired features and labels\n",
        "bike_select <- bike_data %>% \n",
        "  select(c(season, mnth, holiday, weekday, workingday, weathersit,\n",
        "           temp, atemp, hum, windspeed, rentals)) %>% \n",
        "  mutate(across(1:6, factor))\n",
        "\n",
        "# Split 70% of the data for training and the rest for testing\n",
        "set.seed(2056)\n",
        "bike_split <- bike_select %>% \n",
        "  initial_split(prop = 0.7,\n",
        "  # splitting data evenly on the holiday variable\n",
        "                strata = holiday)\n",
        "\n",
        "# Extract the data in each split\n",
        "bike_train <- training(bike_split)\n",
        "bike_test <- testing(bike_split)\n",
        "\n",
        "# Specify multiple regression metrics\n",
        "eval_metrics <- metric_set(rmse, rsq)\n",
        "\n",
        "\n",
        "cat(\"Training Set\", nrow(bike_train), \"rows\",\n",
        "    \"\\nTest Set\", nrow(bike_test), \"rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result is two datasets:\n",
        "\n",
        "-   **bike_train**: A subset of the dataset used to train the model.\n",
        "-   **bike_test**: A subset of the dataset used to validate the model.\n",
        "\n",
        "Now you're ready to train a model by fitting a boosting ensemble algorithm, as in the last notebook. Recall that a gradient boosting estimator is like a random forest algorithm. Instead of building all the trees independently and taking the average result, each tree is built on the outputs of the previous one. This technique is an attempt to incrementally reduce the loss (error) in the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For reproducibility\n",
        "set.seed(2056)\n",
        "\n",
        "# Build an xgboost model specification\n",
        "boost_spec <- boost_tree() %>% \n",
        "  set_engine('xgboost') %>% \n",
        "  set_mode('regression')\n",
        "\n",
        "# Train an xgboost model \n",
        "boost_mod <- boost_spec %>% \n",
        "  fit(rentals ~ ., data = bike_train)\n",
        "\n",
        "# Make and bind predictions to test data a\n",
        "results <- boost_mod %>% \n",
        "  augment(new_data = bike_test) %>% \n",
        "  rename(predictions = .pred)\n",
        "\n",
        "# Evaluate the model\n",
        "boost_metrics <- eval_metrics(data = results,\n",
        "                                  truth = rentals,\n",
        "                                  estimate = predictions) \n",
        "\n",
        "# Plot predicted vs actual\n",
        "boost_plt <- results %>% \n",
        "  ggplot(mapping = aes(x = rentals, y = predictions)) +\n",
        "  geom_point(color = '#4D3161FF') +\n",
        "  # overlay regression line\n",
        "  geom_smooth(method = 'lm', color = 'black', se = F) +\n",
        "  ggtitle(\"Daily Bike Share Predictions\") +\n",
        "  xlab(\"Actual Labels\") +\n",
        "  ylab(\"Predicted Labels\") +\n",
        "  theme(plot.title = element_text(hjust = 0.5))\n",
        "\n",
        "# Return evaluations\n",
        "list(boost_metrics, boost_plt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocess the data by using recipes\n",
        "\n",
        "You trained a model with data that was loaded straight from a source file, with only moderately successful results. In practice, it's common to perform some preprocessing of the data to make it easier for the algorithm to fit a model to it.\n",
        "\n",
        "In this section, you'll explore the Tidymodels package [recipes](https://tidymodels.github.io/recipes/). This package helps you preprocess your data before you train your model. A recipe is an object that defines a series of steps for data processing.\n",
        "\n",
        "There are many preprocessing transformations you can perform to get your data ready for modeling. Let's limit the options to a few common techniques.\n",
        "\n",
        "#### Scale numeric features\n",
        "\n",
        "Numeric features need to be normalized so that they're on the same scale. Normalizing prevents features with large values from producing coefficients that disproportionately affect the predictions. For example, suppose your data includes the following numeric features:\n",
        "\n",
        "|  A  |  B  |  C  |\n",
        "|:---:|:---:|:---:|\n",
        "|  3  | 480 | 65  |\n",
        "\n",
        "Normalizing these features to the same scale might result in the following values. You're assuming that A contains values from 0 to 10, B contains values from 0 to 1,000, and C contains values from 0 to 100:\n",
        "\n",
        "|  A  |  B   |  C   |\n",
        "|:---:|:----:|:----:|\n",
        "| 0.3 | 0.48 | 0.65 |\n",
        "\n",
        "There are multiple ways you can scale numeric data. You can calculate the minimum and maximum values for each column and assign a proportional value between 0 and 1. You can also use the mean and standard deviation of a normally distributed variable to maintain the same spread of values on a different scale.\n",
        "\n",
        "#### Encode categorical variables\n",
        "\n",
        "Encoding involves translating a column with categorical values into one or more numeric columns that take the place of the original.\n",
        "\n",
        "Machine learning models work best with numeric features rather than text values. Generally, you need to convert categorical features into numeric representations. For example, suppose your data includes the following categorical feature:\n",
        "\n",
        "| Size |\n",
        "|:----:|\n",
        "|  S   |\n",
        "|  M   |\n",
        "|  L   |\n",
        "\n",
        "You can apply ordinal encoding to substitute a unique integer value for each category, as shown:\n",
        "\n",
        "| Size |\n",
        "|:----:|\n",
        "|  0   |\n",
        "|  1   |\n",
        "|  2   |\n",
        "\n",
        "Another common technique is to create *dummy* or *indicator variables*. They replace the original categorical feature with numeric columns whose values are either 1 or 0, as shown:\n",
        "\n",
        "| Raw data |  M  |  L  |\n",
        "|:--------:|:---:|:---:|\n",
        "|    S     |  0  |  0  |\n",
        "|    M     |  1  |  0  |\n",
        "|    L     |  0  |  1  |\n",
        "\n",
        "In R, the convention is to exclude a column for the first factor level. In this case, the factor is *S*. The reasons for this include *simplicity* and reducing *linear dependencies*. The full set of encodings can also be used for some models. This technique is traditionally called *one-hot* encoding and can be achieved by using the `one_hot` argument of `step_dummy()`.\n",
        "\n",
        "> Tree-based models created by using the XGBoost engine typically require one to create dummy variables.\n",
        "\n",
        "Now, let's create some recipes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify a recipe\n",
        "bike_recipe <- recipe(rentals ~ ., data = bike_train) %>% \n",
        "  step_normalize(all_numeric_predictors()) %>% \n",
        "  step_dummy(all_nominal_predictors()) \n",
        "\n",
        "# Print out recipe\n",
        "bike_recipe\n",
        "\n",
        "\n",
        "# Summary of the recipe\n",
        "summary(bike_recipe)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You just created your first recipe that contains an outcome and its corresponding predictors. The numeric predictors are normalized. The nominal predictors are converted to a quantitative format. Let's quickly explain:\n",
        "\n",
        "-   The call to `recipe()` with a formula tells the recipe the roles of the variables. Examples are predictor and outcome. The `bike_train` data is used as the reference. See the results of `summary(bike_recipe)`.\n",
        "-   The `step_normalize(all_numeric_predictors())` function specifies that all numeric predictors should be normalized.\n",
        "-   The `step_dummy(all_nominal_predictors())` function specifies that all predictors that are currently factor or character should be converted to a quantitative format (1s/0s).\n",
        "\n",
        "Now that you have a recipe, the next step is to create a model specification. In this case, let's re-create an `xgboost` model specification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# xgboost specification\n",
        "boost_spec <- boost_tree() %>% \n",
        "  set_engine('xgboost') %>% \n",
        "  set_mode('regression')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But how do you combine this model specification with the data preprocessing steps from your recipe?\n",
        "\n",
        "You'll use modeling workflows, which are known as pipelines in Python.\n",
        "\n",
        "### Bundle it all together by using a workflow\n",
        "\n",
        "The [workflows](https://workflows.tidymodels.org/) package allows you to bind modeling and preprocessing objects together. You can then fit the entire workflow to the data so that the model encapsulates all the preprocessing steps and the algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the workflow\n",
        "boost_workflow <- workflow() %>% \n",
        "  add_recipe(bike_recipe) %>% \n",
        "  add_model(boost_spec)\n",
        "\n",
        "# Print out workflow\n",
        "boost_workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The workflow object provides an informative summary of the preprocessing steps that will be done by the recipe and the model specification. Also, a `workflow()` can be fit in much the same way a model can.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "boost_workflow <- boost_workflow %>% \n",
        "  fit(data = bike_train)\n",
        "\n",
        "boost_workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that you have your fitted workflow, how do you make some predictions? You can use `predict()` on a workflow in the same way as you used it on a model.\n",
        "\n",
        "Let's make some predictions on the first six observations of the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "boost_workflow %>% \n",
        "  predict(new_data = bike_test %>% dplyr::slice(1:6))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Workflows are convenient!\n",
        "\n",
        "Why haven't we made predictions on the whole test set, evaluated performance, and created some graphs? We'll get to that. First, let's address a boosted tree's hyperparameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args(boost_tree)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Those are a lot of model arguments: `mtry`, `trees`, `min_n`, `tree_depth`, `learn_rate`, `loss_reduction`, `sample_size`, and `stop_iter`.\n",
        "\n",
        "How do you know what values you should use?\n",
        "\n",
        "That's where model tuning comes in.\n",
        "\n",
        "### Tune model hyperparameters\n",
        "\n",
        "Models have parameters with unknown values that must be estimated to use the model for predicting. These model parameters, known as hyperparameters or tuning parameters, can't be learned directly from a dataset during model training. You can estimate the best values by training many models on a simulated dataset. Then you measure how well all these models perform. This process is called tuning.\n",
        "\n",
        "We won't go into the details of each hyperparameter. But they work together to affect the way the algorithm trains a model. For instance, in boosted trees:\n",
        "\n",
        "-   The `min_n` argument forces the tree to discard any node that has a number of observations below your specified minimum.\n",
        "-   Tuning the value of `mtry` controls the number of variables that will be used at each split of a decision tree.\n",
        "-   Tuning `tree_depth` helps by restricting your tree from growing after it reaches a certain level.\n",
        "\n",
        "In many cases, the default values provided by Tidymodels will work well. See the defaults by running `help(\"boost_tree\")`. There might be some advantages in modifying hyperparameters to get better predictive performance or reduce training time.\n",
        "\n",
        "How do you know what hyperparameter values you should use? Without a deep understanding of how the underlying algorithm works, you'll need to experiment. Fortunately, Tidymodels provides a way to tune hyperparameters by trying multiple combinations and finding the best result for a given performance metric.\n",
        "\n",
        "#### Identify tuning parameters\n",
        "\n",
        "How can you signal to Tidymodels functions which arguments, such as `cost_complexity`, `tree_depth`, and `min_n`, should be optimized? Parameters are marked for tuning by assigning them a value of `tune()`.\n",
        "\n",
        "Let's build the model specification with some tuning. Then you'll put your recipe and model specification together in a workflow for ease of use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify a recipe\n",
        "bike_recipe <- recipe(rentals ~ ., data = bike_train) %>% \n",
        "  step_normalize(all_numeric_predictors()) %>% \n",
        "  step_dummy(all_nominal_predictors()) \n",
        "\n",
        "\n",
        "# Make a tunable model specification\n",
        "boost_spec <- boost_tree(trees = 50,\n",
        "                         tree_depth = tune(),\n",
        "                         learn_rate = tune()) %>% \n",
        "  set_engine('xgboost') %>% \n",
        "  set_mode('regression')\n",
        "\n",
        "\n",
        "# Bundle a recipe and model spec using a workflow\n",
        "boost_workflow <- workflow() %>% \n",
        "  add_recipe(bike_recipe) %>% \n",
        "  add_model(boost_spec)\n",
        "\n",
        "# Print workflow\n",
        "boost_workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create a tuning grid\n",
        "\n",
        "Now that you've specified what parameter to tune, you need to figure out a set of possible values to try out. Then you'll choose the best.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a grid of tuning parameters\n",
        "tree_grid <- grid_regular(tree_depth(),\n",
        "                  # You can specify hyperparameter ranges too\n",
        "learn_rate(range = c(0.01, 0.3), trans = NULL), levels = 5)\n",
        "\n",
        "# Display hyperparameter combinations that will be used for tuning\n",
        "tree_grid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function [`grid_regular()`](https://tidymodels.github.io/dials/reference/grid_regular.html) chooses sensible values to try for each hyperparameter. Here, you asked for five of each. Because you have two hyperparameters to tune, `grid_regular()` returns 5Ã—5 = 25 different possible tuning combinations to try in a tidy tibble format.\n",
        "\n",
        "#### Sample the data\n",
        "\n",
        "As we pointed out earlier, hyperparameters can't be learned directly from the training set. Instead, they're estimated by using simulated datasets created from a process called resampling. One resampling approach is cross-validation.\n",
        "\n",
        "Cross-validation involves taking your training set and randomly dividing it up evenly into *V* subsets or folds. You then use one of the folds for validation and the rest for training. You repeat these steps with all the subsets and combine the results, usually by taking the mean. This process is just one round of cross-validation. To obtain better results, sometimes data scientists cross-validate more than once, perhaps five times.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set.seed(2056)\n",
        "# 5 fold CV repeated once\n",
        "bike_folds <- vfold_cv(data = bike_train, v = 5, repeats = 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Time to tune\n",
        "\n",
        "Now it's time to tune the grid to find out which hyperparameter combination results in the best performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Allow parallel processing\n",
        "doParallel::registerDoParallel()\n",
        "\n",
        "# Model tuning via grid search\n",
        "set.seed(2020)\n",
        "tree_grid <- tune_grid(\n",
        "  object = boost_workflow,\n",
        "  resamples = bike_folds,\n",
        "  grid = tree_grid\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualize tuning results\n",
        "\n",
        "You have trained models for many possible hyperparameter values, so let's explore the results.\n",
        "\n",
        "First, you'll use the function `collect_metrics()` to extract the performance metrics from the tuning results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the results\n",
        "tree_grid %>% \n",
        "  collect_metrics() %>% \n",
        "  mutate(tree_depth = factor(tree_depth)) %>% \n",
        "  ggplot(mapping = aes(x = learn_rate, y = mean,\n",
        "                       color = tree_depth)) +\n",
        "  geom_line(size = 0.6) +\n",
        "  geom_point(size = 2) +\n",
        "  facet_wrap(~ .metric, scales = 'free', nrow = 2)+\n",
        "  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our stubbiest tree, with a depth of 1, is the worst model according to both metrics, rmse and rsq, and across all candidate values of `learn_rate`. A tree depth of 4 and a learn_rate of around 0.1 seem to be best. Let's investigate these tuning parameters further. You can use `show_best()` to display the top sub-models and their performance estimates.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tree_grid %>% \n",
        "  show_best('rmse')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can then use `select_best()` to find the tuning parameter combination with the best performance values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select the tree with the best RMSE\n",
        "best_tree <- tree_grid %>% \n",
        "  select_best('rmse')\n",
        "\n",
        "# Display best tree\n",
        "best_tree\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Finalize your model\n",
        "\n",
        "Now that you have the best performance values, you can use `tune::finalize_workflow()` to update, or finalize, your workflow object with the best estimate values for tree_depth and learn_rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update workflow\n",
        "final_wf <- boost_workflow %>% \n",
        "  finalize_workflow(best_tree)\n",
        "\n",
        "# Print final workflow\n",
        "final_wf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our tuning is done. You've updated your workflow with the best estimated hyperparameter values.\n",
        "\n",
        "#### The last fit: Back to the test set\n",
        "\n",
        "Let's return to the test data and estimate the model performance you expect to see with new data. You can use the function [`last_fit()`](https://tidymodels.github.io/tune/reference/last_fit.html) with your finalized model. This function fits the finalized model on the full training dataset and evaluates the finalized model on the testing data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make a last fit\n",
        "final_fit <- final_wf %>% \n",
        "  last_fit(bike_split)\n",
        "\n",
        "\n",
        "# Collect metrics\n",
        "final_fit %>% \n",
        "  collect_metrics()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How's that for a tune? There seems to be some improvement in the evaluation metrics compared to using the default values for learn_rate and tree_depth hyperparameters. Now we leave it to you to explore how tuning the other hyperparameters affects the model performance.\n",
        "\n",
        "You've seen a number of common techniques used to train predictive models for regression. In a real project, you would likely try a few more algorithms, hyperparameters, and preprocessing transformations. By now you should have  the general idea of the procedure to follow. You can always explore the [reference docs](https://www.tidymodels.org/find/parsnip/#models) or use the `args()` function to see which parsnip object arguments are available.\n",
        "\n",
        "Let's now explore how you can use the trained model with new data.\n",
        "\n",
        "### Use the trained model\n",
        "\n",
        "Before you save your model, let's extract the trained workflow object from the `final_fit` object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract trained workflow\n",
        "bike_boost_model <- final_fit$.workflow[[1]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can save this model to be used later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save trained workflow\n",
        "saveRDS(bike_boost_model, 'bike_boost_model.rds')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can load it whenever you need it and use it to predict labels for new data. This technique is often called *scoring* or *inferencing*.\n",
        "\n",
        "For example, let's try and predict some values from the test set by using the saved model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model\n",
        "loaded_model <- readRDS(\"bike_boost_model.rds\")\n",
        "\n",
        "# Extract predictors\n",
        "bike_new <- bike_test %>% \n",
        "  dplyr::slice(5:9)\n",
        "\n",
        "# Use the model to predict rentals\n",
        "results <- loaded_model %>% \n",
        "  augment(bike_test)\n",
        "\n",
        "# See model predictions \n",
        "results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All is well that ends with a working model.\n",
        "\n",
        "### Summary\n",
        "\n",
        "We've concluded the notebooks for this module on regression. In this notebook, you ran a complex regression, tuned it, saved the model, and used it to predict outcomes for the future.\n",
        "\n",
        "### Further reading\n",
        "\n",
        "To learn more about Tidymodels, see the [Tidymodels documentation](https://www.tidymodels.org/).\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": "",
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.4.1"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
