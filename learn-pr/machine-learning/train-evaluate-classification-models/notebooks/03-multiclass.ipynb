{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multiclass Classification\n",
    "\n",
    "In the last notebook, we looked at binary classification. This  works well when the data observations belong to one of two classes or categories, such as \"True\" or \"False\". When the data can be categorized into more than two classes, you must use a multiclass classification algorithm.\n",
    "\n",
    "Multiclass classification can be thought of as a combination of multiple binary classifiers. There are two ways in which you approach the problem:\n",
    "\n",
    "- **One vs Rest (OVR)**, in which a classifier is created for each possible class value, with a positive outcome for cases where the prediction is *this* class, and negative predictions for cases where the prediction is any other class. A classification problem with four possible shape classes (*square*, *circle*, *triangle*, *hexagon*) would require four classifiers that predict:\n",
    "    - *square* or not\n",
    "    - *circle* or not\n",
    "    - *triangle* or not\n",
    "    - *hexagon* or not\n",
    "    \n",
    "- **One vs One (OVO)**, in which a classifier for each possible pair of classes is created. The classification problem with four shape classes would require the following binary classifiers:\n",
    "    - *square* or *circle*\n",
    "    - *square* or *triangle*\n",
    "    - *square* or *hexagon*\n",
    "    - *circle* or *triangle*\n",
    "    - *circle* or *hexagon*\n",
    "    - *triangle* or *hexagon*\n",
    "\n",
    "In both approaches, the overall model that combines the classifiers generates a vector of predictions in which the probabilities generated from the individual binary classifiers are used to determine which class to predict.\n",
    "\n",
    "Fortunately, in most machine learning frameworks, including scikit-learn, implementing a multiclass classification model is not significantly more complex than binary classification - and in most cases, the estimators used for binary classification implicitly support multiclass classification by abstracting an OVR algorithm, an OVO algorithm, or by allowing a choice of either.\n",
    "\n",
    "> **More Information**: To learn more about estimator support for multiclass classification in Scikit-Learn, see the [Scikit-Learn documentation](https://scikit-learn.org/stable/modules/multiclass.html).\n",
    "\n",
    "### Explore the data\n",
    "\n",
    "Let's start by examining a dataset that contains observations of multiple classes. We'll use a dataset that contains observations of three different species of penguin.\n",
    "\n",
    "> **Citation**: The penguins dataset used in the this exercise is a subset of data collected and made available by [Dr.Â Kristen\n",
    "Gorman](https://www.uaf.edu/cfos/people/faculty/detail/kristen-gorman.php)\n",
    "and the [Palmer Station, Antarctica LTER](https://pal.lternet.edu/), a\n",
    "member of the [Long Term Ecological Research\n",
    "Network](https://lternet.edu/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the training dataset\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/ml-basics/penguins.csv\n",
    "penguins = pd.read_csv('penguins.csv')\n",
    "\n",
    "# Display a random sample of 10 observations\n",
    "sample = penguins.sample(10)\n",
    "sample"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset contains the following columns:\n",
    "* **CulmenLength**: The length in mm of the penguin's culmen (bill).\n",
    "* **CulmenDepth**: The depth in mm of the penguin's culmen.\n",
    "* **FlipperLength**: The length in mm of the penguin's flipper.\n",
    "* **BodyMass**: The body mass of the penguin in grams.\n",
    "* **Species**: An integer value that represents the species of the penguin.\n",
    "\n",
    "The **Species** column is the label we want to train a model to predict. The dataset includes three possible species, which are encoded as 0, 1, and 2. The actual species names are revealed by the code below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "penguin_classes = ['Adelie', 'Gentoo', 'Chinstrap']\n",
    "print(sample.columns[0:5].values, 'SpeciesName')\n",
    "for index, row in penguins.sample(10).iterrows():\n",
    "    print('[',row[0], row[1], row[2], row[3], int(row[4]),']',penguin_classes[int(row[4])])"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we know what the features and labels in the data represent, let's explore the dataset. First, let's see if there are any missing (*null*) values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Count the number of null values for each column\n",
    "penguins.isnull().sum()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It looks like there are some missing feature values, but no missing labels. Let's dig a little deeper and see the rows that contain nulls."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show rows containing nulls\n",
    "penguins[penguins.isnull().any(axis=1)]"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are two rows that contain no feature values at all (*NaN* stands for \"not a number\"), so these won't be useful in training a model. Let's discard them from the dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Drop rows containing NaN values\n",
    "penguins=penguins.dropna()\n",
    "#Confirm there are now no nulls\n",
    "penguins.isnull().sum()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we've dealt with the missing values, let's explore how the features relate to the label by creating some box charts."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "penguin_features = ['CulmenLength','CulmenDepth','FlipperLength','BodyMass']\n",
    "penguin_label = 'Species'\n",
    "for col in penguin_features:\n",
    "    penguins.boxplot(column=col, by=penguin_label, figsize=(6,6))\n",
    "    plt.title(col)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the box plots, it looks like species 0 and 2 (Adelie and Chinstrap) have similar data profiles for culmen depth, flipper length, and body mass, but Chinstraps tend to have longer culmens. Species 1 (Gentoo) tends to have fairly clearly differentiated features from the others; which should help us train a good classification model.\n",
    "\n",
    "### Prepare the data\n",
    "\n",
    "Just as for binary classification, before training the model, we need to separate the features and label, and then split the data into subsets for training and validation. We'll also apply a *stratification* technique when splitting the data to maintain the proportion of each label value in the training and validation datasets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and labels\n",
    "penguins_X, penguins_y = penguins[penguin_features].values, penguins[penguin_label].values\n",
    "\n",
    "# Split data 70%-30% into training set and test set\n",
    "x_penguin_train, x_penguin_test, y_penguin_train, y_penguin_test = train_test_split(penguins_X, penguins_y,\n",
    "                                                                                    test_size=0.30,\n",
    "                                                                                    random_state=0,\n",
    "                                                                                    stratify=penguins_y)\n",
    "\n",
    "print ('Training Set: %d, Test Set: %d \\n' % (x_penguin_train.shape[0], x_penguin_test.shape[0]))"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train and evaluate a multiclass classifier\n",
    "\n",
    "Now that we have a set of training features and corresponding training labels, we can fit a multiclass classification algorithm to the data to create a model. Most scikit-learn classification algorithms inherently support multiclass classification. We'll try a logistic regression algorithm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set regularization rate\n",
    "reg = 0.1\n",
    "\n",
    "# train a logistic regression model on the training set\n",
    "multi_model = LogisticRegression(C=1/reg, solver='lbfgs', multi_class='auto', max_iter=10000).fit(x_penguin_train, y_penguin_train)\n",
    "print (multi_model)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can use the trained model to predict the labels for the test features, and compare the predicted labels to the actual labels:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "penguin_predictions = multi_model.predict(x_penguin_test)\n",
    "print('Predicted labels: ', penguin_predictions[:15])\n",
    "print('Actual labels   : ' ,y_penguin_test[:15])"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at a classification report."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn. metrics import classification_report\n",
    "\n",
    "print(classification_report(y_penguin_test, penguin_predictions))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true,
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As with binary classification, the report includes *precision* and *recall* metrics for each class. However, while with binary classification we could focus on the scores for the *positive* class; in this case, there are multiple classes so we need to look at an overall metric (either the macro or weighted average) to get a sense of how well the model performs across all three classes.\n",
    "\n",
    "You can get the overall metrics separately from the report using the scikit-learn metrics score classes, but with multiclass results you must specify which average metric you want to use for precision and recall."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "print(\"Overall Accuracy:\",accuracy_score(y_penguin_test, penguin_predictions))\n",
    "print(\"Overall Precision:\",precision_score(y_penguin_test, penguin_predictions, average='macro'))\n",
    "print(\"Overall Recall:\",recall_score(y_penguin_test, penguin_predictions, average='macro'))"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's look at the confusion matrix for our model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Print the confusion matrix\n",
    "mcm = confusion_matrix(y_penguin_test, penguin_predictions)\n",
    "print(mcm)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The confusion matrix shows the intersection of predicted and actual label values for each class - in simple terms, the diagonal intersections from top-left to bottom-right indicate the number of correct predictions.\n",
    "\n",
    "When dealing with multiple classes, it's generally more intuitive to visualize this as a heat map, like this:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(mcm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(penguin_classes))\n",
    "plt.xticks(tick_marks, penguin_classes, rotation=45)\n",
    "plt.yticks(tick_marks, penguin_classes)\n",
    "plt.xlabel(\"Predicted Species\")\n",
    "plt.ylabel(\"Actual Species\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The darker squares in the confusion matrix plot indicate high numbers of cases, and you can hopefully see a diagonal line of darker squares indicating cases where the predicted and actual label are the same.\n",
    "\n",
    "In the case of a multiclass classification model, a single ROC curve showing true positive rate vs false positive rate is not possible. However, you can use the rates for each class in a One vs Rest (OVR) comparison to create a ROC chart for each class."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Get class probability scores\n",
    "penguin_prob = multi_model.predict_proba(x_penguin_test)\n",
    "\n",
    "# Get ROC metrics for each class\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "for i in range(len(penguin_classes)):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(y_penguin_test, penguin_prob[:,i], pos_label=i)\n",
    "    \n",
    "# Plot the ROC chart\n",
    "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label=penguin_classes[0] + ' vs Rest')\n",
    "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label=penguin_classes[1] + ' vs Rest')\n",
    "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label=penguin_classes[2] + ' vs Rest')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To quantify the ROC performance, you can calculate an aggregate area under the curve score that is averaged across all of the OVR curves."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auc = roc_auc_score(y_penguin_test,penguin_prob, multi_class='ovr')\n",
    "print('Average AUC:', auc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocess data in a pipeline\n",
    "\n",
    "Again, just like with binary classification, you can use a pipeline to apply preprocessing steps to the data before fitting it to an algorithm to train a model. Let's see if we can improve the penguin predictor by scaling the numeric features in a transformation steps before training. We'll also try a different algorithm (a support vector machine), just to show that we can!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define preprocessing for numeric columns (scale them)\n",
    "feature_columns = [0,1,2,3]\n",
    "feature_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "# Create preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('preprocess', feature_transformer, feature_columns)])\n",
    "\n",
    "# Create training pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('regressor', SVC(probability=True))])\n",
    "\n",
    "\n",
    "# fit the pipeline to train a linear regression model on the training set\n",
    "multi_model = pipeline.fit(x_penguin_train, y_penguin_train)\n",
    "print (multi_model)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can evaluate the new model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get predictions from test data\n",
    "penguin_predictions = multi_model.predict(x_penguin_test)\n",
    "penguin_prob = multi_model.predict_proba(x_penguin_test)\n",
    "\n",
    "# Overall metrics\n",
    "print(\"Overall Accuracy:\",accuracy_score(y_penguin_test, penguin_predictions))\n",
    "print(\"Overall Precision:\",precision_score(y_penguin_test, penguin_predictions, average='macro'))\n",
    "print(\"Overall Recall:\",recall_score(y_penguin_test, penguin_predictions, average='macro'))\n",
    "print('Average AUC:', roc_auc_score(y_penguin_test,penguin_prob, multi_class='ovr'))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.imshow(mcm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(penguin_classes))\n",
    "plt.xticks(tick_marks, penguin_classes, rotation=45)\n",
    "plt.yticks(tick_marks, penguin_classes)\n",
    "plt.xlabel(\"Predicted Species\")\n",
    "plt.ylabel(\"Actual Species\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use the model with new data observations\n",
    "\n",
    "Now let's save our trained model so we can use it again later."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model as a pickle file\n",
    "filename = './penguin_model.pkl'\n",
    "joblib.dump(multi_model, filename)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "OK, so now we have a trained model. Let's use it to predict the class of a new penguin observation:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load the model from the file\n",
    "multi_model = joblib.load(filename)\n",
    "\n",
    "# The model accepts an array of feature arrays (so you can predict the classes of multiple penguin observations in a single call)\n",
    "# We'll create an array with a single array of features, representing one penguin\n",
    "x_new = np.array([[50.4,15.3,224,5550]])\n",
    "print ('New sample: {}'.format(x_new[0]))\n",
    "\n",
    "# The model returns an array of predictions - one for each set of features submitted\n",
    "# In our case, we only submitted one penguin, so our prediction is the first one in the resulting array.\n",
    "penguin_pred = multi_model.predict(x_new)[0]\n",
    "print('Predicted class is', penguin_classes[penguin_pred])"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also submit a batch of penguin observations to the model, and get back a prediction for each one."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This time our input is an array of two feature arrays\n",
    "x_new = np.array([[49.5,18.4,195, 3600],\n",
    "         [38.2,20.1,190,3900]])\n",
    "print ('New samples:\\n{}'.format(x_new))\n",
    "\n",
    "# Call the web service, passing the input data\n",
    "predictions = multi_model.predict(x_new)\n",
    "\n",
    "# Get the predicted classes.\n",
    "for prediction in predictions:\n",
    "    print(prediction, '(' + penguin_classes[prediction] +')')"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "Classification is one of the most common forms of machine learning, and by following the basic principles we've discussed in this notebook you should be able to train and evaluate classification models with scikit-learn. It's worth spending some time investigating classification algorithms in more depth, and a good starting point is the [Scikit-Learn documentation](https://scikit-learn.org/stable/user_guide.html)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-azureml_py38-py",
   "language": "python",
   "display_name": "azureml_py38"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernel_info": {
   "name": "conda-env-azureml_py38-py"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}