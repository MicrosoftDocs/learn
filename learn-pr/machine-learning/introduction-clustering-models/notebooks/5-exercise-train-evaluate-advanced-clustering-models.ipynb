{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Exercise - Train and evaluate advanced clustering models**\n",
        "\n",
        "### Clustering: K-means and hierarchical\n",
        "\n",
        "In the last notebook, you learned that data can be broken into clusters. You learned how to estimate the number of clusters in your data points, by creating a series of clustering models with an incrementing number of clusters, and measuring the *within cluster sum of squares* (WCSS) within each cluster. In this notebook, you further explore K-means clustering, and also take a look at hierarchical clustering.\n",
        "\n",
        "> **Citation**: The seeds dataset used in this exercise was originally published by the Institute of Agrophysics of the Polish Academy of Sciences in Lublin, and can be downloaded from the UCI dataset repository (Dua, D. and Graff, C. (2019). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science).\n",
        "\n",
        "To get started, run the following cell to load and pre-process your data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the core tidyverse and tidymodels\n",
        "library(tidyverse)\n",
        "library(tidymodels)\n",
        "\n",
        "# Read the csv file into a tibble\n",
        "seeds <- read_csv(file = \"https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/data/seeds.csv\")\n",
        "\n",
        "# Narrow down to desired features\n",
        "seeds_select <- seeds %>% \n",
        "  select(!groove_length) %>% \n",
        "  mutate(species = factor(species))\n",
        "\n",
        "# Specify a recipe for PCA and extract 2 PCA components\n",
        "features_2d <- recipe(~ ., data = seeds_select) %>% \n",
        "  update_role(species, new_role = \"ID\") %>% \n",
        "  step_normalize(all_predictors()) %>% \n",
        "  step_pca(all_predictors(), num_comp = 2, id = \"pca\") %>% \n",
        "  prep() %>% \n",
        "  bake(new_data = NULL)\n",
        "\n",
        "# Preprocess and obtain data for clustering\n",
        "# Drop target column and normalize data\n",
        "seeds_features<- recipe(~ ., data = seeds_select) %>% \n",
        "  step_rm(species) %>% \n",
        "  step_normalize(all_predictors()) %>% \n",
        "  prep() %>% \n",
        "  bake(new_data = NULL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K-means clustering\n",
        "\n",
        "The algorithm you used to create your test clusters is called *K-means*. This is a commonly used clustering algorithm that separates a dataset into *K* clusters of equal variance. As a user, you define the number of clusters, *K*. The basic algorithm has the following steps:\n",
        "\n",
        "1.  A set of K centroids are randomly chosen.\n",
        "\n",
        "2.  Clusters are formed by assigning the data points to their closest centroid.\n",
        "\n",
        "3.  The means of each cluster is computed and the centroid is moved to the mean.\n",
        "\n",
        "4.  Steps 2 and 3 are repeated until a stopping criteria is met. Typically, the algorithm terminates when each new iteration results in negligible movement of centroids, and the clusters become static.\n",
        "\n",
        "5.  When the clusters stop changing, the algorithm has *converged*, defining the locations of the clusters. Note that the random starting point for the centroids means that re-running the algorithm might result in slightly different clusters. So, training usually involves multiple iterations, reinitializing the centroids each time, and the model with the best WCSS is selected.\n",
        "\n",
        "Now, back to our previous notebook: after creating a series of clustering models with different numbers of clusters, and plotting the WCSS across the clusters, you noticed a bend at around `k = 3`. This bend indicates that additional clusters beyond the third have little value, and that there are two to three reasonably well separated clusters of data points.\n",
        "\n",
        "So, let's perform *K-means* clustering by specifying `k = 3` clusters, and add the classifications to the dataset by using `augment`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set.seed(2056)\n",
        "# Fit and predict clusters with k = 3\n",
        "final_kmeans <- kmeans(seeds_features, centers = 3, nstart = 100, iter.max = 1000)\n",
        "\n",
        "# Add cluster prediction to the data set\n",
        "results_kmeans <- augment(final_kmeans, seeds_features) %>% \n",
        "# Bind pca_data - features_2d\n",
        "  bind_cols(features_2d)\n",
        "\n",
        "results_kmeans %>% \n",
        "  slice_head(n = 5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see those cluster assignments with the two dimensional data points. We'll add some touch of interactivity by using the [plotly package](https://plotly.com/r/getting-started/), so feel free to hover.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "library(plotly)\n",
        "# Plot km_cluster assignment on the PC data\n",
        "cluster_plot <- results_kmeans %>% \n",
        "  ggplot(mapping = aes(x = PC1, y = PC2)) +\n",
        "  geom_point(aes(shape = .cluster), size = 2) +\n",
        "  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\"))\n",
        "\n",
        "# Make plot interactive\n",
        "ggplotly(cluster_plot)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hopefully, the data has been separated into three distinct clusters.\n",
        "\n",
        "So what's the practical use of clustering? In some cases, you might have data that you need to group into distinct clusters, without knowing how many clusters there are or what they indicate. For example, a marketing organization might want to separate customers into distinct segments, and then investigate how those segments exhibit different purchasing behaviors.\n",
        "\n",
        "Sometimes, clustering is used as an initial step towards creating a classification model. You start by identifying distinct groups of data points, and then assign class labels to those clusters. You can then use this labelled data to train a classification model.\n",
        "\n",
        "In the case of the seeds data, the different species of seed are already known and encoded as 0 (*Kama*), 1 (*Rosa*), or 2 (*Canadian*). You can use these identifiers to compare the species classifications to the clusters identified by your unsupervised algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot km_cluster assignment on the PC data\n",
        "clust_spc_plot <- results_kmeans %>% \n",
        "  ggplot(mapping = aes(x = PC1, y = PC2)) +\n",
        "  geom_point(aes(shape = .cluster, color = species), size = 2, alpha = 0.8) +\n",
        "  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\"))\n",
        "\n",
        "# Make plot interactive\n",
        "ggplotly(clust_spc_plot)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There might be some differences between the cluster assignments and class labels as shown by the different colors (species) within each cluster (shape). But the K-means model should have done a reasonable job of clustering the observations, so that seeds of the same species are generally in the same cluster.\n",
        "\n",
        "### Hierarchical clustering\n",
        "\n",
        "Hierarchical clustering methods make fewer distributional assumptions compared to K-means methods. However, K-means methods are generally more scalable, sometimes very much more so.\n",
        "\n",
        "Hierarchical clustering creates clusters by either a *divisive* method or an *agglomerative* method. The divisive method is a top-down approach, starting with the entire dataset and then finding partitions in a stepwise manner. Agglomerative clustering is a bottom-up approach. In this lab, you will work with agglomerative clustering, commonly referred to as *AGNES* (AGglomerative NESting), which roughly works as follows:\n",
        "\n",
        "1.  The linkage distances between each of the data points is computed.\n",
        "\n",
        "2.  Points are clustered pairwise with their nearest neighbor.\n",
        "\n",
        "3.  Linkage distances between the clusters are computed.\n",
        "\n",
        "4.  Clusters are combined pairwise into larger clusters.\n",
        "\n",
        "5.  Steps 3 and 4 are repeated until all data points are in a single cluster.\n",
        "\n",
        "A fundamental question in hierarchical clustering is: *how do we measure the dissimilarity between two clusters of observations?* You can compute this in a number of ways:\n",
        "\n",
        "-   *Ward's minimum variance* method minimizes the total within-cluster variance. At each step, the pair of clusters with the smallest between-cluster distance are merged. It tends to produce more compact clusters.\n",
        "\n",
        "-   *Average linkage* uses the mean pairwise distance between the members of the two clusters. It can vary in the compactness of the clusters it creates.\n",
        "\n",
        "-   *Complete* or *maximal linkage* uses the maximum distance between the members of the two clusters. It tends to produce clusters that are compact contours by their borders, but they are not necessarily compact inside.\n",
        "\n",
        "Several different distance metrics are used to compute linkage functions:\n",
        "\n",
        "-   Euclidian, or l2 distance, is the most widely used. This is the only metric for the Ward linkage method.\n",
        "\n",
        "-   Manhattan, or l1 distance, is robust to outliers and has other interesting properties.\n",
        "\n",
        "-   Cosine similarity is the dot product between the location vectors, divided by the magnitudes of the vectors. Notice that this metric is a measure of similarity, whereas the other two metrics are measures of difference. Similarity can be quite useful when you're working with data such as images or text documents.\n",
        "\n",
        "You can visualize hierarchical clustering results by using an attractive, tree-based representation called a *dendrogram*. After the dendrogram has been constructed, you slice this structure horizontally to identify the clusters formed.\n",
        "\n",
        "### Agglomerative clustering\n",
        "\n",
        "Let's see an example of clustering the seeds data by using an agglomerative clustering algorithm. There are many functions available in R for hierarchical clustering.\n",
        "\n",
        "For example, the [`hclust()`](https://rdrr.io/r/stats/hclust.html) function is one way to perform hierarchical clustering in R. It only needs one input: a distance matrix structure computed by using distance metrics (for example, Euclidean), as produced by [`dist()`](https://rdrr.io/pkg/factoextra/man/dist.html). The function `hclust()` also allows you to specify the agglomeration method to be used (such as `\"complete\"`, `\"average\"`, `\"single\"`, or `\"ward.D\"`).\n",
        "\n",
        "Great! Let's fit multiple hierarchical clustering models based on different agglomeration methods, and see how the choice in agglomeration method changes the clustering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For reproducibility\n",
        "set.seed(2056)\n",
        "\n",
        "# Distance between observations matrix\n",
        "d <- dist(x = seeds_features, method = \"euclidean\")\n",
        "\n",
        "# Hierarchical clustering using Complete Linkage\n",
        "seeds_hclust_complete <- hclust(d, method = \"complete\")\n",
        "\n",
        "# Hierarchical clustering using Average Linkage\n",
        "seeds_hclust_average <- hclust(d, method = \"average\")\n",
        "\n",
        "# Hierarchical clustering using Ward Linkage\n",
        "seeds_hclust_ward <- hclust(d, method = \"ward.D2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The [factoextra package](https://rpkgs.datanovia.com/factoextra/index.html) provides functions ([`fviz_dend()`](https://rdrr.io/pkg/factoextra/man/fviz_dend.html)) to visualize hierarchical clustering. Let's visualize the dendrogram representation of the clusters starting with the *complete aggromeration* method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "library(factoextra)\n",
        "\n",
        "# Visualize cluster separations\n",
        "fviz_dend(seeds_hclust_complete, main = \"Complete Linkage\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What about average linkage?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize cluster separations\n",
        "fviz_dend(seeds_hclust_average, main = \"Average Linkage\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly, the Ward linkage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize cluster separations\n",
        "fviz_dend(seeds_hclust_ward, main = \"Ward Linkage\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perfect! Let's take a moment to analyze the nature of the clusters.\n",
        "\n",
        "You can do this mathematically by evaluating the *agglomerative coefficient (AC)*, which measures the clustering structure of the dataset. Values closer to 1 suggest a more balanced clustering structure, and values closer to 0 suggest less well-formed clusters. `cluster::agnes()` allows us to compute the hierarchical clustering as well as the AC metric, too.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "library(cluster)\n",
        "#Compute ac values\n",
        "ac_metric <- list(\n",
        "  complete_ac = agnes(seeds_features, metric = \"euclidean\", method = \"complete\")$ac,\n",
        "  average_ac = agnes(seeds_features, metric = \"euclidean\", method = \"average\")$ac,\n",
        "  ward_ac = agnes(seeds_features, metric = \"euclidean\", method = \"ward\")$ac\n",
        ")\n",
        "\n",
        "ac_metric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As explained earlier, complete and Ward linkages tend to produce tight clustering of objects.\n",
        "\n",
        "Now, let's determine the optimal number of clusters. Although hierarchical clustering doesn't require you to pre-specify the number of clusters, you still need to specify the number of clusters to extract. Let's use the WCSS method to determine the optimal number of clusters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine and visualize optimal n. of clusters\n",
        "#  hcut (for hierarchical clustering)\n",
        "fviz_nbclust(seeds_features, FUNcluster = hcut, method = \"wss\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just like in K-means clustering, the optimal number of clusters for this dataset is 3.\n",
        "\n",
        "Let's color our dendrogram according to k = 3, and analyze how observations will be grouped. We'll go with the Ward linkage method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize clustering structure for 3 groups\n",
        "fviz_dend(seeds_hclust_ward, k = 3, main = \"Ward Linkage\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plausible enough!\n",
        "\n",
        "You can now go ahead and cut the hierarchical clustering model into three clusters, and extract the cluster labels for each observation associated with a specified cut. To do this, you use `cutree()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hierarchical clustering using Ward Linkage\n",
        "seeds_hclust_ward <- hclust(d, method = \"ward.D2\")\n",
        "\n",
        "# Group data into 3 clusters\n",
        "results_hclust <- tibble(\n",
        "  cluster_id = cutree(seeds_hclust_ward, k = 3)) %>% \n",
        "  mutate(cluster_id = factor(cluster_id)) %>% \n",
        "  bind_cols(features_2d)\n",
        "\n",
        "results_hclust %>% \n",
        "  slice_head(n = 5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can compare K-means and hierarchical clustering by counting the number of observations of each species in the corresponding clusters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare k-m and hc\n",
        "results_hclust %>% \n",
        "  count(species, cluster_id) %>% \n",
        "  rename(cluster_id_hclust = cluster_id, n_hclust = n) %>% \n",
        "  bind_cols(results_kmeans %>% \n",
        "              count(species, .cluster) %>%\n",
        "              select(!species) %>% \n",
        "              rename(cluster_id_kmclust = .cluster, n_kmclust = n))\n",
        "            \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ignoring the `cluster_id` columns, because they are arbitrary, you can see that the observations were grouped quite similarly by the two algorithms. You can of course make a confusion matrix to better visualize this, but for now, you can move on. Let's wrap it up by making some plots that show how your observations were grouped into clusters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot h-cluster assignment on the PC data\n",
        "hclust_spc_plot <- results_hclust %>% \n",
        "  ggplot(mapping = aes(x = PC1, y = PC2)) +\n",
        "  geom_point(aes(shape = cluster_id, color = species), size = 2, alpha = 0.8) +\n",
        "  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\"))\n",
        "\n",
        "# Make plot interactive\n",
        "ggplotly(hclust_spc_plot)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Summary**\n",
        "\n",
        "Here we practiced using K-means and hierarchical clustering. This unsupervised learning has the ability to take unlabelled data and identify which of these data are similar to one another.\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": "",
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.4.1"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
