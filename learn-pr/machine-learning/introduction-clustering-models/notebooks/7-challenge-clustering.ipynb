{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Clustering Challenge**\n",
                "\n",
                "Clustering is an *unsupervised* machine learning technique in which you train a model to group similar entities into clusters based on their features.\n",
                "\n",
                "In this exercise, you must separate a dataset consisting of three numeric features (**A**, **B**, and **C**) into clusters.\n",
                "\n",
                "Let's begin by importing the data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the core tidyverse and make it available in your current R session\n",
                "suppressPackageStartupMessages(\n",
                "  library(tidyverse))\n",
                "\n",
                "# Read the csv file into a tibble\n",
                "clust_data <- read_csv(file = \"https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/challenges/data/clusters.csv\", show_col_types = FALSE)\n",
                "\n",
                "# Print the first 10 rows of the data\n",
                "clust_data %>% \n",
                "  slice_head(n = 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Your challenge is to identify the number of discrete clusters present in the data, and create a clustering model that separates the data into that number of clusters. You should also visualize the clusters to evaluate the level of separation achieved by your model.\n",
                "\n",
                "Add markdown and code cells as required to create your solution.\n",
                "\n",
                "### **Use PCA to create a 2D version of the features for visualization**\n",
                "\n",
                "*`Principal Component Analysis`* (PCA) is a dimension reduction method that aims at reducing the feature space, such that, most of the information or variability in the data set can be explained using fewer uncorrelated features. Let's see this in action by creating a specification of a `recipe` that will estimate the *principal components* based on our three variables. We'll later `prep` and`bake` the recipe to apply the computations.\n",
                "\n",
                "\n",
                "**Question 1.**\n",
                "\n",
                "For this question, create a specification of a `recipe` that will:\n",
                "- Normalize all predictors.\n",
                "- Convert all predictors into **two** principal components.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "set.seed(2056)\n",
                "# Load the core tidymodels packages and make them available in your current R session\n",
                "suppressPackageStartupMessages(\n",
                "  library(tidymodels))\n",
                "\n",
                "# Specify a recipe\n",
                "pca_rec <- recipe(____, data = clust_data) %>%\n",
                "  # Recipe step for normalizing predictors\n",
                "  ____(all_predictors()) %>% \n",
                "  # Recipe step for converting all predictors into 2 Principal Components\n",
                "  ____(____, num_comp = ____, id = \"pca\")\n",
                "\n",
                "# Print out recipe\n",
                "pca_rec\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = ottr::check(\"tests/Question 1.R\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Well done! Compared to supervised learning techniques, we have no `outcome` variable in this recipe.\n",
                "\n",
                "Now, time to prep and bake the recipe!\n",
                "\n",
                "**Question 2.**\n",
                "\n",
                "In this section:\n",
                "- Prep the recipe you created above. Prepping a recipe estimates the required quantities and statistics required by PCA.\n",
                "- Bake the prepped recipe. This takes the trained (prepped) recipe and applies its operations to a data set to create a design matrix. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Estimate the recipe \n",
                "pca_estimates <- prep(____)\n",
                "\n",
                "# Return preprocessed clust_data using bake\n",
                "features_2d <- pca_estimates %>% \n",
                "  ____(new_data = NULL)\n",
                "\n",
                "# Print baked data set\n",
                "features_2d %>% \n",
                "  slice_head(n = 5)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = ottr::check(\"tests/Question 2.R\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ðŸ¤© Good job! After prep and bake, you obtain two components which capture the maximum amount of information (i.e. variance) in the original variables.\n",
                "\n",
                "From the output of our prepped recipe `pca_estimates`, we can examine how much variance each component accounts for:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Examine how much variance each PC accounts for\n",
                "pca_estimates %>% \n",
                "  tidy(id = \"pca\", type = \"variance\") %>% \n",
                "  filter(str_detect(terms, \"percent\"))\n",
                "\n",
                "\n",
                "theme_set(theme_light())\n",
                "# Plot how much variance each PC accounts for\n",
                "pca_estimates %>% \n",
                "  tidy(id = \"pca\", type = \"variance\") %>% \n",
                "  filter(terms == \"percent variance\") %>% \n",
                "  ggplot(mapping = aes(x = component, y = value)) +\n",
                "  geom_col(fill = \"midnightblue\", alpha = 0.7) +\n",
                "  ylab(\"% of total variance\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This output tibbles and plots shows how well each principal component is explaining the original six variables. For example, the first principal component (PC1) explains about `84.93%` of the variance of the three variables. The second principal component explains an additional `7.63%`, giving a cumulative percent variance of `92.56%`. This is certainly better. It means that the first two variables seem to have some power in summarizing the original three variables.\n",
                "\n",
                "Now that we have the data points translated to two dimensions PC1 and PC2, we can visualize them in a plot:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize PC scores\n",
                "features_2d %>% \n",
                "  ggplot(mapping = aes(x = PC1, y = PC2)) +\n",
                "  geom_point(size = 2, color = \"dodgerblue3\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Calculate WCSS for multiple cluster numbers to determine the right number of clusters**\n",
                "\n",
                "Now here lies one of the fundamental problems with clustering - without known class labels, how do you know how many clusters to separate your data into?\n",
                "\n",
                "\n",
                "**Question 3.**\n",
                "\n",
                "In this section, we'll try to find the optimal number of clusters our data can be divided into by using our data sample, `clust_data`, to create a series of clustering models with an incrementing number of clusters, and then measure how tightly the data points are grouped within each cluster.\n",
                "\n",
                "A metric often used to measure this tightness is the *within cluster sum of squares* (WCSS), with lower values meaning that the data points are closer. You can then plot the WCSS for each model.\n",
                "\n",
                "- Prep and bake `clust_data` to obtain a new tibble where all the predictors are normalized.\n",
                "- Use the built-in `kmeans()` function to perform clustering by creating a series of 10 clustering models.\n",
                "- Extract the WCSS, `tot.withinss` for each cluster.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Obtain normalized Data for estimating clusters\n",
                "clust_estimate_data <- recipe(~ ., data = clust_data) %>%\n",
                "  # Step to normalize all predictors\n",
                "  ____(all_predictors()) %>% \n",
                "  # Prep the recipe\n",
                "  ____ %>% \n",
                "  # Apply trained recipe to train set: clust_data\n",
                "  ____\n",
                "\n",
                "# Print out data\n",
                "clust_estimate_data %>% \n",
                "  slice_head(n = 5)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, let's explore the WCSS of different numbers of clusters.\n",
                "\n",
                "We'll get to use `map()` from the [purrr](https://purrr.tidyverse.org/) package to apply functions to each element in list.\n",
                "\n",
                "> [`map()`](https://purrr.tidyverse.org/reference/map.html) functions allow you to replace many for loops with code that is both more succinct and easier to read. The best place to learn about the [`map()`](https://purrr.tidyverse.org/reference/map.html) functions is the [iteration chapter](http://r4ds.had.co.nz/iteration.html) in R for data science.\n",
                ">\n",
                "> `broom::glance.kmeans()` accepts a model object and returns a tibble with exactly one row of model summaries. The summaries are typically goodness of fit measures, p-values for hypothesis tests on residuals, or model convergence information.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "set.seed(2056)\n",
                "# Create 10 models with 1 to 10 clusters\n",
                "kclusts <- tibble(k = ____) %>% \n",
                "  mutate(\n",
                "    # Create a series of clustering models with centers k\n",
                "    model = map(k, ~ kmeans(x = clust_estimate_data, centers = .x, nstart = 20)),\n",
                "    # Extract the Total WCSS tot.withinss, for each model\n",
                "    tot.withinss = map_dbl(____, ~ glance(.x)$tot.withinss))\n",
                "\n",
                "\n",
                "\n",
                "# Plot Total within-cluster sum of squares (tot.withinss) for each model with center k\n",
                "kclusts %>% \n",
                "  ggplot(mapping = aes(x = k, y = _____)) +\n",
                "  geom_line(size = 1.2, alpha = 0.5, color = \"dodgerblue3\") +\n",
                "  geom_point(size = 2, color = \"dodgerblue3\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = ottr::check(\"tests/Question 3.R\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "That went well! What do you think the chart is telling us?\n",
                "\n",
                "\n",
                "\n",
                "We seek to **minimize** the the total within-cluster sum of squares, by performing K-means clustering. The plot shows a large reduction in WCSS (so greater *tightness*) as the number of clusters increases from one to two, and a further noticeable reduction from two to three, then three to four clusters. After that, the reduction is less pronounced, resulting in an `elbow` ðŸ’ªin the chart at around four clusters. This is a good indication that there are four reasonably well separated clusters of data points.\n",
                "\n",
                "### **Use K-Means**\n",
                "\n",
                "**Question 4.**\n",
                "\n",
                "\n",
                "In the previous section, we noticed that there is a good indication that there are four reasonably well separated clusters of data points. In this section:\n",
                "\n",
                "Extract the model separates the data into k = 4 clusters. Starting with the clustering results `kclusts`:\n",
                "- create a subset where the models contains `k = 4` clusters.\n",
                "- And then, using `pull()`, extract the clustering model in the `model` column.\n",
                "- And then retrieve the model components, such as cluster assignments, using `pluck()`\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract the model with k = 4 clusters\n",
                "final_kmeans <- kclusts %>% \n",
                "  # Narrow down to the model with K = 4 centers\n",
                "  filter(____) %>%\n",
                "  # Extract the clustering model\n",
                "  ___(model) %>% \n",
                "  # Retrieve model components\n",
                "  ____(1)\n",
                "\n",
                "# Print model components\n",
                "final_kmeans\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = ottr::check(\"tests/Question 4.R\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What can you make out of the kmeans object, `final_kmeans`? One of the available components is the `Clustering vector`: A vector of integers (from 1:k) indicating the cluster to which each observation in our data is allocated. Be sure to explore the other available components.\n",
                "\n",
                "\n",
                "### **Plot the clustered points**\n",
                "\n",
                "Great! Let's go ahead and visualize the clusters obtained.\n",
                "\n",
                "**Question 5.**\n",
                "\n",
                "In this section:\n",
                "\n",
                "- Add the cluster assignments generated by the model to the 2D version (2 Principal Components) of our data set, i.e `features_2d`. Hint: `augment`accepts a model object and a dataset and adds information about each observation in the dataset.\n",
                "\n",
                "- Visualize the cluster assignments using a scatter plot. Assign the `shape` and `color` aesthetics to the cluster number assigned to each data point.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load plotting libraries \n",
                "suppressPackageStartupMessages({\n",
                "  library(plotly)\n",
                "  library(paletteer)\n",
                "  \n",
                "})\n",
                "\n",
                "\n",
                "# Augment featured_2d with information from the clustering model\n",
                "results <-  augment(_____, _____) \n",
                "\n",
                "# Print some rows of the results\n",
                "results %>% \n",
                "  slice_head(n = 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Voila! There goes the two Principal Components (PC1, PC2) which capture the maximum amount of information (i.e. variance) in the original variables along with their corresponding cluster assignment (.cluster).\n",
                "\n",
                "Now, let's see this in an interactive plot!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot cluster assignments\n",
                "cluster_plot <- results %>% \n",
                "  ggplot(mapping = aes(x = PC1, y = PC2)) +\n",
                "  geom_point(aes(____, ____), size = 2, alpha = 0.8) +\n",
                "  paletteer::scale_color_paletteer_d(\"ggsci::category10_d3\")\n",
                "\n",
                "# Make plot interactive\n",
                "ggplotly(cluster_plot)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = ottr::check(\"tests/Question 5.R\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ðŸ¤©ðŸ¤© Hopefully we are able to observe that the data has been separated into four distinct clusters. \n",
                "\n",
                "K-Means is a commonly used clustering algorithm that separates a dataset into K clusters of equal variance such that observations within the same cluster are as similar as possible (i.e., high intra-class similarity), whereas observations from different clusters are as dissimilar as possible (i.e., low inter-class similarity).\n",
                "\n",
                "The first step in K-Means clustering is the data scientist specifying the number of clusters K to partition the observations into. Hierarchical clustering is an alternative approach which does not require the number of clusters to be defined in advance. Let's jump right into it.\n",
                "\n",
                "### **Try agglomerative clustering**\n",
                "\n",
                "Hierarchical clustering creates clusters by either a *divisive* method or *agglomerative* method. The `divisive` method is a `top down` approach starting with the entire dataset and then finding partitions in a stepwise manner. `Agglomerative clustering` is a `bottom up` approach in which each observation is initially considered as a single element cluster and at each step of the algorithm, two clusters that are most similar are combined into a bigger cluster until all points are members of one single big cluster.\n",
                "\n",
                "\n",
                "**Question 6.**\n",
                "\n",
                "In this section we'll cluster the data using an agglomerative clustering algorithm. There are many functions available in R for hierarchical clustering.\n",
                "\n",
                "The `hclust()` function is one way to perform hierarchical clustering. It requires a distance matrix structure and a specification of the agglomeration method to be used. \n",
                "\n",
                "- Use the `dist` function to compute the distance matrix (distances between the rows of the data) using the \"euclidean\" method.\n",
                "\n",
                "- Use `hclust` to perform Hierarchical clustering with the \"ward.D2\" (Ward linkage) agglomeration method.\n",
                "\n",
                "Please see the corresponding Learn module for more information on the above.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For reproducibility\n",
                "set.seed(2056)\n",
                "\n",
                "# Distance between observations matrix\n",
                "d <- ____(x = clust_estimate_data, method = \"____\")\n",
                "\n",
                "# Hierarchical clustering using Ward Linkage\n",
                "hclust_ward <- ____(d, method = \"____\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = ottr::check(\"tests/Question 6.R\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = \" # BEGIN TEST CONFIG\n",
                "\n",
                "success_message: Fantastic! You have successsfully performed hierarchical clustering using the Ward linkage method.\n",
                "failure_message: Almost there! Let's take a look at this again. Ensure you have used the Ward linkage method to perform Hierarchical clustering and the underlying distance metric used was the Euclidean distance.\n",
                "\" # END TEST CONFIG\n",
                "\n",
                "\n",
                "## Test ##\n",
                "test_that('hierarchical clustering was computed', {\n",
                "  expect_equal(hclust_ward$dist.method, \"euclidean\")\n",
                "  expect_equal(hclust_ward$method, \"ward.D2\")\n",
                "  \n",
                "  \n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The [factoextra](https://rpkgs.datanovia.com/factoextra/index.html) provides functions ([`fviz_dend()`](https://rdrr.io/pkg/factoextra/man/fviz_dend.html)) to visualize hierarchical clustering. Let's visualize the dendrogram representation of the clusters from the ward linkage method.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(factoextra)\n",
                "\n",
                "# Visualize cluster separations\n",
                "fviz_dend(hclust_ward, main = \"Ward Linkage\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Although hierarchical clustering does not require one to pre-specify the number of clusters, one still needs to specify the number of clusters to extract.\n",
                "\n",
                "We found out, using the `WCSS` method, that the optimal number of clusters was 4. Let's color our dendrogram according to k = 4 and observe how observations will be grouped.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize clustering structure for 4 groups\n",
                "fviz_dend(hclust_ward, k = 4, main = \"Ward Linkage\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Perfect! We can now go ahead and `cut` the hierarchical clustering model into four clusters and extract the cluster labels for each observation associated with a given cut. This is done using `cutree()`\n",
                "\n",
                "**Question 7.**\n",
                "\n",
                "In this section, we are going to `cut` the hierarchical clustering model into four clusters and extract the cluster labels for each observation.\n",
                "\n",
                "Starting with the Principal Components version of the data:\n",
                "\n",
                "- Using the `cutree` function, extract k = 4 groups of data obtained from the Hierarchical clustering process. Assign this to a column name `cluster_id`\n",
                "\n",
                "- Also, encode the `cluster_id` column as a factor.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "set.seed(2056)\n",
                "# Extract cluster labels obtained from the Hierarchical clustering process\n",
                "results_hclust <- features_2d %>% \n",
                "  ____(cluster_id = ____)\n",
                "\n",
                "\n",
                "# Print some rows of the results\n",
                "results_hclust %>% \n",
                "  slice_head(n = 5)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                ". = ottr::check(\"tests/Question 7.R\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Awesome! Now, let's inspect the cluster assignmnents visually.\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "### **View the agglomerative cluster assignments**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot h-cluster assignmnet on the PC data\n",
                "hclust_spc_plot <- results_hclust %>% \n",
                "  ggplot(mapping = aes(x = PC1, y = PC2)) +\n",
                "  geom_point(aes(shape = cluster_id, color = cluster_id), size = 2, alpha = 0.8) +\n",
                "  paletteer::scale_color_paletteer_d(\"ggsci::category10_d3\")\n",
                "\n",
                "# Make plot interactive\n",
                "ggplotly(hclust_spc_plot)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Neat! How does this compare to the K-Means results we visualized earlier?\n",
                "\n",
                "We can see that the observations were grouped quite similarly by the two clustering algorithms, **K-Means Clustering** and **Hierarchical Clustering**. \n",
                "\n",
                "We'll wrap it at that! In this challenge, you created unsupervised machine learning models, i.e K-Means and Hierarchical clustering models, to help you group unlabeled data observations into clusters. \n",
                "\n",
                "Congratulations on this amazing feat!\n",
                "\n",
                "Happy Learning,\n",
                "\n",
                "[Eric](https://twitter.com/ericntay), Gold Microsoft Learn Student Ambassador.\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
