Congratulations on building an audio binary classification speech model!  

You now have a better understanding of how an analog audio turns to digital sound, and how to create spectrogram images of your wave files. You used the PyTorch Speech Commands dataset, parsed the classes down to `yes` and `no`, and then looked at ways to understand and visualize audio data patterns. From there, you took the spectrograms, created images, and used a convolutional neural network to build your model.

You can expand on this knowledge by looking at other datasets and sounds, and also by looking at the `MFCC` transformer. Then you can build your model.

Be sure to check out these other modules, too:

* [Introduction to PyTorch](/learn/modules/intro-machine-learning-pytorch)
* [Computer Vision with PyTorch](/learn/modules/intro-computer-vision-pytorch)
* [Natural Language Processing with PyTorch](/learn/modules/intro-natural-language-processing-pytorch)

[!include[](../../../includes/open-link-in-new-tab-note.md)]


