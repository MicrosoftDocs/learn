### YamlMime:ModuleUnit
uid: learn.pytorch.intro-machine-learning.autograd
title: Automatic differentiation
metadata:
  title: Automatic differentiation
  description: When training neural networks, the most frequently used algorithm is back propagation. In this algorithm, parameters (model weights) are adjusted according to the gradient of the loss function with respect to the given parameter.
  author: cassiebreviu
  ms.author: cassieb
  ms.date: 04/14/2020
  ms.topic: interactive-tutorial
  ms.prod: azure
durationInMinutes: 10
sandbox: true
notebook: https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/intro-to-pytorch/6-autograd.ipynb
quiz:
  title: Check your knowledge
  questions:
    - content: "The purpose for the `torch.autograd` engine is to:"
      choices:
        - content: "automatically grade a model's accuracy"
          isCorrect: false
          explanation: "Incorrect, it automatically computes gradients during model optimization"
        - content: "automatically optimize a model's internal layer structure"
          isCorrect: false
          explanation: "Incorrect, it automatically computes gradients during model optimization"
        - content: "automatically optimize the set of data used to build a model"
          isCorrect: false
          explanation: "Incorrect, it automatically computes gradients during model optimization"
        - content: "automatically compute gradients during model optimization"
          isCorrect: true
          explanation: "Correct!"
