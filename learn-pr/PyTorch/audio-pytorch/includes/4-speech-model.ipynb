{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Speech Model\n",
    "\n",
    "Now that we have created the spectrogram images its time to build the computer vision model. If you are following along with the learning path then you already created a computer vision model in the second module in this path. We will be using the [torchvision]() package to build our vision model. Lets import the packages we need to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cassieb\\Anaconda3\\envs\\pytorchaudio\\lib\\site-packages\\torchaudio\\extension\\extension.py:14: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Spectorgram Images into DataLoader for Training\n",
    "\n",
    "Here we provide the path to our image data and use the [ImageFolder]() helper to load the images into tensors. The labels are created based on the name of the folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 7985\n",
      "    Root location: ./.data/spectrograms\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(201, 81), interpolation=PIL.Image.BILINEAR)\n",
      "               ToTensor()\n",
      "           )\n",
      "torch.Size([3, 201, 81])\n"
     ]
    }
   ],
   "source": [
    "data_path = './.data/spectrograms' #looking in subfolder train\n",
    "\n",
    "yes_no_dataset = datasets.ImageFolder(\n",
    "    root=data_path,\n",
    "    transform=transforms.Compose([transforms.Resize((201,81)),\n",
    "                                  transforms.ToTensor()\n",
    "                                  ])\n",
    ")\n",
    "print(yes_no_dataset)\n",
    "print(yes_no_dataset[5][0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data\n",
    "- Split the data to use 80% to train the model and 20% to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6388\n",
      "1597\n"
     ]
    }
   ],
   "source": [
    "#split data to test and train\n",
    "#use 80% to train\n",
    "train_size = int(0.8 * len(yes_no_dataset))\n",
    "test_size = len(yes_no_dataset) - train_size\n",
    "yes_no_train_dataset, yes_no_test_dataset = torch.utils.data.random_split(yes_no_dataset, [train_size, test_size])\n",
    "\n",
    "print(len(yes_no_train_dataset))\n",
    "print(len(yes_no_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the data into the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    yes_no_train_dataset,\n",
    "    batch_size=15,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    yes_no_test_dataset,\n",
    "    batch_size=15,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets take a look at what our tensor looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5608, 0.4824, 0.5098, 0.4353, 0.3490, 0.4824, 0.5843, 0.4392, 0.4667,\n",
       "        0.3608, 0.4431, 0.5059, 0.4667, 0.4824, 0.4745, 0.4118, 0.4431, 0.2039,\n",
       "        0.4510, 0.3765, 0.3804, 0.4118, 0.4745, 0.4706, 0.4392, 0.5020, 0.5020,\n",
       "        0.5137, 0.5059, 0.3922, 0.3059, 0.3804, 0.4196, 0.3647, 0.3647, 0.4392,\n",
       "        0.3451, 0.3804, 0.3608, 0.4078, 0.6392, 0.6980, 0.7529, 0.5333, 0.7137,\n",
       "        0.8000, 0.5882, 0.6196, 0.7373, 0.6863, 0.6863, 0.6118, 0.5922, 0.4863,\n",
       "        0.5725, 0.5608, 0.5098, 0.4745, 0.4745, 0.4431, 0.4706, 0.4000, 0.4039,\n",
       "        0.4314, 0.4275, 0.3333, 0.1569, 0.4353, 0.4824, 0.4588, 0.4118, 0.3961,\n",
       "        0.4314, 0.4118, 0.2667, 0.3686, 0.4980, 0.4941, 0.4275, 0.4863, 0.5922])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset[0][0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get GPU for training, else use CPU if GPU is not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Neural Netowrk\n",
    "- Create the Convolutional Neural Network and set the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNet(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (flatten): Flatten()\n",
      "  (fc1): Linear(in_features=51136, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(51136, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x,dim=1)\n",
    "    \n",
    "model = CNNet().to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test Functions\n",
    "- Here we will set the cost function, learning_rate, and optimizer. Then set up the train and test functions that we will call next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function used to determine best parameters\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# used to create optimal parameters\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training function\n",
    "\n",
    "def train(dataloader, model, loss, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = cost(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "\n",
    "# Create the validation/test function\n",
    "\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y) in enumerate(dataloader):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += cost(pred, Y).item()\n",
    "            correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "\n",
    "    print(f'\\nTest Error:\\nacc: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "- Now lets set the number of epochs and call our `train` and `test` functions for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.683419  [    0/ 6388]\n",
      "loss: 0.695789  [ 1500/ 6388]\n",
      "loss: 0.692987  [ 3000/ 6388]\n",
      "loss: 0.666870  [ 4500/ 6388]\n",
      "loss: 0.692348  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 49.7%, avg loss: 0.046441\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.693147  [    0/ 6388]\n",
      "loss: 0.693147  [ 1500/ 6388]\n",
      "loss: 0.693147  [ 3000/ 6388]\n",
      "loss: 0.693147  [ 4500/ 6388]\n",
      "loss: 0.723022  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 49.7%, avg loss: 0.046441\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.693147  [    0/ 6388]\n",
      "loss: 0.666938  [ 1500/ 6388]\n",
      "loss: 0.773418  [ 3000/ 6388]\n",
      "loss: 0.666485  [ 4500/ 6388]\n",
      "loss: 0.685241  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 49.7%, avg loss: 0.046441\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.695083  [    0/ 6388]\n",
      "loss: 0.706812  [ 1500/ 6388]\n",
      "loss: 0.620915  [ 3000/ 6388]\n",
      "loss: 0.488353  [ 4500/ 6388]\n",
      "loss: 0.471648  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 86.9%, avg loss: 0.022696\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.271708  [    0/ 6388]\n",
      "loss: 0.274831  [ 1500/ 6388]\n",
      "loss: 0.166102  [ 3000/ 6388]\n",
      "loss: 0.617657  [ 4500/ 6388]\n",
      "loss: 0.122830  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 89.7%, avg loss: 0.016228\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.347690  [    0/ 6388]\n",
      "loss: 0.256594  [ 1500/ 6388]\n",
      "loss: 0.358795  [ 3000/ 6388]\n",
      "loss: 0.380411  [ 4500/ 6388]\n",
      "loss: 0.844688  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 90.8%, avg loss: 0.014417\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.339621  [    0/ 6388]\n",
      "loss: 0.173808  [ 1500/ 6388]\n",
      "loss: 0.449916  [ 3000/ 6388]\n",
      "loss: 0.249338  [ 4500/ 6388]\n",
      "loss: 0.115090  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 92.3%, avg loss: 0.012824\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.199789  [    0/ 6388]\n",
      "loss: 0.266175  [ 1500/ 6388]\n",
      "loss: 0.135942  [ 3000/ 6388]\n",
      "loss: 0.120991  [ 4500/ 6388]\n",
      "loss: 0.436009  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 92.8%, avg loss: 0.012056\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.520262  [    0/ 6388]\n",
      "loss: 0.126356  [ 1500/ 6388]\n",
      "loss: 0.225927  [ 3000/ 6388]\n",
      "loss: 0.128359  [ 4500/ 6388]\n",
      "loss: 0.157707  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 93.1%, avg loss: 0.011095\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.074408  [    0/ 6388]\n",
      "loss: 0.227902  [ 1500/ 6388]\n",
      "loss: 0.094457  [ 3000/ 6388]\n",
      "loss: 0.229964  [ 4500/ 6388]\n",
      "loss: 0.205378  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 93.6%, avg loss: 0.010279\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.193544  [    0/ 6388]\n",
      "loss: 0.209379  [ 1500/ 6388]\n",
      "loss: 0.170117  [ 3000/ 6388]\n",
      "loss: 0.366720  [ 4500/ 6388]\n",
      "loss: 0.118462  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 93.6%, avg loss: 0.010135\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.146690  [    0/ 6388]\n",
      "loss: 0.123937  [ 1500/ 6388]\n",
      "loss: 0.150614  [ 3000/ 6388]\n",
      "loss: 0.052775  [ 4500/ 6388]\n",
      "loss: 0.131337  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 94.0%, avg loss: 0.009229\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.155237  [    0/ 6388]\n",
      "loss: 0.051295  [ 1500/ 6388]\n",
      "loss: 0.274036  [ 3000/ 6388]\n",
      "loss: 0.062302  [ 4500/ 6388]\n",
      "loss: 0.116733  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 94.5%, avg loss: 0.009083\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.244489  [    0/ 6388]\n",
      "loss: 0.084151  [ 1500/ 6388]\n",
      "loss: 0.180910  [ 3000/ 6388]\n",
      "loss: 0.147533  [ 4500/ 6388]\n",
      "loss: 0.064108  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 94.8%, avg loss: 0.008690\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.236073  [    0/ 6388]\n",
      "loss: 0.030132  [ 1500/ 6388]\n",
      "loss: 0.263236  [ 3000/ 6388]\n",
      "loss: 0.107276  [ 4500/ 6388]\n",
      "loss: 0.104241  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 94.6%, avg loss: 0.008002\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}\\n-------------------------------')\n",
    "    train(train_dataloader, model, cost, optimizer)\n",
    "    test(test_dataloader, model)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Test the Model\n",
    " \n",
    "Awesome! You should have got somewhere between a 93%-95% accuracy by the 15th epoch. Here we grab a batch from our test data and see how the model performs on the predicted vs the actual result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0], device='cuda:0')\n",
      "Actual:\n",
      "tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss, correct = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, (X, Y) in enumerate(test_dataloader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        pred = model(X)\n",
    "        print(\"Predicted:\")\n",
    "        print(f\"{pred.argmax(1)}\")\n",
    "        print(\"Actual:\")\n",
    "        print(f\"{Y}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your own voice!\n",
    "\n",
    "Here comes the fun part! Testing with your _own_ voice. First record a wave file of our own voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywebrtc import AudioRecorder, CameraStream\n",
    "from IPython.display import Audio\n",
    "\n",
    "camera = CameraStream(constraints={'audio': True,'video':False})\n",
    "recorder = AudioRecorder(stream=camera)\n",
    "recorder\n",
    "\n",
    "# TODO: This recording logic doesnt work yes and not sure we can support this in learn\n",
    "with open('recording.webm', 'wb') as f:\n",
    "    f.write(recorder.audio.value)\n",
    "!ffmpeg -i recording.webm -ac 1 -f wav file.wav -y -hide_banner -\n",
    "waveform, sr = torchaudio.load(\".//.data//myaudio//file.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `create_image` method takes the waveform and creates our image dataset to test with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image(waveform, filename, my_audio_dir):\n",
    "    #make directory\n",
    "    os.makedirs(f'{my_audio_dir}//spectrograms//test//', mode=0o777, exist_ok=True)\n",
    "\n",
    "    spectrogram_tensor = torchaudio.transforms.Spectrogram()(waveform)\n",
    "    \n",
    "    img_path = f'{my_audio_dir}//spectrograms//test//{filename}.png'\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.imsave(img_path, spectrogram_tensor.log2()[0,:,:].numpy(), cmap='gray')\n",
    "    return datasets.ImageFolder(\n",
    "                root= f'{my_audio_dir}//spectrograms//',\n",
    "                transform=transforms.Compose([transforms.Resize((201,81)),\n",
    "                                              transforms.ToTensor()\n",
    "                                              ])\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `predict` methoed gets the prediction for our voice using the model we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tensor):\n",
    "    # Use the model to predict the label of the waveform\n",
    "    tensor = tensor.to(device)\n",
    "    tensor = model(tensor.unsqueeze(0))\n",
    "    index = tensor.argmax(dim=-1)\n",
    "    if index == 1:\n",
    "        return 'yes'\n",
    "    return 'no'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set the actual `label` you recorded, and see if the model works with your recording!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: yes Actual: yes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label = 'yes'\n",
    "filename = 'yes_2.wav'\n",
    "#filename = 'no_2.wav'\n",
    "\n",
    "## load audio file\n",
    "myaudio_dir = './/.data//myaudio//yes'\n",
    "#myaudio_dir = './/.data//myaudio//no'\n",
    "waveform, sample_rate = torchaudio.load(f'{myaudio_dir}//{filename}')\n",
    "\n",
    "#create image and save to spectorgrams folder\n",
    "test_img = create_image(waveform, filename, myaudio_dir)\n",
    "\n",
    "print(f\"Predicted: {predict(test_img[0][0])} Actual: {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
