{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from typing import Tuple"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we understand how to get the data and model, we're ready to train the neural network. First, we need to load the data using the technique we discussed in the first unit. In order not to clutter this notebook, we've added the `get_data` function and `NeuralNetwork` class you've already seen to a separate `kintro.py` file, which we'll import here. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "!wget -Nq https://raw.githubusercontent.com/MicrosoftDocs/tensorflow-learning-path/main/intro-keras/kintro.py\n",
        "from kintro import *"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to understand what happens during training, we need to add a little more detail to our neural network visualization:\n",
        "\n",
        "![Basic neural network with details](notebooks/images/2-fashion-nn-with-details.png)\n",
        "\n",
        "There's a lot of new information in this diagram, so I'll expand on each of the new concepts here. \n",
        "\n",
        "Notice that we've added weights $W$ to the connections between layers, and bias $b$ as input to `Dense` layers &mdash; $W$ and $b$ are the neural network's parameters. Our goal when training our network (also known as fitting) is to find the parameters $W$ and $b$ that minimize the differences between the actual and predicted labels for our data. \n",
        "\n",
        "Notice also that we added a Loss function to the diagram. This function takes in the outputs of the model $y'$ (the predicted labels) and the actual labels $y$, measures their differences, and combines those into a single output, which we call the loss. The loss gives us a single number that quantifies how similar our predictions are to the actual labels: a high loss indicates that they're different, and a low loss indicates that our predictions are accurate. The loss function most frequently used in classification is cross-entropy loss, and Keras provides several varieties of this loss. Because our dataset includes a single integer label for each example, we use the [`SparseCategoricalCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) loss function.\n",
        "\n",
        "Now let's look at what happens in the `Dense` layers. `Dense` layers add a linear operation involving the input data and parameters $W$ and $b$. For example, the top node of the first `Dense` layer performs the following calculation:\n",
        "\n",
        "$$\n",
        "z^1_1 = w^1_{1,1} x_1 + ... + w^1_{1,784} x_{784} + b^1_1\n",
        "$$\n",
        "\n",
        "If we specify a ReLU activation function, the output of the linear operation is then passed as input to a ReLU function:\n",
        "\n",
        "$$\n",
        "a^1_1 = ReLU(z^1_1)\n",
        "$$\n",
        "\n",
        "Mathematically speaking, we can now think of our neural network as a function $\\ell$ that takes as input the data $X$, expected labels $y$, and parameters $W$ and $b$, then performs a sequence of operations on that data, and returns a loss. \n",
        "\n",
        "$$\n",
        "\\mathrm{loss} = \\ell(X, y, W, b)\n",
        "$$\n",
        "\n",
        "Our goal is to find the parameters $W$ and $b$ that lead to the lowest possible loss. (We can't change our data $X$ or the corresponding labels $y$ &mdash; they're fixed &mdash; but we can adjust $W$ and $b$.) It turns out that problems of this kind fall in the well-studied mathematical area of optimization. The simplest minimization algorithm is gradient descent, and in this sample we use a variation known as Stochastic Gradient Descent or [`SGD`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD). The training process is roughly as follows: First, the parameters $W$ and $b$ are initialized to random values. We then make predictions using those values, and evaluate the loss function to measure the quality of the predictions. And last, we use the optimizer to adjust the values of $W$ and $b$ such that the loss will be a bit lower next time. We repeat this process until we've found parameters that give us a very small loss value and good predictions.\n",
        "\n",
        "We're now ready to \"compile\" the model &mdash; this is where we tell it that we want to use the `SGD` optimizer and the `SparseCategoricalCrossentropy` loss function. We also tell the model that we want it to report on the accuracy during training. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "learning_rate = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "(train_dataset, test_dataset) = get_data(batch_size)\n",
        "\n",
        "model = NeuralNetwork()\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "metrics = ['accuracy']\n",
        "model.compile(optimizer, loss_fn, metrics)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few details from the code above deserve a quick explanation. \n",
        "\n",
        "Notice that we pass `from_logits=True` to the loss function. This is because the categorical cross-entropy function requires a probability distribution as input, meaning that the numbers should be between zero and one, and they should add up to one. Our network produces a vector of numbers that have no upper or lower bound (called \"logits\"), so we need to normalize them to get a probability distribution. This is typically done using the `softmax` function, and specifying `from_logits=True` automatically calculates the softmax before computing the loss.\n",
        "\n",
        "Notice also that we pass a `learning_rate` to the `SGD` optimizer. The learning rate is a parameter needed in the gradient descent algorithm. We could have left it at the default, which is 0.01, but it's important to know how to specify it because different learning rates can lead to very different prediction accuracies.\n",
        "\n",
        "Finally, notice that we specified a `batch_size`, which we used in the construction of the `Dataset`, as we saw earlier. This is important during training, because it tells the model that we want to train on 64 images at a time. You might be wondering why 64? Why not train on a single image at a time? Or all 60,000 images at once? Doing a complete training step for each individual image would be inefficient because we would have to perform all the calculations 60,000 times in order to account for every input image. If we included all the input images in $X$, we'd need a lot of memory, and we'd spend a lot of time computing each training step. So we settle for a size in between, called the \"mini-batch\" size. \n",
        "\n",
        "Now that we've configured our model with the parameters we need for training, we can call `fit` to train the model. We specify the number of epochs as 5, which means that we want to iterate over the complete set of 60,000 training images five times while training the neural network. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "epochs = 5\n",
        "print('\\nFitting:')\n",
        "model.fit(train_dataset, epochs=epochs)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fitting:\n",
            "Epoch 1/5\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.6220 - accuracy: 0.7797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.4599 - accuracy: 0.8352\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.4215 - accuracy: 0.8494\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.3995 - accuracy: 0.8566\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.3863 - accuracy: 0.8601\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f03f8030d90>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training has found values for the parameters $W$ and $b$ such that, when we provide an image as input, we'll get a reasonable prediction as output. Our model is now ready to be tested. Remember that when we loaded the data, we obtained two datasets, one with training data and another with test data. It's time to use the test dataset."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "print('\\nEvaluating:')\n",
        "(test_loss, test_accuracy) = model.evaluate(test_dataset)\n",
        "print(f'\\nTest accuracy: {test_accuracy * 100:>0.1f}%, test loss: {test_loss:>8f}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating:\n",
            "157/157 [==============================] - 0s 638us/step - loss: 0.4626 - accuracy: 0.8252\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "Test accuracy: 82.5%, test loss: 0.462619\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've achieved pretty good test accuracy, considering that we used such a simple network and only five epochs of training. We're done with training and can now save the model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "  model.save('outputs/model')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: outputs/model/assets\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our neural network has appropriate values for its parameters, we can use it to make a prediction."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "a7d8d32a02de2fe32a77a4e581138922e011c09664b6c2991156e76c4176efab"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('fashion-tf': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "conda-env-py37_tensorflow-py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}