{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative networks\n",
        "\n",
        "Recurrent Neural Networks (RNNs) and their gated cell variants such as Long Short-Term Memory cells (LSTMs) and Gated Recurrent Units (GRUs) provide a mechanism for language modeling &mdash; they can learn word ordering and make predictions for the next word in a sequence. This allows us to use RNNs for **generative tasks**, such as ordinary text generation, machine translation, and even image captioning.\n",
        "\n",
        "In the RNN architecture we discussed in the previous notebook, each RNN unit produced the next hidden state as an output. However, we could also add another output to each recurrent unit, which would allow us to output a **sequence** with the same length as the original sequence. Moreover, we could use RNN units that don't accept an input at each step, instead they take an initial state vector and produce a sequence of outputs.\n",
        "\n",
        "This allows for different neural architectures, as is shown in the picture below:\n",
        "\n",
        "![Image showing common recurrent neural network patterns.](notebooks/images/unreasonable-effectiveness-of-rnn.jpg)\n",
        "*Image from blog post [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by [Andrej Karpaty](http://karpathy.github.io/)*\n",
        "\n",
        "* **One-to-one** is a traditional neural network with one input and one output.\n",
        "* **One-to-many** is a generative architecture that accepts one input value, and generates a sequence of output values. For example, if we want to train an **image captioning** network that produces a textual description of a picture, we can use a picture as input, pass it through a CNN to obtain the hidden state, and then have a recurrent chain generate the caption word-by-word.\n",
        "* **Many-to-one** corresponds to the RNN architectures we described in the previous unit, such as text classification.\n",
        "* **Many-to-many**, or **sequence-to-sequence** corresponds to tasks such as **machine translation**, in which case the RNN first collects all the information from the input sequence into the hidden state, and then unrolls this state into the output sequence.\n",
        "\n",
        "In this unit, we will focus on simple generative models that help us generate text. For simplicity, let's build a **character-level network**, which generates text letter by letter. During training, we need to take a text corpus, and split it into letter sequences. \n",
        "\n",
        "> For the sandbox environment, we need to run the following cell to make sure the required library is installed, and data is prefetched. If you're running locally, you can skip the following cell."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install --quiet tensorflow_datasets==4.4.0\n",
        "!cd ~ && wget -q -O - https://mslearntensorflowlp.blob.core.windows.net/data/tfds-ag-news.tgz | tar xz"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "ds_train, ds_test = tfds.load('ag_news_subset').values()"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building character vocabulary\n",
        "\n",
        "To build a character-level generative network, we need to split the text into individual characters instead of words. The `TextVectorization` layer that we've been using can't do that, so we have two options:\n",
        "\n",
        "* Manually load text and implement our own tokenization, as in [this official Keras example](https://keras.io/examples/generative/lstm_character_level_text_generation/).\n",
        "* Use the `Tokenizer` class for character-level tokenization.\n",
        "\n",
        "We'll go with the second option. `Tokenizer` can also be used to tokenize into words, so we should be able to switch from character-level to word-level tokenization quite easily.\n",
        "\n",
        "To do character-level tokenization, we need to pass the `char_level=True` parameter:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(x):\n",
        "    return x['title']+' '+x['description']\n",
        "\n",
        "def tupelize(x):\n",
        "    return (extract_text(x),x['label'])\n",
        "\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True,lower=False)\n",
        "tokenizer.fit_on_texts([x['title'].numpy().decode('utf-8') for x in ds_train])"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also want to use one special token to denote **end-of-sequence**, which we'll call `<eos>`. Let's add it manually to the vocabulary:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "eos_token = len(tokenizer.word_index)+1\n",
        "tokenizer.word_index['<eos>'] = eos_token\n",
        "\n",
        "vocab_size = eos_token + 1"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, to encode text into sequences of numbers, we can use:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.texts_to_sequences(['Hello, world!'])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "[[48, 2, 10, 10, 5, 44, 1, 25, 5, 8, 10, 13, 78]]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a generative RNN to generate titles\n",
        "\n",
        "Here's how we'll train an RNN to generate titles: on each step, we'll take one title as input, and for each input character in that title, we'll train the network to generate the next character as output:\n",
        "\n",
        "![Image showing an example RNN generation of the word 'HELLO'.](notebooks/images/rnn-generate.png)\n",
        "\n",
        "For the last character of our sequence, we'll ask the network to generate the `<eos>` token.\n",
        "\n",
        "The main difference between the generative RNN we're using here and the RNNs we've seen previously is that each cell of the generative RNN will produce an output, not just the final cell. This can be achieved by specifying the `return_sequences` parameter of the RNN cell.\n",
        "\n",
        "Thus, during training, an input to the network would be a sequence of encoded characters of some length, and the corresponding output would be a sequence of the same length, but shifted by one element and terminated by `<eos>`. The minibatch will consist of several such sequences, and we'll need to use **padding** to align all sequences.\n",
        "\n",
        "Let's create functions that transform the dataset for us. Because we want to pad sequences at the minibatch level, we'll first create the minibatches by calling `.batch()`, and then use `map` to do transformation. Here's the code for the function that does the transformation, which takes a whole minibatch as a parameter:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def title_batch(x):\n",
        "    x = [t.numpy().decode('utf-8') for t in x]\n",
        "    z = tokenizer.texts_to_sequences(x)\n",
        "    z = tf.keras.preprocessing.sequence.pad_sequences(z)\n",
        "    return tf.one_hot(z,vocab_size), tf.one_hot(tf.concat([z[:,1:],tf.constant(eos_token,shape=(len(z),1))],axis=1),vocab_size)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few important things that we do here:\n",
        "* We first extract the actual text from the string tensor.\n",
        "* `text_to_sequences` converts the list of strings into a list of integer tensors.\n",
        "* `pad_sequences` pads those tensors to their maximum length.\n",
        "* We finally one-hot encode all the characters, and also do the shifting and `<eos>` appending. We will soon see why we need one-hot-encoded characters.\n",
        "\n",
        "However, this function is **Pythonic** &mdash; it cannot be automatically translated into the TensorFlow computational graph. (Computational graphs will be discussed in more detail in the module on going beyond Keras of this learning path.) If we try to pass it directly to the `Dataset.map` function, which is how we intend to use this function, we'll get an error. To get around this problem, we need to enclose this Pythonic call by using the `py_function` wrapper: "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def title_batch_fn(x):\n",
        "    x = x['title']\n",
        "    a,b = tf.py_function(title_batch,inp=[x],Tout=(tf.float32,tf.float32))\n",
        "    return a,b"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note**: You may be wondering if instead of calling the `py_function` wrapper we could transform the dataset using standard Python functions before passing it to `fit`. While this can definitely be done, when using `Dataset.map` the data transformation pipeline is executed using TensorFlow's computational graph, which takes advantage of GPU computations and minimizes the need to pass data between CPU/GPU.\n",
        "\n",
        "We can now build our generator network and start training. Our network can be based on any recurrent cell which we discussed in the previous unit (simple, LSTM or GRU). In our example we'll use LSTM.\n",
        "\n",
        "Because the network takes characters as input, the vocabulary size is pretty small. Therefore we don't need an embedding layer &mdash; we can feed one-hot-encoded input directly into the LSTM cell. The output layer is then a `Dense` classifier that will convert the LSTM output into one-hot-encoded token numbers.\n",
        "\n",
        "In addition, since we're dealing with variable-length sequences, we can use a `Masking` layer to create a mask that will ignore the padded part of the string. This is not strictly needed, because we're not interested in everything that goes beyond the `<eos>` token, but we will use it to get some experience with this layer type. The input shape is `(None, vocab_size)`, where `None` indicates the sequence of variable length, and the output shape is also `(None, vocab_size)`, as you can see from the `summary` printout below: "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Masking(input_shape=(None,vocab_size)),\n",
        "    keras.layers.LSTM(128,return_sequences=True),\n",
        "    keras.layers.Dense(vocab_size,activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy')\n",
        "\n",
        "model.fit(ds_train.batch(8).map(title_batch_fn))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "masking (Masking)            (None, None, 84)          0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 128)         109056    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, None, 84)          10836     \n",
            "=================================================================\n",
            "Total params: 119,892\n",
            "Trainable params: 119,892\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "15000/15000 [==============================] - 281s 19ms/step - loss: 1.5350\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f75f4326e50>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating output\n",
        "\n",
        "Now that we've trained the model, we want to use it to generate output. First of all, we need a way to decode text represented by a sequence of token numbers. We could use the `tokenizer.sequences_to_texts` function, but it doesn't work well with character-level tokenization. Therefore we will take a dictionary of tokens from the tokenizer (called `word_index`), build a reverse map, and write our own decoding function:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_map = {val:key for key, val in tokenizer.word_index.items()}\n",
        "\n",
        "def decode(x):\n",
        "    return ''.join([reverse_map.get(t,'') for t in x])"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's do the generation. We first encode a string passed as parameter into a sequence, and then on each step we call our network to infer the next character. \n",
        "\n",
        "The output of the network is a vector of `vocab_size` elements representing probablities of each token, and we can find the most probably token number by using `argmax`. We then append this character to the generated list of tokens, and proceed with the generation. This process of generating one character is repeated `size` times to generate the required number of characters, and we terminate early if `eos_token` is encountered."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model,size=100,start='Today '):\n",
        "        inp = tokenizer.texts_to_sequences([start])[0]\n",
        "        chars = inp\n",
        "        for i in range(size):\n",
        "            out = model(tf.expand_dims(tf.one_hot(inp,vocab_size),0))[0][-1]\n",
        "            nc = tf.argmax(out)\n",
        "            if nc==eos_token:\n",
        "                break\n",
        "            chars.append(nc.numpy())\n",
        "            inp = inp+[nc]\n",
        "        return decode(chars)\n",
        "    \n",
        "generate(model)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "'Today #39;s share of the contract on the contract on the contral (AFP)'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling output during training\n",
        "\n",
        "Because we don't have any useful metrics such as *accuracy*, the only way we can see that our model is getting better is by **sampling** generated strings during training. We use **callbacks** for that, which are functions that we can pass to the `fit` function, and that will be called periodically during training. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_callback = keras.callbacks.LambdaCallback(\n",
        "  on_epoch_end = lambda batch, logs: print(generate(model))\n",
        ")\n",
        "\n",
        "model.fit(ds_train.batch(8).map(title_batch_fn),callbacks=[sampling_callback],epochs=3)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "14999/15000 [============================>.] - ETA: 0s - loss: 1.2697Today #9;seat for the control in the confidence in Iraq\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "15000/15000 [==============================] - 281s 19ms/step - loss: 1.2697\n",
            "Epoch 2/3\n",
            "14999/15000 [============================>.] - ETA: 0s - loss: 1.2057Today #9;seaging #39; start for the control in service\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "15000/15000 [==============================] - 282s 19ms/step - loss: 1.2057\n",
            "Epoch 3/3\n",
            "15000/15000 [==============================] - ETA: 0s - loss: 1.1750Today #9;seague #39; start for the contract for the contract\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "15000/15000 [==============================] - 284s 19ms/step - loss: 1.1750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f7669c86b50>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example already generates some pretty good text, but it can be further improved in several ways:\n",
        "* **More text**. We only used titles for our task, but you may want to experiment with full text. Remember that RNNs aren't that great at handling long sequences, so it makes sense to either split them into shorter sentences, or to always train on a sequence of fixed length `num_chars` (say, 256). You may try to change the example above into this architecture, using the [official Keras tutorial](https://keras.io/examples/generative/lstm_character_level_text_generation/) as inspiration.\n",
        "* **Multilayer LSTM**. It makes sense to try 2 or 3 layers of LSTM cells. As we mentioned in the previous unit, each layer of LSTM extracts certain patterns from the text, and in the case of a character-level generator we can expect lower LSTM levels to be responsible for extracting syllables, and higher levels to extract words and word combinations. This can be implemented by adding layers sequentially to the model.\n",
        "* You may also want to experiment with **GRU units** and see which one performs better, and with **different hidden layer sizes**. A hidden layer that's too large may result in overfitting (because the network will learn the exact text), and a hidden layer that's too small might not produce good results."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soft text generation and temperature\n",
        "\n",
        "In the code for the `generate` function, we took the character with the highest probability as the next character in the generated text. This resulted in text that cycles between the same character sequences again and again, like in this example:\n",
        "```\n",
        "today of the second the company and a second the company ...\n",
        "```\n",
        "\n",
        "However, if we look at the probability distribution for the next character, it may be that there are several high probabilities that are pretty similar. For example, when looking for the next character in the sequence '*play*', it's similarly likely that it's either space or **e** (as in the word *player*).\n",
        "\n",
        "Therefore, it's not always the best choice to select the character with the absolute highest probability &mdash; choosing the second or third highest might still lead to meaningful text, and may avoid cycling through character sequences. Therefore, a better strategy is to **sample** characters from the probability distribution given by the network output.\n",
        "\n",
        "This sampling can be done using the `np.multinomial` function which implements a **multinomial distribution**. A function that implements this **soft** text generation is defined below:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_soft(model,size=100,start='Today ',temperature=1.0):\n",
        "        inp = tokenizer.texts_to_sequences([start])[0]\n",
        "        chars = inp\n",
        "        for i in range(size):\n",
        "            out = model(tf.expand_dims(tf.one_hot(inp,vocab_size),0))[0][-1]\n",
        "            probs = tf.exp(tf.math.log(out)/temperature).numpy().astype(np.float64)\n",
        "            probs = probs/np.sum(probs)\n",
        "            nc = np.argmax(np.random.multinomial(1,probs,1))\n",
        "            if nc==eos_token:\n",
        "                break\n",
        "            chars.append(nc)\n",
        "            inp = inp+[nc]\n",
        "        return decode(chars)\n",
        "\n",
        "words = ['Today ','On Sunday ','Moscow, ','President ','Little red riding hood ']\n",
        "    \n",
        "for i in [0.3,0.8,1.0,1.3,1.8]:\n",
        "    print(f\"\\n--- Temperature = {i}\")\n",
        "    for j in range(5):\n",
        "        print(generate_soft(model,size=300,start=words[j],temperature=i))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Temperature = 0.3\n",
            "Today talls release on the profit with US start on Talks\n",
            "On Sunday G to Be Troops Seen Than Arrest Trade (AP)\n",
            "Moscow, CO #39;s Stocks State for Space Space Strike\n",
            "President ADP Game Sales From U.S. Troops For London (AP)\n",
            "Little red riding hood illon #39;s strike in Sadr market\n",
            "\n",
            "--- Temperature = 0.8\n",
            "Today tal cracks of flutter over throws take add US supplies\n",
            "On Sunday ADP #39;s DCE and China urges to Beltight all formated\n",
            "Moscow, centure hiking min-fire power against North Korea\n",
            "President SK Returns to Rest for Dead (Thes creeps (Update5)\n",
            "Little red riding hood ol hy new a will be up, likely to give England\n",
            "\n",
            "--- Temperature = 1.0\n",
            "Today #9;seatunfor down chency #39; snock defensive to ceas shate\n",
            "On Sunday otermade decidewall, Indonesian primories Valudal\n",
            "Moscow, Gb nates fall, 2-oney cup house buy\n",
            "President PIE News Dopor, relaces compatieve with rates\n",
            "Little red riding hood NL athlem in Taiwan, President restirnies\n",
            "\n",
            "--- Temperature = 1.3\n",
            "Today Uddivent homplance croppsine forewalls it 4rents, games stycussmore\n",
            "On Sunday D's Scired Extend It Usse (washingencoms)\n",
            "Moscow, (AFP)\n",
            "President tless Enry \\$1 leagme #39; forces gatalbory\n",
            "Little red riding hood B ustrarony, IM from cla marcedaper g:0-line\n",
            "\n",
            "--- Temperature = 1.8\n",
            "Today tkania fumspureing UTI: SGOine \\$50ph plaim UBduday\n",
            "On Sunday #49;sek SuN, SAOBCOR?,2\n",
            "Moscow, erapchooses, sxizers y6 be inflywart a-valy\n",
            "President last: fefeurkiheskurld cletio undernoudabordtim spitk\n",
            "Little red riding hood ?n RPoch FeekoII\n"
          ]
        }
      ],
      "execution_count": 15,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've introduced one more parameter called **temperature**, which indicates how strongly we should choose higher probability characters over lower probability ones. If the temperature is close to 0, we choose the highest probability character, and when the temperature approaches infinity then all probabilities become equal, and we randomly select the next character. In the example above we can observe that the output becomes meaningless when we increase the temperature too much, and starts cycling when we lower it closer to 0. "
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "conda-env-py37_tensorflow-py",
      "language": "python",
      "display_name": "py37_tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "322def6d9eafe65d19dfaee2fc4a6353a7e975f38a3d058434ac22ddbd6ca8db"
    },
    "kernel_info": {
      "name": "conda-env-py37_tensorflow-py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}