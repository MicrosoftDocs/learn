{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations on completing the last module of this learning path! In this module, you learned about basic TensorFlow concepts such as tensors, variables, automatic differentiation, eager execution, and graph execution. You then used those concepts to re-implement the model, training, testing, and prediction code from module 1, but this time at a lower level, using TensorFlow. You're now prepared to customize your code in case you ever need more flexibility than Keras offers.\n",
        "\n",
        "Here's the complete training, testing, and prediction code, for your future reference:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import gzip\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from typing import Tuple\n",
        "import time\n",
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class NeuralNetwork(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    initializer = tf.keras.initializers.GlorotUniform()\n",
        "    self.W1 = tf.Variable(initializer(shape=(784, 20)))\n",
        "    self.b1 = tf.Variable(tf.zeros(shape=(20,)))\n",
        "    self.W2 = tf.Variable(initializer(shape=(20, 10)))\n",
        "    self.b2 = tf.Variable(tf.zeros(shape=(10,)))\n",
        "\n",
        "  def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "    x = tf.reshape(x, [-1, 784])\n",
        "    x = tf.matmul(x, self.W1) + self.b1\n",
        "    x = tf.nn.relu(x)\n",
        "    x = tf.matmul(x, self.W2) + self.b2\n",
        "    return x\n",
        "\n",
        "\n",
        "labels_map = {\n",
        "    0: 'T-Shirt',\n",
        "    1: 'Trouser',\n",
        "    2: 'Pullover',\n",
        "    3: 'Dress',\n",
        "    4: 'Coat',\n",
        "    5: 'Sandal',\n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker',\n",
        "    8: 'Bag',\n",
        "    9: 'Ankle Boot',\n",
        "  }\n",
        "\n",
        "\n",
        "def read_images(path: str, image_size: int, num_items: int) -> np.ndarray:\n",
        "  with gzip.open(path, 'rb') as file:\n",
        "    data = np.frombuffer(file.read(), np.uint8, offset=16)\n",
        "    data = data.reshape(num_items, image_size, image_size)\n",
        "  return data\n",
        "\n",
        "\n",
        "def read_labels(path: str, num_items: int) -> np.ndarray:\n",
        "  with gzip.open(path, 'rb') as file:\n",
        "    data = np.frombuffer(file.read(num_items + 8), np.uint8, offset=8)\n",
        "    data = data.astype(np.int64)\n",
        "  return data\n",
        "\n",
        "\n",
        "def get_data(batch_size: int) -> Tuple[tf.data.Dataset, tf.data.Dataset]:\n",
        "  image_size = 28\n",
        "  num_train = 60000\n",
        "  num_test = 10000\n",
        "\n",
        "  training_images = read_images('data/FashionMNIST/raw/train-images-idx3-ubyte.gz', image_size, num_train)\n",
        "  test_images = read_images('data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz', image_size, num_test)\n",
        "  training_labels = read_labels('data/FashionMNIST/raw/train-labels-idx1-ubyte.gz', num_train)\n",
        "  test_labels = read_labels('data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz', num_test)\n",
        "\n",
        "  # (training_images, training_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((training_images, training_labels))\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "\n",
        "  train_dataset = train_dataset.map(lambda image, label: (float(image) / 255.0, label))\n",
        "  test_dataset = test_dataset.map(lambda image, label: (float(image) / 255.0, label))\n",
        "\n",
        "  train_dataset = train_dataset.batch(batch_size).shuffle(500)\n",
        "  test_dataset = test_dataset.batch(batch_size).shuffle(500)\n",
        "\n",
        "  return (train_dataset, test_dataset)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def fit_one_batch(X: tf.Tensor, y: tf.Tensor, model: tf.keras.Model, loss_fn: tf.keras.losses.Loss, \n",
        "optimizer: tf.keras.optimizers.Optimizer) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_prime = model(X, training=True)\n",
        "    loss = loss_fn(y, y_prime)\n",
        "\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return (y_prime, loss)\n",
        "\n",
        "\n",
        "def fit(dataset: tf.data.Dataset, model: tf.keras.Model, loss_fn: tf.keras.losses.Loss, \n",
        "optimizer: tf.optimizers.Optimizer) -> None:\n",
        "  batch_count = len(dataset)\n",
        "  loss_sum = 0\n",
        "  correct_item_count = 0\n",
        "  current_item_count = 0\n",
        "  print_every = 100\n",
        "\n",
        "  for batch_index, (X, y) in enumerate(dataset):\n",
        "    (y_prime, loss) = fit_one_batch(X, y, model, loss_fn, optimizer)\n",
        "\n",
        "    y = tf.cast(y, tf.int64)\n",
        "    correct_item_count += (tf.math.argmax(y_prime, axis=1) == y).numpy().sum()\n",
        "\n",
        "    batch_loss = loss.numpy()\n",
        "    loss_sum += batch_loss\n",
        "    current_item_count += len(X)\n",
        "\n",
        "    if ((batch_index + 1) % print_every == 0) or ((batch_index + 1) == batch_count):\n",
        "      batch_accuracy = correct_item_count / current_item_count * 100\n",
        "      print(f'[Batch {batch_index + 1:>3d} - {current_item_count:>5d} items] accuracy: {batch_accuracy:>0.1f}%, loss: {batch_loss:>7f}')\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def evaluate_one_batch(X: tf.Tensor, y: tf.Tensor, model: tf.keras.Model, \n",
        "loss_fn: tf.keras.losses.Loss) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  y_prime = model(X, training=False)\n",
        "  loss = loss_fn(y, y_prime)\n",
        "\n",
        "  return (y_prime, loss)\n",
        "\n",
        "\n",
        "def evaluate(dataset: tf.data.Dataset, model: tf.keras.Model, \n",
        "loss_fn: tf.keras.losses.Loss) -> Tuple[float, float]:\n",
        "  batch_count = len(dataset)\n",
        "  loss_sum = 0\n",
        "  correct_item_count = 0\n",
        "  current_item_count = 0\n",
        "\n",
        "  for (X, y) in dataset:\n",
        "    (y_prime, loss) = evaluate_one_batch(X, y, model, loss_fn)\n",
        "\n",
        "    correct_item_count += (tf.math.argmax(y_prime, axis=1).numpy() == y.numpy()).sum()\n",
        "    loss_sum += loss.numpy()\n",
        "    current_item_count += len(X)\n",
        "\n",
        "  average_loss = loss_sum / batch_count\n",
        "  accuracy = correct_item_count / current_item_count\n",
        "  return (average_loss, accuracy)\n",
        "\n",
        "\n",
        "def training_phase():\n",
        "  learning_rate = 0.1\n",
        "  batch_size = 64\n",
        "  epochs = 5\n",
        "\n",
        "  (train_dataset, test_dataset) = get_data(batch_size)\n",
        "\n",
        "  model = NeuralNetwork()\n",
        "\n",
        "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  optimizer = tf.optimizers.SGD(learning_rate)\n",
        "\n",
        "  print('\\nFitting:')\n",
        "  t_begin = time.time()\n",
        "  for epoch in range(epochs):\n",
        "    print(f'\\nEpoch {epoch + 1}\\n-------------------------------')\n",
        "    fit(train_dataset, model, loss_fn, optimizer)\n",
        "  t_elapsed = time.time() - t_begin\n",
        "  print(f'\\nTime per epoch: {t_elapsed / epochs :>.3f} sec' )\n",
        "\n",
        "  print('\\nEvaluating:')\n",
        "  (test_loss, test_accuracy) = evaluate(test_dataset, model, loss_fn)\n",
        "  print(f'Test accuracy: {test_accuracy * 100:>0.1f}%, test loss: {test_loss:>8f}')\n",
        "\n",
        "  model.save_weights('outputs/weights')\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def predict(model: tf.keras.Model, X: np.ndarray) -> tf.Tensor:\n",
        "  y_prime = model(X, training=False)\n",
        "  probabilities = tf.keras.layers.Softmax(axis=1)(y_prime)\n",
        "  predicted_indices = tf.math.argmax(input=probabilities, axis=1)\n",
        "  return predicted_indices\n",
        "\n",
        "\n",
        "def inference_phase():\n",
        "  print('\\nPredicting:')\n",
        "\n",
        "  model = NeuralNetwork()\n",
        "  model.load_weights('outputs/weights')\n",
        "\n",
        "  url = 'https://raw.githubusercontent.com/MicrosoftDocs/tensorflow-learning-path/main/intro-keras/predict-image.png'\n",
        "\n",
        "  with Image.open(requests.get(url, stream=True).raw) as image:\n",
        "    X = np.asarray(image, dtype=np.float32).reshape((-1, 28, 28)) / 255.0\n",
        "\n",
        "  predicted_vector = model.predict(X)\n",
        "  predicted_index = np.argmax(predicted_vector)\n",
        "  predicted_name = labels_map[predicted_index]\n",
        "\n",
        "  print(f'Predicted class: {predicted_name}')\n",
        "\n",
        "\n",
        "training_phase()\n",
        "inference_phase()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fitting:\n",
            "\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "[Batch 100 -  6400 items] accuracy: 62.9%, loss: 0.675659\n",
            "[Batch 200 - 12800 items] accuracy: 68.4%, loss: 0.427568\n",
            "[Batch 300 - 19200 items] accuracy: 71.7%, loss: 0.630323\n",
            "[Batch 400 - 25600 items] accuracy: 73.5%, loss: 0.448896\n",
            "[Batch 500 - 31968 items] accuracy: 74.9%, loss: 0.473284\n",
            "[Batch 600 - 38368 items] accuracy: 75.8%, loss: 0.420568\n",
            "[Batch 700 - 44768 items] accuracy: 76.6%, loss: 0.495648\n",
            "[Batch 800 - 51168 items] accuracy: 77.2%, loss: 0.811230\n",
            "[Batch 900 - 57568 items] accuracy: 77.6%, loss: 0.714009\n",
            "[Batch 938 - 60000 items] accuracy: 77.8%, loss: 0.662069\n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "[Batch 100 -  6400 items] accuracy: 83.0%, loss: 0.513966\n",
            "[Batch 200 - 12800 items] accuracy: 82.8%, loss: 0.350917\n",
            "[Batch 300 - 19200 items] accuracy: 82.9%, loss: 0.503186\n",
            "[Batch 400 - 25600 items] accuracy: 82.9%, loss: 0.464897\n",
            "[Batch 500 - 32000 items] accuracy: 83.0%, loss: 0.417334\n",
            "[Batch 600 - 38400 items] accuracy: 83.0%, loss: 0.582050\n",
            "[Batch 700 - 44800 items] accuracy: 83.2%, loss: 0.409937\n",
            "[Batch 800 - 51200 items] accuracy: 83.2%, loss: 0.384605\n",
            "[Batch 900 - 57568 items] accuracy: 83.3%, loss: 0.557697\n",
            "[Batch 938 - 60000 items] accuracy: 83.3%, loss: 0.397784\n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "[Batch 100 -  6400 items] accuracy: 84.2%, loss: 0.361334\n",
            "[Batch 200 - 12800 items] accuracy: 84.2%, loss: 0.380039\n",
            "[Batch 300 - 19200 items] accuracy: 84.4%, loss: 0.393137\n",
            "[Batch 400 - 25600 items] accuracy: 84.2%, loss: 0.194664\n",
            "[Batch 500 - 32000 items] accuracy: 84.3%, loss: 0.456724\n",
            "[Batch 600 - 38400 items] accuracy: 84.4%, loss: 0.601021\n",
            "[Batch 700 - 44800 items] accuracy: 84.4%, loss: 0.381207\n",
            "[Batch 800 - 51168 items] accuracy: 84.4%, loss: 0.545259\n",
            "[Batch 900 - 57568 items] accuracy: 84.5%, loss: 0.482320\n",
            "[Batch 938 - 60000 items] accuracy: 84.5%, loss: 0.360190\n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "[Batch 100 -  6400 items] accuracy: 85.7%, loss: 0.428344\n",
            "[Batch 200 - 12800 items] accuracy: 85.6%, loss: 0.643802\n",
            "[Batch 300 - 19200 items] accuracy: 85.4%, loss: 0.569402\n",
            "[Batch 400 - 25600 items] accuracy: 85.3%, loss: 0.445594\n",
            "[Batch 500 - 32000 items] accuracy: 85.3%, loss: 0.406925\n",
            "[Batch 600 - 38400 items] accuracy: 85.2%, loss: 0.447160\n",
            "[Batch 700 - 44768 items] accuracy: 85.3%, loss: 0.582219\n",
            "[Batch 800 - 51168 items] accuracy: 85.3%, loss: 0.379701\n",
            "[Batch 900 - 57568 items] accuracy: 85.3%, loss: 0.366390\n",
            "[Batch 938 - 60000 items] accuracy: 85.3%, loss: 0.519495\n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "[Batch 100 -  6400 items] accuracy: 85.4%, loss: 0.385170\n",
            "[Batch 200 - 12800 items] accuracy: 86.0%, loss: 0.340800\n",
            "[Batch 300 - 19200 items] accuracy: 85.8%, loss: 0.414918\n",
            "[Batch 400 - 25600 items] accuracy: 85.8%, loss: 0.246535\n",
            "[Batch 500 - 32000 items] accuracy: 86.0%, loss: 0.443926\n",
            "[Batch 600 - 38368 items] accuracy: 86.0%, loss: 0.451538\n",
            "[Batch 700 - 44768 items] accuracy: 85.9%, loss: 0.359621\n",
            "[Batch 800 - 51168 items] accuracy: 85.9%, loss: 0.340068\n",
            "[Batch 900 - 57568 items] accuracy: 85.9%, loss: 0.294357\n",
            "[Batch 938 - 60000 items] accuracy: 85.9%, loss: 0.444298\n",
            "\n",
            "Time per epoch: 4.345 sec\n",
            "\n",
            "Evaluating:\n",
            "Test accuracy: 84.8%, test loss: 0.427533\n",
            "\n",
            "Predicting:\n",
            "Predicted class: Ankle Boot\n"
          ]
        }
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "322def6d9eafe65d19dfaee2fc4a6353a7e975f38a3d058434ac22ddbd6ca8db"
    },
    "kernelspec": {
      "name": "conda-env-py37_tensorflow-py",
      "language": "python",
      "display_name": "py37_tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "conda-env-py37_tensorflow-py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}