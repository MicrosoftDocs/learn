{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part of the training process requires calculating derivatives that involve tensors. So let's learn about TensorFlow's built-in [automatic differentiation](https://www.tensorflow.org/guide/autodiff) engine, using a very simple example. Consider the following two tensors:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  U =\n",
        "  \\begin{bmatrix}\n",
        "    1 & 2\n",
        "  \\end{bmatrix}\n",
        "  &&\n",
        "  V =\n",
        "  \\begin{bmatrix}\n",
        "    3 & 4 \\\\\n",
        "    5 & 6\n",
        "  \\end{bmatrix}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Now suppose that you want to multiply $U$ by $V$, and then sum all the values in the resulting tensor, such that the result is a scalar. In math notation, you might represent this as the following scalar function $f$:\n",
        "\n",
        "$$\n",
        "f(U, V) = \\mathrm{sum} (U \\, V) = \\sum_j \\sum_i u_i \\, v_{ij}\n",
        "$$\n",
        "\n",
        "Your goal is to calculate the derivative of $f$ with respect to each of its inputs: $\\frac{\\partial f}{\\partial U}$ and $\\frac{\\partial f}{\\partial V}$. Start by creating the two tensors $U$ and $V$. Then create a [tf.GradientTape](https://www.tensorflow.org/guide/autodiff#gradient_tapes), and tell TensorFlow to watch for mathematical operations involving $U$ and $V$, recording those operations onto your *tape*. The tape then lets you calculate the derivatives of the function $f$ with respect to $U$ and $V$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decimal points in tensor values ensure they are floats, which automatic differentiation requires.\n",
        "U = tf.constant([[1., 2.]])\n",
        "V = tf.constant([[3., 4.], [5., 6.]])\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  tape.watch(U)\n",
        "  tape.watch(V)\n",
        "  W = tf.matmul(U, V)\n",
        "  f = tf.math.reduce_sum(W)\n",
        "\n",
        "print(tape.gradient(f, U)) # df/dU\n",
        "print(tape.gradient(f, V)) # df/dV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TensorFlow automatically watches tensors that are defined as `Variable` instances. So let's turn `U` and `V` into variables, and remove the `watch` calls:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decimal points in tensor values ensure they are floats, which automatic differentiation requires.\n",
        "U = tf.Variable(tf.constant([[1., 2.]]))\n",
        "V = tf.Variable(tf.constant([[3., 4.], [5., 6.]]))\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  W = tf.matmul(U, V)\n",
        "  f = tf.math.reduce_sum(W)\n",
        "\n",
        "print(tape.gradient(f, U)) # df/dU\n",
        "print(tape.gradient(f, V)) # df/dV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you'll see later, in deep learning, you need to calculate the derivatives of the loss function with respect to the model parameters. Those parameters are variables because they change during training. Therefore, the fact that variables are automatically watched is handy in this scenario.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional explanation of the math\n",
        "\n",
        "Let's take a look at the math used to compute the derivatives. You only need to understand matrix multiplication and partial derivatives to follow along, but if the math isn't as interesting to you, feel free to skip to the next notebook.\n",
        "\n",
        "Start by thinking of $U$ and $V$ as generic 1 &times; 2 and 2 &times; 2 matrices:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  U =\n",
        "  \\begin{bmatrix}\n",
        "    u_1 & u_2\n",
        "  \\end{bmatrix}\n",
        "  &&\n",
        "  V =\n",
        "  \\begin{bmatrix}\n",
        "    v_{11} & v_{12} \\\\\n",
        "    v_{21} & v_{22}\n",
        "  \\end{bmatrix}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Then the scalar function $f$ can be written as:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  f(U, V)\n",
        "  &= \\mathrm{sum}(U \\, V) \\\\\n",
        "  &= \\mathrm{sum} \n",
        "    \\left( \n",
        "      \\begin{bmatrix}\n",
        "        u_1 & u_2\n",
        "      \\end{bmatrix}\n",
        "      \\begin{bmatrix}\n",
        "        v_{11} & v_{12} \\\\\n",
        "        v_{21} & v_{22}\n",
        "      \\end{bmatrix}\n",
        "    \\right) \\\\\n",
        "  &= \\mathrm{sum}\n",
        "    \\left(\n",
        "      \\begin{bmatrix}\n",
        "        u_1 v_{11} + u_2 v_{21} & u_1 v_{12} + u_2 v_{22}\n",
        "      \\end{bmatrix}\n",
        "    \\right) \\\\\n",
        "  &= u_1 v_{11} + u_2 v_{21} + u_1 v_{12} + u_2 v_{22}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "You can now calculate the derivatives of $f$ with respect to each of its inputs:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial U} =\n",
        "  \\begin{bmatrix}\n",
        "    \\frac{\\partial f}{\\partial u_1} & \\frac{\\partial f}{\\partial u_2}\n",
        "  \\end{bmatrix} = \n",
        "  \\begin{bmatrix}\n",
        "    v_{11} + v_{12} & v_{21} + v_{22}\n",
        "  \\end{bmatrix} = \n",
        "  \\begin{bmatrix}\n",
        "    7 & 11\n",
        "  \\end{bmatrix} \n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial V} =\n",
        "  \\begin{bmatrix}\n",
        "    \\frac{\\partial f}{\\partial v_{11}} & \\frac{\\partial f}{\\partial v_{12}} \\\\\n",
        "    \\frac{\\partial f}{\\partial v_{21}} & \\frac{\\partial f}{\\partial v_{22}} \n",
        "  \\end{bmatrix} = \n",
        "  \\begin{bmatrix}\n",
        "    u_1 & u_1 \\\\\n",
        "    u_2 & u_2\n",
        "  \\end{bmatrix} = \n",
        "  \\begin{bmatrix}\n",
        "    1 & 1 \\\\\n",
        "    2 & 2\n",
        "  \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "As you can see, when you plug in the numerical values of $U$ and $V$, you get the same result as TensorFlow's automatic differentiation.\n"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "a7d8d32a02de2fe32a77a4e581138922e011c09664b6c2991156e76c4176efab"
    },
    "kernel_info": {
      "name": "azureml_py38_PT_and_TF"
    },
    "kernelspec": {
      "display_name": "azureml_py38",
      "language": "python",
      "name": "conda-env-azureml_py38-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
