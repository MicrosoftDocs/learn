Copilot for Data Factory in Microsoft Fabric is an AI-powered assistant designed to simplify and enhance data integration workflows. It acts as a subject-matter expert (SME), enabling you to create, transform, and manage dataflows and pipelines using natural language inputs.

## Copilot for Dataflow Gen2

Copilot for Dataflow Gen2 can enhance your productivity by assisting with several key tasks that often challenge users. For example, many people struggle with writing complex queries or modifying existing ones to fit new requirements. With **Query generation**, you can simply describe what you need in natural language, and Copilot will create or update queries for you. If you need sample data or want to reference existing queries, Copilot streamlines this process, making data preparation faster and more intuitive.

When it comes to **Data transformation**, defining the right steps to filter, aggregate, or reshape data can be confusing, especially for those new to Mashup code. Copilot allows you to specify your transformation needs in plain language, then automatically generates the required Mashup code. This removes the guesswork and helps you achieve accurate results without deep coding expertise.

Understanding complex queries is another common pain point. With **Code explanation**, Copilot breaks down the generated Mashup code, explaining the logic and purpose behind each transformation step. This feature helps you build confidence in your dataflows, learn best practices, and work smarter by focusing on your data goals rather than technical details.

By leveraging these Copilot features, you can overcome common obstacles in data preparation and transformation, making it easier to use Dataflow Gen2 effectively and efficiently.

Here's an example of how you can interact with Copilot in the Dataflow Gen2 editor: 

> [!div class="mx-imgBorder"]
> [![Screenshot of copilot in a Fabric Dataflow Gen2.](../media/copilot-dataflow-gen2.png)](../media/copilot-dataflow-gen2.png#lightbox)

We’ll explore this topic in more detail in a later module.

## Copilot for Data Pipelines

Copilot for Data Pipelines can significantly enhance your productivity by addressing common challenges in pipeline creation and management. Many users find it difficult to design complex pipelines from scratch, especially when they are unsure which activities to include or how to configure them. With **Pipeline generation**, you can simply describe your desired workflow in natural language—for example, "Copy data from Azure SQL to a Lakehouse and send a notification when complete"—and Copilot will automatically generate the necessary Data Pipeline activities. This removes the guesswork and accelerates the setup process.

Troubleshooting errors in pipelines is another area where users often struggle, particularly when error messages are unclear or solutions are not obvious. The **Error troubleshooting** feature in Copilot provides clear explanations of pipeline errors and actionable guidance to resolve issues. For instance, if a pipeline fails due to a missing dataset, Copilot will identify the problem and suggest steps to fix it, helping you maintain smooth workflows and minimize downtime.

Understanding the structure and relationships within a complex pipeline can also be challenging, especially for those new to Data Factory. With **Pipeline summarization**, Copilot can generate a concise overview of your pipeline, describing the activities involved and how they interact. This makes it easier to review, share, and optimize your pipelines, allowing you to work smarter and focus on achieving your data integration goals.

Here's an example of how you can interact with Copilot in the Data Factory pipeline editor:

> [!div class="mx-imgBorder"]
> [![Screenshot of copilot in a Fabric Pipeline.](../media/copilot-data-pipeline.png)](../media/copilot-data-pipeline.png#lightbox)

## Benefits

By using Copilot for Data Factory, you can streamline your data integration processes, reduce errors, and focus on deriving value from your data. Here are some key benefits:

- **Efficiency**: Automates the creation and management of dataflows and pipelines, saving time and effort.  
- **Accessibility**: Enables both citizen and professional data engineers to work effectively using natural language inputs.  
- **Error reduction**: Provides actionable insights and fixes for pipeline errors, reducing downtime.  
- **Collaboration**: Enhances team productivity with clear pipeline summaries and code explanations.