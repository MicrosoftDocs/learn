### YamlMime:ModuleUnit
uid: learn.wwl.detect-mitigate-unfairness-models-with-azure-machine-learning.knowledge-check
title: Knowledge check
metadata:
  title: Knowledge check
  description: "Knowledge check"
  ms.date: 03/15/2021
  author: wwlpublish
  ms.author: gmalc
  ms.topic: interactive-tutorial
  ms.service: azure
azureSandbox: false
durationInMinutes: 3
quiz:
  questions:
  - content: "You are training a binary classification model to support admission approval decisions for a college degree program. How can you evaluate if the model is fair, and doesn't discriminate based on ethnicity?"
    choices:
    - content: "Evaluate each trained model with a validation dataset, and use the model with the highest accuracy score. An accurate model is inherently fair."
      isCorrect: false
      explanation: "Incorrect. An accurate model scores well against the validation dataset, but that does not mean it is fair with regards to sensitive feature groups like ethnicity."
    - content: "Remove the ethnicity feature from the training dataset."
      isCorrect: false
      explanation: "Incorrect. Even if the training dataset doesn't include an explicit ethnicity feature, there may be other correlated features (for example, postal code) that introduce unintentional bias with regard to ethnicity."
    - content: "Compare disparity between selection rates and performance metrics across ethnicities."
      isCorrect: true
      explanation: "Correct. By using ethnicity as a sensitive field, and comparing disparity between selection rates and performance metrics for each ethnicity value, you can evaluate the fairness of the model."
  - content: "You have used Fairlearn to evaluate a model in a notebook. You register the model in your Azure Machine Learning workspace. You want to be able to select the model in Azure Machine Learning studio and from there view its fairness dashboard to compare disparity for performance metrics. What should you do?"
    choices:
    - content: "Run an experiment in which you upload the dashboard metrics for the model."
      isCorrect: true
      explanation: "Correct. Use the upload_dashboard_dictionary function to upload the dashboard metrics. Then you can view the dashboard in the experiment run page in Azure Machine Learning studio, which you can get to from the model"
    - content: "Save the notebook in your Azure Machine Learning workspace."
      isCorrect: false
      explanation: "Incorrect. Saving the notebook would not enable you to view the dashboard in Azure Machine Learning studio by selecting the model."
    - content: "Use the selection_rate_group_summary function to get the fairness data, and save it as a file dataset in your Azure Machine Learning workspace."
      isCorrect: false
      explanation: "Incorrect. The selection_rate_group_summary function retrieves only the selection rates, not performance metrics; and you can't view a dashboard from a dataset."
  - content: "You plan to use the Grid Search mitigation technique to find an optimal model for a binary classifier that predicts whether or not a candidate will be successful in an employment role. You want to ensure that the model selects an equal number of candidates from each category in the Gender feature. Which parity constraint should you use?"
    choices:
    - content: "Demographic parity."
      isCorrect: true
      explanation: "Correct. Demographic parity will try to minimize disparity between selection rates across the gender categories"
    - content: "Error rate parity."
      isCorrect: false
      explanation: "Incorrect. Error rate parity tries to ensure comparable rates of error across sensitive feature groups; which doesn't ensure equal selection rates."
    - content: "Bounded group loss."
      isCorrect: false
      explanation: "Incorrect. Bounded group loss is designed for regression models, not classification models."