Many online sites encourage users to share their views. People trust other people's feedback about products, services, brands, and more. These comments are often frank, insightful, and seen to be free of marketing bias. But not all content is well intended.

Azure AI Foundry Content Safety is an AI service designed to provide a more comprehensive approach to content moderation. Foundry Content Safety helps organizations to prioritize work for human moderators in a growing number of situations:

## Education

The number of learning platforms and online educational sites is growing rapidly, with more and more information being added all the time. Educators need to be sure that students aren't being exposed to inappropriate content, or inputting harmful requests to LLMs. In addition, both educators and students want to know that the content they're consuming is correct and close to the source material.

## Social

Social media platforms are dynamic and fast moving, requiring real-time moderation. Moderation of user-generated content includes posts, comments, and images. Foundry Content Safety helps moderate content that is nuanced and multi-lingual to identify harmful material.

## Brands

Brands are making more use of chat rooms and message forums to encourage loyal customers to share their views. However offensive material can damage a brand, and discourage customers from contributing. They want to be assured that inappropriate material can be quickly identified and removed. Brands are also adding generative AI services to help people to communicate with them, and therefore need to guard against bad actors attempting to exploit large language models (LLMs).

## E-Commerce

User content is generated by reviewing products and discussing products with other people. This material is powerful marketing, but when inappropriate content is posted it damages consumer confidence. In addition, regulatory and compliance issues are increasingly important. Foundry Content Safety helps screen product listings for fake reviews and other unwanted content.

## Gaming

Gaming is a challenging area to moderate due to its highly visual and often violent graphics. Gaming has strong communities where people are enthusiastic about sharing progress and their experiences. Supporting human moderators to keep gaming safe includes monitoring avatars, usernames, images, and text-based materials. Foundry Content Safety has advanced AI vision tools to help moderate gaming platforms to detect misconduct.

## Generative AI services

Organizations are increasingly using generative AI services to enable internal data to be accessed more easily. To maintain the integrity and safety of internal data, both user prompts and AI-generated outputs need to be checked to prevent malicious use of these systems.

## News

News websites need to moderate user comments to prevent the spread of misinformation. Foundry Content Safety can identify language that includes hate speech and other harmful content.

## Other situations

There are many other situations where content needs to be moderated. Foundry Content Safety can be customized to identify problematic language for specific cases.
