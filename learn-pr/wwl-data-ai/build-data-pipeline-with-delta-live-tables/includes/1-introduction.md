A data pipeline is a series of data processing steps. Raw data is ingested from data sources, then it undergoes data processing and transformation, and finally it's stored for analysis.

Lakeflow Declarative Pipelines is a framework within the Databricks Lakehouse Platform for building and running data pipelines in a **declarative** manner. This means you specify what data transformations you want to achieve, and the system automatically figures out how to execute them efficiently, handling many of the complexities of traditional data engineering. It's the evolution of Databricks' Delta Live Tables (DLT), offering a unified approach for both batch and streaming workloads.

Lakeflow Declarative Pipelines has several features for streamlining data engineering tasks and for enhancing data infrastructure reliability. You can define **data quality** rules and *expectations* directly within your pipeline code. The system monitors data quality in real-time, providing visibility and control over your data's integrity. With **Change Data Capture** (CDC), it handles inserts, updates, and deletes automatically in addition to handling out-of-order events.