Lakeflow Declarative Pipelines are a powerful framework provided by Databricks that simplifies the construction and management of reliable data pipelines for big data and machine learning applications. Utilizing Lakeflow Declarative Pipelines, developers can define data transformations declaratively in Python or SQL, which the system automatically orchestrates and manages.

In this module, you learned how to:

- Describe Lakeflow Declarative Pipelines
- Ingest data into materialized views and streaming tables
- Use Lakeflow Declarative Pipelines for Real time Data Processing

## Learn more

- [Lakeflow Declarative Pipelines](/azure/databricks/ddlt/)
- [Load data with Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt/load)