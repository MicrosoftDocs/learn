No matter which governance approach you choose, there are some good practices it should promote.

|**Ways to take action for AI governance**|**Description**|
|-|-|
|**Make resources available** ![Photograph showing person sitting at a desk and reading.](../media/make-resources-available.jpg)| Employees need guidance to learn responsible AI principles and incorporate them into their work. A handbook, a manual, or a training session can fulfill that task. |
|**Create a centralized AI inventory** ![Photograph showing person working next to a whiteboard in an office.](../media/create-centralized-ai-inventory.jpg)| Having a list of all the AI models and systems operating in your organization is key to prioritize efforts and optimize resources. Besides, it's also helpful to make audits and compliance tests easier.|
|**Develop tools** ![Photograph showing people looking at data on a computer screen.](../media/develop-tools.jpg)| Checking compliance in every AI system in your organization can be draining. Consider building tools to automate this task: such tools would monitor and validate systems and raise a flag if anything shifts outside of performance metrics.|

## AI governance engagement

The specific processes and policies for your AI governance system depend on whether your company is using external systems or developing AI in-house. Based on this factor, we provided recommendations to help your company govern your AI engagements.

### Engage with AI systems developers

Your governance processes should reflect how you acquire and build AI capabilities. The following are practical recommendations for two common scenarios—building AI in house and adopting external solutions—with steps you can apply in either case.

**If you’re building AI in house** (developing solutions or integrating AI into your own products and services):

Your governance system should:

- Review or advise on new AI projects before launch, especially sensitive use cases.
- Create clear channels for employees at all levels to raise ethical concerns early.
- Provide guidance to mitigate risks during design, development, and go to market.
- Establish processes to monitor deployed systems for model drift, data quality issues, and performance decay.

Your development teams should:

- Follow detailed standards and checklists that reflect your organization’s ethical principles.
- Use technology-specific guidance (such as for facial recognition or generative AI) and document decisions.

**If you’re adopting external AI solutions** (buying or licensing tools and services):

- Vet vendors for their responsible AI practices and evidence of controls that align with your principles.
- Require documentation on data usage, privacy, security, and model evaluation.
- Define how you'll operate and monitor the solution safely in your environment, for example data access, logging, and escalation.
- Include your policies and requirements in procurement criteria, contracts, and service level agreements.

**For custom integrations or hybrid approaches** (mixing your own development with external components):

- Include your principles and control requirements in requests for proposals and partner agreements.
- Ensure all components—internal and external—are covered by the same governance checks, testing, and monitoring.
- Train users and operators on safe usage, data handling, and how to report issues.

**Across all scenarios**:

- Maintain a central AI inventory to track where systems run, what they do, and who owns them.
- Standardize intake, impact assessment, approval gates, and monitoring so you can scale governance consistently.
- Use automated tools where possible to enforce policies (such as data labeling, access controls, bias checks) and to surface deviations for review.

### Engage with external stakeholders

Responsible AI is a shared endeavor that extends beyond your walls. Collaborating with governments, industry groups, nonprofits, and academic partners help raise standards, align expectations, and scale best practices across the ecosystem.

Organizations can contribute in many ways: joining cross industry initiatives, informing policy, and investing in workforce readiness. For example, Microsoft participates in the **Partnership on AI (PAI)**—a coalition of researchers, nonprofits, and companies dedicated to advancing responsible AI. Such collaborations help shape norms, surface emerging risks, and accelerate practical solutions that benefit society.

Next, discover how an AI governance system works in a real company using Microsoft as an example.
