### YamlMime:ModuleUnit
uid: learn.wwl.optimize-performance-with-spark-and-delta-live-tables.knowledge-check
title: Module assessment
metadata:
  title: Module assessment
  description: "Knowledge check"
  ms.date: 08/22/2025
  author: weslbo
  ms.author: wedebols
  ms.topic: unit
  module_assessment: true
azureSandbox: false
labModal: false
durationInMinutes: 3
quiz:
  questions:
  - content: "What is the primary benefit of combining serverless compute with Delta Live Tables (DLT) in Azure Databricks for large-scale data pipelines?"
    choices:
    - content: "Manual control over fixed cluster sizes for predictable capacity."
      isCorrect: false
      explanation: "Incorrect. Serverless removes most manual provisioning and fixed sizing."
    - content: "Elastic scaling with reduced infrastructure management and parallel execution of transformations."
      isCorrect: true
      explanation: "Correct. Serverless adds elastic, pay-as-you-go scaling while DLT orchestrates parallel transformations, improving performance and reducing ops overhead."
    - content: "Improved user interface customization options for notebooks."
      isCorrect: false
      explanation: "Incorrect. UI customization isn't the focus of serverless or DLT."
  - content: "Which action best enables cost-based optimization (CBO) to choose efficient query plans in Azure Databricks?"
    choices:
    - content: "Run ANALYZE TABLE to refresh table and column statistics regularly."
      isCorrect: true
      explanation: "Correct. Up-to-date statistics guide the optimizer to better execution plans."
    - content: "Disable data partitioning to avoid data skew."
      isCorrect: false
      explanation: "Incorrect. Partitioning often improves performance by pruning irrelevant data."
    - content: "Turn off caching to ensure queries always read from disk."
      isCorrect: false
      explanation: "Incorrect. Caching can speed up repeated queries by avoiding disk I/O."
  - content: "In a Delta Live Tables (DLT) pipeline implementing Change Data Capture (CDC), which pattern should you use to synchronize source changes into a target Delta table?"
    choices:
    - content: "Overwrite the entire target table on each run to guarantee freshness."
      isCorrect: false
      explanation: "Incorrect. Full overwrites are inefficient and unnecessary for CDC."
    - content: "Use a MERGE operation on a business key with whenMatchedUpdateAll and whenNotMatchedInsertAll."
      isCorrect: true
      explanation: "Correct. MERGE applies inserts/updates efficiently, keeping the target in sync with the source."
    - content: "Apply a cross join to replicate rows across all partitions."
      isCorrect: false
      explanation: "Incorrect. Cross joins don't capture row-level changes and are inefficient."

