Large-scale data analytics solutions combine conventional data warehousing used to support business intelligence (BI) with *data lakehouse* techniques that are used to integrate data from files and external sources. A conventional data warehousing solution typically involves copying data from transactional data stores into a relational database with a schema that's optimized for querying and building multidimensional models. Data lakehouse solutions on the other hand, are used with large volumes of data in multiple formats, which is batch loaded or captured in real-time streams and stored in a *data lake* from which distributed processing engines like Apache Spark are used to process it. 

## Learning objectives

In this module, you will learn how to:

- Identify common elements of a large-scale data analytics solution
- Describe key features for data ingestion pipelines
- Identify common types of analytical data store
- Identify platform-as-a-service (PaaS) analytics services in Azure
- Provision Azure Synapse Analytics and use it to ingest, process, and query data
- Describe features of Microsoft Fabric - a software-as-a-service (SaaS) solution for data analytics
- Use Microsoft Fabric to ingest and analyze data
