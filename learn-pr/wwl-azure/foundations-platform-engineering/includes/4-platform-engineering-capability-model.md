Platform engineering is meant to be a journey. A gradual, iterative approach is generally more effective than attempting a large-scale, immediate implementation or relying solely on top-down mandates. Incremental progress, starting with minimally viable products (MVPs), allows teams to refine their approach over time while incorporating feedback along the way.

The platform engineering lifecycle represents a structured approach to ensuring a platform is reliable, scalable, and continuously improving. This lifecycle encompasses distinct stages, each contributing to the platform's long-term success.

An essential element of the lifecycle is the **Platform Engineering Capability Model**, which provides a comprehensive framework for assessing, planning, and implementing platform engineering efforts. The model outlines maturity levels, best practices, and critical capabilities required at each stage of the lifecycle, ensuring alignment with organizational goals and user needs.

The model outlines the progress of maturing platform engineering practices across five stages: **Initial**, **Repeatable**, **Defined**, **Managed**, and **Optimizing**. In the **Initial** stage, organizations have limited structure, with ad hoc processes and minimal investment in platform capabilities. As they progress to the **Repeatable** stage, basic processes emerge, but adoption and governance remain inconsistent. The **Defined** stage marks the establishment of clear standards and processes, with users beginning to adopt platform solutions intentionally. In the **Managed** stage, platforms are actively governed, resources are provisioned and managed efficiently, and user interactions are consistent through standardized interfaces. Finally, in the **Optimizing** stage, platforms are continuously improved through robust feedback mechanisms, measured outcomes, and adaptive capabilities aligned with user needs and organizational goals.

The model is evaluated based on six capabilities: **Investment**, reflecting the allocation of resources and funding; **Adoption**, focused on user discovery and utilization; **Governance**, ensuring resource accessibility, cost control, and data/IP protection; **Provisioning & Management**, defining how resources are deployed and maintained; **Interfaces**, addressing user interactions with the platform; and **Measurement & Feedback**, emphasizing continuous improvement through performance metrics and user insights. Together, these capabilities closely align with the key areas outlined in the [Cloud Native Computing Foundation's platform engineering maturity model](https://tag-app-delivery.cncf.io/whitepapers/platform-eng-maturity-model/#model-table) and reflect the level of the organization's platform engineering maturity.

To use the Platform Engineering Capability Model, first assess where your organization currently stands in each of the six capability areas. You can perform this assessment manually or complete [the Platform Engineering Capability Model survey](https://aka.ms/PECapabilitySurvey). Once you've identified your current stages set future goals for growth and chart your organization's progress for each capability. The progress doesn’t need to happen across all capabilities at once. Focus on the areas that make the most sense for your organization.

:::image type="content" source="../media/platform-engineering-key-drivers-be56c898.png" alt-text="Diagram showing key drivers and stages of the Platform Engineering Capability Model.":::

### Investment

As the Investment capability evolves through each stage, its focus is on how staff and funds are allocated to platform capabilities, with an emphasis on budget and staffing, scope management, and measuring return on investment (ROI).

- Initial (Voluntary): Platform capabilities emerge out of necessity, driven by individual engineers voluntarily addressing immediate tactical needs. Budget and staffing are minimal, with work typically unfunded and performed alongside existing responsibilities. Solutions are narrowly scoped, targeting specific issues with limited knowledge sharing across teams. ROI is measured by how effectively immediate requirements are addressed and their impact on core project outcomes.
- Repeatable (Ad-hoc Contributions): Dedicated teams begin to address recurring challenges, such as inconsistent provisioning or security gaps, but efforts remain largely reactive. Budgets and staffing are limited to cross-cutting concerns, with constrained empowerment across the organization. Scope management focuses on specific issues without a broader platform-wide perspective. ROI is gauged by improvements in addressing key challenges, such as backlog reduction.
- Defined (Operationalized - Dedicated Team): Centrally funded platform teams emerge, focusing on accelerating software delivery and addressing technical requirements. Leadership begins fostering collaboration and implementing initial DevOps practices, but challenges remain in measuring team value. Budget and staffing are formalized for central teams to meet technical needs. Solutions become broader, addressing common challenges across teams, although the focus remains to be short term. ROI is measured by gains in delivery speed.
- Managed (Scalable - As Product): A cultural shift occurs, treating developers as customers with leadership emphasizing empathy and a product-led approach. Platform teams operate like product teams, staffed with developers, product managers, and user experience experts. Scope management aligns with product roadmaps, reviewed collaboratively with engineering teams to meet organization-wide needs. ROI is assessed through enhanced developer satisfaction, reflecting continuous improvements and alignment with user needs.
- Optimizing (Enabled Ecosystem): Investment focuses on innovation, maintaining platform relevance with contributions encouraged across the organization. Platform teams introduce advanced capabilities, such as security and performance enhancements, enabling product teams to build without relying on a centralized backlog. Budgets extend beyond central teams, with funding available across the organization. Scope management emphasizes enabling rapid, organization-wide knowledge sharing. ROI is measured through sustained improvements in developer satisfaction.

### Adoption

The Adoption capability focuses on how users discover and use your platform engineering solutions and their offerings, reflected by the discovery, selection, and usage of services, tools, and technologies. As organizations mature, the approach to adoption shifts from informal and sporadic usage to a more structured and participatory model where users actively engage with the platform, contribute to its evolution. This progression reflects how user discovery, decision-making, and usage practices evolve over time, from initial informal discovery to full participation in the platform’s development.

- Initial (Informal): Adoption is inconsistent, with teams independently improving processes without organization-wide coordination. External tools are often preferred over internal ones. Platforms are discovered informally, mainly through word-of-mouth or chance encounters, with engineering teams selecting services based on their specific needs. Each team maintains its own scripts and tools tailored to its unique requirements.
- Repeatable (Mandated): The organization mandates the use of shared platforms, but capabilities are limited to common use cases, making it difficult to accommodate unusual requirements. User discovery relies on platform team guidance, often through internal documentation or directives. Teams may select mandated services through informal discussions with the platform team. Despite processes being built around platform standards, teams may not fully adopt them or may be unsatisfied with the results.
- Defined (Advertised): Platform capabilities are actively promoted, aligning with team needs. The platform team collaborates with engineering teams to offer high-quality services that reduce operational overhead. However, some teams might still experience low ROI due to reliance on outdated practices and technical debt. Teams discovers capabilities through directives covering typical use cases, and the platform team encourages usage through collaboration. Advocacy for platform use also occurs informally through team ambassadors.
- Managed (Value Driven): Product teams recognize and choose platform capabilities for the clear value they provide in reducing cognitive load and offering high-quality services. Platforms are supported by extensive documentation, ergonomic interfaces, and self-service UX for quick provisioning. Teams now prefers internal platforms over building solutions themselves or relying on external providers. Discovery and decision-making are streamlined, with teams using templates, forums, and documentation to fully support platform adoption.
- Optimizing (Participatory): Product teams actively contribute to improving platform capabilities by suggesting new features and fixes. Processes are in place for users to identify requirements and collaborate on contributions. Developer advocates and ambassadors foster an internal community, extending platform ownership to contributors. Platform engineers work closely with product teams to understand needs and suggest new capabilities, empowering users to submit pull requests and engage in reviews.

### Governance

As the Governance capability evolves, its focus is on ensuring that users have access to the resources and capabilities they need, while managing costs, data, and intellectual property. This progression is assessed based on several categories, including defining policies and frameworks, implementing policies, monitoring and mitigating compliance, and managing access. Governance evolves from manual and reactive processes to an integrated, predictive system that balances centralized control with adaptive management for evolving needs.

- Initial (Independent): Governance is manual, relying on centralized control and gatekeeping, which hinders scalability. Developers and security teams work independently, responding reactively to policy violations. Compliance is maintained through minimal standards, with security measures often being added as afterthoughts. Access permissions are granted based on immediate needs, without a standardized process.
- Repeatable (Documented): The organization starts documenting and sharing policies, but these remain basic and inconsistently applied. Governance tools like ticketing systems are introduced to manage policy reviews, but the process remains manual and slow. Audit processes are established but still reactive. Some roles and permissions are standardized, but enforcement remains uneven.
- Defined (Standardized): Governance becomes centralized and standardized to improve consistency and efficiency across all teams. Policies are documented and centrally managed, with some degree of automation in the implementation process. Key governance standards are upheld through regular auditing, and access control is automated with a formal RBAC system, though development teams still have limited control over policy changes.
- Managed (Integrated): Security and compliance are seamlessly integrated into workflows, with automation ensuring policies are consistently applied across systems and teams. Real-time monitoring and advanced analytics help detect and prevent gaps in governance. Policies are embedded into CI/CD pipelines, and access management is governed by least privilege principles with automated reviews, ensuring a more proactive and integrated approach to governance.
- Optimizing (Predictive): Governance becomes dynamic and context-aware, responding to changing conditions and optimizing access control. Predictive analytics help identify potential risks before they occur, enabling proactive mitigation. Policies are continuously refined using advanced analytics, and access control dynamically adjusts based on real-time factors such as user location and access time, ensuring compliance while enabling tailored workflows.

### Provisioning and Management

With the Provisioning and Management capability, the focus is on how users create, deploy, and manage resources. The process evolves from manual, siloed operations to an adaptive, automated system that balances flexibility with governance, ensuring resources are provisioned efficiently while meeting compliance requirements. This progression spans stages categorized by defining provisioning processes, responding to and managing requests, and monitoring resource allocation.

- Initial (Manual): Developers manually set up infrastructure based on guidance from IT or Architecture teams, leading to inconsistencies and delays. Without standardized processes, requests are reviewed manually, increasing the risk of errors. This approach becomes unsustainable as demand grows, with siloed operations creating inefficiencies.
- Repeatable (Coordinated): The organization begins to centralize provisioning processes by using ticketing systems to manage infrastructure requests. While manual approvals are still required, some errors are reduced, but bottlenecks remain. Teams starts using standard tools for monitoring resources, although the view remains siloed and project-specific.
- Defined (Paved): Provisioning processes are formalized across the organization using Infrastructure as Code (IaC), standardizing templates and tools. Requests are handled through structured workflows, though the platform team may struggle with increasing demand. Centralized dashboards allow for monitoring resource allocation, providing better performance insights.
- Managed (Automated): Provisioning becomes automated and integrated into CI/CD pipelines, minimizing manual effort and ensuring consistent deployments. Governance and compliance checks are embedded into workflows. Automated self-service capabilities allow users to provision resources within controlled parameters. Scaling is automated based on usage patterns to optimize performance.
- Optimizing (Adaptive): Provisioning becomes adaptive, using intelligent systems to anticipate infrastructure needs in real time. This approach ensures efficient resource allocation while maintaining governance and compliance. Systems proactively handle requests, balancing flexibility with governance, while performance and cost-efficiency are optimized through predictive analytics.

### Interfaces

In the Interfaces capability, the primary consideration is how users interact with and consume the platform services and products. Its advancements focus on establishing standards, increasing user autonomy, and seamlessly integrating platform capabilities into existing workflows. The approach evolves from inconsistent, manual processes to a self-service, integrated system that enhances user experience and operational efficiency.

- Initial (Custom Processes): Users interact with the platform through various inconsistent, custom processes that address immediate needs but lack standardization. Engineers independently set up environments by consulting colleagues or relying on personal practices, and they select tools and processes for diagnosing application behavior without any established guidelines. Knowledge sharing is informal, and provisioning services often require deep support from providers due to the lack of formalized processes, which limits scalability and efficiency.
- Repeatable (Local Standards): Engineers and teams begin informally defining standards to enhance knowledge sharing, though consistency remains a challenge due to reliance on individual commitment. Some teams may use documentation or containers to define their setup processes, but these practices diverge over time, requiring effort to reconcile. Diagnosing application behavior becomes more standardized within teams, with some reliance on DevOps or IT teams for access to deployed resources. While local standards emerge, they remain loosely defined and inconsistent across teams.
- Defined (Standard Tooling): Interfaces become more consistent, with the introduction of standardized tooling and documented practices. Central teams manage templates and documentation, with so-called paved roads or golden paths guiding how capabilities should be provisioned and observed. These tools and processes meet broad organizational needs, although expert support is still often required. Teams may modify templates, but changes aren't always integrated back centrally, which can lead to some inefficiencies in maintaining consistency. Diagnosing application behavior follows standardized practices for accessing and analyzing deployed resources, providing greater consistency across teams.
- Managed (Self-Service Solutions): The platform enables greater user autonomy by providing self-service solutions with minimal maintainer support. Users have access to consistent, easy-to-use interfaces that allow them to discover and modify templates, creating a user-centric environment that enhances usability. Tools for diagnosing application behavior and observing resources are made available on demand through the platform, ensuring users have the resources they need without heavy dependence on external teams. Knowledge sharing is facilitated through discovery and modification of templates, which increases the value of platform capabilities.
- Optimizing (Integrated Services): Platform capabilities are seamlessly integrated into the tools and processes that teams already use, such as CLI or IDEs, making them a natural part of users’ workflows. Some capabilities are automatically provisioned based on user needs, and the platform provides flexible building blocks for higher-level use cases that may require deeper customization. Platform teams continuously assess which capabilities are most effective, guiding further investments to optimize platform offerings. The platform automatically sets up observability for deployed applications, offering real-time access to diagnostic data and streamlining the process of monitoring and managing application behavior.

### Measurements and Feedback

The Measurements and Feedback capability involves gathering, analyzing, and incorporating metrics and feedback to assess the success of platform engineering practices. Its maturity is reflected by transitioning from ad-hoc and informal methods to a proactive, data-driven culture where feedback and insights are integrated into continuous improvement processes, guiding strategic decisions and platform development.

- Initial (Ad-hoc): In the initial stage, measurement and feedback processes are inconsistent and fragmented. Metrics are gathered without clear alignment to organizational goals, resulting in incomplete and unreliable data. Feedback is collected informally and often anecdotal, with minimal engagement from stakeholders. As a result, decisions are made based on limited information, and measuring the true ROI of platform engineering practices is difficult. Documentation of feedback and outcomes is minimal, and learnings are rarely captured or shared.
- Repeatable (Structured Processes): Basic feedback mechanisms, such as surveys or forums, are established to capture user experiences more systematically, but these processes still vary across teams. The measurement of success is often focused on activity-based metrics such as deployments or timelines, providing some insight into performance but lacking a broader, outcome-based perspective. Feedback remains informal and bottom-up, though it starts influencing planning. There's some effort to engage stakeholders, but it's still limited, and initial documentation of processes and feedback is created but isn't comprehensive or consistently utilized.
- Defined (Consistent): Feedback collection becomes more formalized and standardized, allowing for deeper insights into user needs and key metrics. Metrics shift towards outcome-based measurements, such as developer productivity, though linking them to financial performance remains a challenge. Feedback analysis is systematic, using both qualitative and quantitative methods, and standard metrics like DORA (DevOps Research and Assessment is a set of metrics that measure software delivery performance, including lead time, deployment frequency, mean time to restore, and change failure rate) or SPACE (Satisfaction and well-being, Performance, Activity, Communication and collaboration, and Efficiency is a framework used to measure developer productivity across these five dimensions) are employed. Regular review sessions with cross-functional teams ensure active engagement with stakeholders. Comprehensive documentation of feedback processes, outcomes, and lessons learned is maintained and shared across teams.
- Managed (Insights): At this stage, feedback mechanisms and measurement frameworks are robust and focused on strategic business outcomes. Data-driven insights guide platform operations, and feedback is integrated into platform roadmaps, driving continuous improvements. Advanced analytics are employed to assess the platform’s impact on business outcomes, such as revenue growth, and feedback is correlated with performance metrics to identify key areas for strategic improvement. Stakeholders across the organization are deeply involved in the feedback process, with structured collaboration to avoid silos. Real-time, dynamic documentation reflects ongoing feedback and lessons learned, accessible to all stakeholders.
- Optimizing (Proactive): Feedback and measurement processes are closely integrated into the organization's culture, creating a proactive approach to anticipating and adapting to future challenges and opportunities. Predictive analytics and advanced metrics are used to forecast future needs and opportunities, enabling the platform to continuously evolve in response to changing conditions. Feedback is fully integrated into a continuous improvement cycle, and a culture of feedback is established across all levels of the organization. Dynamic, real-time documentation reflects ongoing feedback and is continuously updated, ensuring that lessons learned are shared and accessible to all stakeholders.
