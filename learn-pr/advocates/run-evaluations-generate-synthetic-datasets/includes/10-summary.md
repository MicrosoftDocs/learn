In this workshop, you learned how to use the Azure AI Evaluation SDK to run evaluations for different scenarios and how to generate a synthetic dataset. You’re now ready to configure and run your own evaluations for your generative AI apps!

As you continue in your journey in developing AI responsibly, remember to keep responsible AI principles at the forefront of your approach to integrating AI. The generative AI development lifecycle is an iterative process where the measurement phase is critical to iterative development and getting apps into production. You’re encouraged to explore other evaluation scenarios and evolve your own approach to measuring the performance and safety of your AI systems.

> [!NOTE]
> After completing the workshop, if you've finished exploring Azure AI Services, delete the Azure resource that you created during the workshop.

### Learn more

- [Evaluation of generative AI applications](/azure/ai-studio/concepts/evaluation-approach-gen-ai)
- [Evaluation and monitoring metrics for generative AI](/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning)
- [Content risk mitigation strategies with Azure AI](/azure/ai-studio/concepts/evaluation-improvement-strategies)
- [Manually evaluate prompts in Azure AI Foundry chat playground](/azure/ai-studio/how-to/evaluate-generative-ai-app)
- [Evaluate with the Azure AI Evaluation SDK](/azure/ai-studio/how-to/develop/evaluate-sdk)
- [Evaluate with Azure AI Foundry](/azure/ai-studio/how-to/evaluate-generative-ai-app)
- [View evaluation results in Azure AI Foundry](/azure/ai-studio/how-to/evaluate-results)
- [Azure AI Evaluation client library for Python](/python/api/azure-ai-evaluation/azure.ai.evaluation)
