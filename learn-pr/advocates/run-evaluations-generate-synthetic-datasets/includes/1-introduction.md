This workshop contains examples of real-world scenarios for evaluating generative AI outputs for quality and safety. Your goal is to complete the exercises in the [RAI-workshops](https://github.com/azure-samples/RAI-workshops) repository which accompanies this module.

## Learning objectives

By completing this module, you're able to:

- Assess a generative AI app response using performance and quality metrics
- Assess a generative AI app response using risk and safety metrics
- Run an evaluation and track the results in Azure AI Foundry portal
- Create a custom evaluator with Prompty
- Send queries to an endpoint and run evaluators on the resulting query and response
- Generate a synthetic dataset using conversation starters

## Prerequisites

- An Azure subscription â€“ [Create one for free](https://azure.microsoft.com/free/cognitive-services/)
- Familiarity with Azure and the Azure portal
- Ability to understand Python at a beginner level
- Developing in browser:
- [GitHub account](https://docs.github.com/get-started/signing-up-for-github)
- Developing locally:
- [Docker desktop](https://www.docker.com/products/docker-desktop/)
- [Visual Studio Code](https://code.visualstudio.com/)
    - [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)