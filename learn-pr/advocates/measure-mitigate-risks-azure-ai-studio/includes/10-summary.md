In this workshop, you learned how to create and apply a mitigation plan across the four layers of technical mitigations:
 
- User experience
- System message & grounding
- Safety system
- Model 

While manual evaluation enables human graders to spot-check, output performance, assessing your app with AI assistance in Azure AI Foundry portal is a critical step in discovering model vulnerabilities at scale. Safeguarding your app with Azure AI Content Safety enables you to detect and mitigate problematic text and images. 

The next step for your app pre-deployment is to operationalize by creating a deployment and operational readiness plan. Operationalizing includes planning for phased delivery, developing an incident response plan, and ensuring you build features and processes to monitor your AI system and user feedback.

Although the process applied today might look linear, it's an iterative process. As you introduce new features, monitor usage, and/or implement user feedback, you're encouraged to revisit each step in the generative AI development lifecycle.

> [!NOTE]
> After completing the workshop, if you've finished exploring Azure AI Services, delete the Azure resource that you created during the workshop.

### Learn more

- [Infuse responsible AI tools and practices in your LLMops](https://azure.microsoft.com/blog/infuse-responsible-ai-tools-and-practices-in-your-llmops/)
- [Model catalog and collections in Azure AI Foundry](/azure/ai-studio/how-to/model-catalog-overview)
- [Harm categories in Azure AI Content Safety](/azure/ai-services/content-safety/concepts/harm-categories)
- [Evaluation and monitoring metrics for generative AI](/azure/ai-studio/concepts/evaluation-metrics-built-in)
