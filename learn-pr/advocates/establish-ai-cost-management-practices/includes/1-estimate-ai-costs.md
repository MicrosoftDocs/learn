If your organization is enthusiastic about the potential benefits of AI, you might be less concerned about how much AI services cost you. Hopefully your proof-of-concept application is destined for production, which will come with more rigorous processes and expectations about ongoing cost. Cost many even be a factor in your organization deciding to proceed from concept to mission-critical application, as it weighs up the return on investment.

> [!NOTE]
> "Planning and estimating" is a capability in the "Quantify Business Value" domain of the FinOps Foundation's FinOps Framework. This includes estimating the business value of the defined scenario and comparing it to the estimated cost.  

Let's start by estimating the usage and cost of a Large Language Model (LLM) deployment.

## Tokens as a billing unit

In Module 1 Understanding cost management for AI, you learned that language models don't read text like humans do. Instead, the tokenization process breaks up the words into tokens containing about four English characters, recognizing commonly occurring sequences. The number of tokens sent to a large language model (input) and the numbers of tokens in the response generated by the LLM (output) are used in the billing method for Azure AI Foundry models, including Azure OpenAI models. Those services don't charge any additional amounts for the underlying compute infrastructure (CPUs, memory, etc.) that the models are running on.

When you add the Azure OpenAI service to the Microsoft Azure Pricing Calculator, you're presented with the following choices:

- **Region** - where the model is deployed
- **Model Type** - including language models, image models, speech models, etc.
- **Model** - The specific model version, like "GPT 4.1"
- **Pricing Strategy** - Standard (On-Demand), Provisioned (PTU) or Batch (On-Demand)
- **Deployment type** - Global, Data Zone, or Regional

Just like the resource choices in the rest of your application architecture impact your overall AI application cost, service choices impact the price per 1,000 tokens for your model usage.

Next, you need to enter how many input and output tokens you expect your application to process within a month, and in some cases, cached input tokens. Not all models use cached input, where tokens are stored and retrieved without needing to reprocess text. 

## Estimating your token usage

This is where cost estimating may feel more like guessing than actual science. A good approach for a website chatbot scenario is to generate a typical "conversation" between a regular user and your LLM. Then there are a few different methods you can use to convert this conversation into tokens, including JTokkit in Java, and tiktoken in Python.

Say your conversation is 86 input tokens and 587 output tokens. Multiply that by how many visitors your website gets in a month, and what percentage of them you expect may use your new chatbot. Now you have a more informed estimate of your expected token usage, which can be used to estimate the cost of your LLM.

**Conversation input tokens (end user) * Conversation output tokens (LLM response) * (Expected monthly chatbot users)**

If you haven't yet published your application, you may be unsure of how realistic these usage estimates are. It's important to monitor the actual usage metrics from deployment onwards and adjust your forecasted costs if necessary.

Other AI services have other cost considerations - you might need to estimate how many images will be processed, how many minutes of speech will be processed, or what compute instance size you'll need for your Azure Machine Learning resources.
