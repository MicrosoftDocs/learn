holistic approach when interpreting results. Evaluating generative AI applications requires a thorough understanding of various metrics to measure performance and reliability. Azure provides tools for both manual and automated evaluations, including custom evaluators through its AI Foundry and Azure AI Evaluation SDK.

The quality and nature of data are crucial in ensuring effective and reliable AI evaluation processes. High-quality, diverse, and representative datasets allow evaluation metrics to accurately reflect AI model performance across various scenarios, while poor or biased data can mislead outcomes.

Accurately interpreting evaluation results is essential, particularly as AI systems are increasingly deployed in critical domains like healthcare and finance. Proper interpretation goes beyond reading metrics; it involves contextualizing results, considering the AI system’s goals, and comparing them to benchmarks.

Once you have your evaluation results, you’re encouraged to either create or refine your existing mitigation plan based on your assessment. Measuring enables you to implement more targeted mitigations when doing prompt engineering and configuring content filters. As you continue to improve your AI system, be sure to execute subsequent evaluations and refine your approach as needed.

### Learn more

- [Evaluation of generative AI applications](/azure/ai-studio/concepts/evaluation-approach-gen-ai)
- [Evaluation and monitoring metrics for generative AI](/azure/ai-studio/concepts/evaluation-metrics-built-in?tabs=warning)
- [How to evaluate generative AI models and applications with Azure AI Foundry](/azure/ai-studio/how-to/evaluate-generative-ai-app)
- [Evaluate with the Azure AI Evaluation SDK](/azure/ai-studio/how-to/develop/evaluate-sdk)
- [View evaluation results in Azure AI Foundry](/azure/ai-studio/how-to/evaluate-results)
