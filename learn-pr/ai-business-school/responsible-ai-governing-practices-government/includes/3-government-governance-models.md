As discussed previously, each organization has its own guiding principles for responsible AI. But ultimately, those principles need to be part of a larger strategy to be effective.

Businesses and government agencies alike are using their guiding principles as a foundation for a system of internal oversight, or “governance,” that provides guardrails for their AI solutions. Here are a few examples of how governments around the world are approaching AI governance.

## Introduction

Governments play a key role in encouraging AI innovation and responsibility throughout society. They're accountable to citizens to ensure that AI systems across industries don't violate anti-discrimination, data protection, and data privacy laws. Governments have the power to create new laws and invest in research regarding trustworthy AI systems. They're uniquely positioned to bring together stakeholders across business, borders, and academia.

But governments aren't just a referee or coach when it comes to AI in society – they're also a player. Governments around the world are increasingly using AI themselves, from bots that engage citizens to AI models that detect tax compliance trends. As they do so, they're seeing a need to create internal guidelines to ensure those systems are designed and managed responsibly.

Managing internal AI solutions is especially important for government agencies. First of all, governments collect a high concentration of sensitive, personal data, heightening the importance of data security and prudence with AI applications. Plus, unlike some private enterprises, government regulations and services often have a significant impact on people’s lives, and governments are accountable to citizens rather than shareholders.

The oversight, or “governance,” of AI might include developing and enforcing policies for testing, documenting and managing AI systems, or training requirements for the employees who design or use AI systems. For governments, one of the most interesting questions is where the authority to develop and enforce these policies should be placed:

* What kind of structure should the governance system have? (for example, executive body, committee, and appointed or elected members)
  * Should a centralized body set responsible AI policies for all agencies, or should individual agencies have some autonomy to develop their own standards?
* How should AI projects be funded?
* How should the governance system apply to solutions that are purchased from third-party AI vendors?
* What mechanisms should the office have to enforce responsible AI policy?

Here, we’ll share a few examples of how governments around the world are approaching these questions. Governments are some of the largest and most complex organizations on the planet, so it should come as no surprise that there’s no “one size fits all” governance model for engaging with AI responsibly. We recognize that local, regional, and federal agencies need to tailor their AI governance systems to their priorities and culture, and their systems must be flexible enough to evolve as the AI landscape rapidly changes.

## Canada

Canada is one of the first nations to implement a clear governance model for responsible AI. To learn more, we interviewed Cathy Cobey—Global Trusted AI Advisory Leader at EY and a Technical Committee member working with Canada’s CIO Strategy Council to develop standards for data governance and the ethical design and use of automated decision systems. Cathy Cobey explained that Alex Benay, the former Chief Information Officer of Canada and Deputy Minister at the Treasury Board of Canada Secretariat, has conducted extensive work on responsible AI both within and beyond the public sector. Through the CIO Strategic Council, he assembled thought leaders from across the country/region to develop voluntary shared standards for automated decision systems.

When it came to internal oversight, the Treasury Board issued a directive requiring agencies and departments to meet ethical standards when designing automated decision-making systems. Effective April 1, 2020, all federal departments and agencies are required to complete a mandatory impact assessment to determine what safeguards to put in place for their automated decision system. The safeguards, designated for four separate impact levels, are designed to provide greater transparency and quality assurance as the impact level increases. And this applies not only to systems built in-house. To be short-listed as an AI vendor to the Canadian government, third-party vendors are required to disclose how they would help agencies build their AI system within a trusted and ethical framework.<sup>1</sup>

While Benay was a visionary leader in this space, his approach illustrates the benefits of a centralized structure. Through a central office, like the Treasury Board in this case, leaders can set responsible AI policies for the entire government and then rigorously enforce them across all departments. The benefits of this model are that it delivers consistent guidance to all agencies, enables quick decision-making with clear lines of accountability, and allows governments to implement policies at scale and enforce them at every stage of the AI lifecycle.

## The United States

The United States has taken a similar approach to AI governance. We interviewed Monika Wilczak—Managing Director in AI at EY Advisory—to learn more. With over 20 years of experience working in data science and AI, Wilczak has seen the landscape evolve first-hand. Monika Wilczak recounts that in early 2019, the US President issued an Executive Order on Maintaining American Leadership in Artificial Intelligence. The order tasked the National Institute of Standards and Technology (NIST) with establishing guidelines and standards to enable the regulation of AI technologies in consultation with the private sector, academia and non-governmental entities.<sup>2</sup> While still in progress, NIST is working on standards to ensure AI technologies are accurate, reliable, resilient, objective, secure, explainable, safe, and accountable.

The Executive Order also directs federal agencies to prioritize AI investments in their R&D missions. As Wilczak explains, the goal is to give agencies the freedom to explore how AI can be used to solve their unique challenges. In September 2019, as part of the American AI Initiative, the administration released details of nondefense AI R&D investments for each federal agency, showing that these investments address a wide breadth and depth of AI challenges across many federal agencies, programs, and initiatives.

With innovation happening in every corner of the government, interagency collaboration has the potential to accelerate the spread of responsible AI best practices. Coupled with standards like those from the NIST, there's the opportunity to foster consistency and trustworthiness across the government as a whole.

## India

India’s policymakers intend to use AI to solve the country’s/region's pressing social challenges such as income inequality and food insecurity, so they're cautious about overregulating it. NITI Aayog, a government think tank established with the aim of achieving the Sustainable Development Goals, created a national strategy for AI that emphasizes an “AI for all” approach. Anna Roy, advisor to NITI Aayog, says India prefers to keep an “open mind” when it comes to the ethics of AI and focuses on funding academic and commercial research.<sup>3</sup>

India chose to implement a responsible AI governance model that provides their departments with key guiding principles for engaging with AI responsibly, then empowers them to execute their own projects as they see fit. They believe deeply that India can play an important role in testing AI use cases across healthcare, education, agriculture, and more that will fulfill inclusive AI criteria under the “AI for all” mandate.

Because they see the challenges of AI as common to all countries/regions and large corporations alike, NITI Aayog has called for the development of international standards for ethical AI.

## Partnering with other stakeholders

It’s often helpful to partner with stakeholders outside of the government when creating AI governance systems.

As previously noted by Susan Etlinger, it’s important to consult with technology experts like data scientists and machine learning engineers who can speak to the capabilities and limitations of AI. Leading think tanks, policy advisors, and non-governmental organizations should also be included—especially those that focus on underserved communities. It’s helpful to loop in researchers from academic institutions who represent a variety of disciplines such as sociology, economics, philosophy, applied ethics, law, humanities, social psychology, and neuroscience. Finally, it can be helpful to work with business leaders who are utilizing AI at scale and organizations such as the [Organization for Economic Co-operation and Development (OECD)](https://www.oecd.org), [Partnership on AI (PAI)](https://www.partnershiponai.org), and [World Economic Forum (WEF)](https://www.weforum.org).

There's one more important voice for governments to consider: their citizens. Many governments, such as Canada and the UK, are making their responsible AI policies, non-private training data, and source code public, thereby enabling citizens to access, review, and provide their perspective on them. Public feedback gives governments the opportunity to tailor their governance model to reflect citizen priorities. For example, some countries/regions may choose to prioritize rapid innovation over personal privacy, while others may place more emphasis on protecting human rights.

Next, let’s learn more about how governments can approach policy discussions regarding responsible AI.

> [!TIP]
> For more on how enterprises and other organizations are governing their use of responsible AI, see this module: [Identify governing practices for responsible AI](/training/modules/responsible-ai-governing-practices/index).
