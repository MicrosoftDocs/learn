Now that you successfully implemented Cosmos DB, you need to determine how you can integrate it with Azure IoT services. You plan to explore its usage of both the hot and cold path options. This exploration makes it easier to account for smart appliance inventory and device telemetry scenarios that are part of your cloud-native application design. You also want to identify other data stores that you might be able to use in your design.

## What are Azure Cosmos DB-specific design considerations?

When you design Azure Cosmos DB database and container hierarchy, the proper choice of the partition key is essential for ensuring optimal performance and efficiency. This choice is relevant in IoT scenarios that typically involve large volumes of streaming data.

When choosing the most suitable partition key, you should consider the usage patterns and the 20-GB limit on the size of an individual logical partition. In general, a best practice is to create a partition key with hundreds or thousands of distinct values. This method leads to the balanced usage of storage and compute resources across the items that are associated with these partition key values. At the same time, the combined size of items that are sharing the same partition key value must not exceed 20 GB.

For example, when collecting IoT data, you might choose to use the **/date** property for telemetry streaming and **/deviceId** for device inventory, if these properties represent the targets of the most common data queries. Alternatively, you can construct a synthetic partition key, such as a concatenation of the values of **/deviceId** and **/date**. Another approach is to append a random number within a designated range at the end of the partition key value. This approach helps ensure the balanced distribution of the workload across multiple partitions. This way, as you load the items into the target collection, you can perform parallel writes across multiple partitions.

## What are data pipelines in IoT scenarios?

A common occurrence in IoT scenarios is the implementation of multiple concurrent data paths, either by partitioning the ingested data stream, or by forwarding data records to multiple pipelines. The corresponding architectural pattern is referred to as Lambda architecture and consists of two distinct types of pipelines.

A fast (hot) processing pipeline:

- Performs real-time processing.
- Analyzes data.
- Displays data content.
- Generates short term, time-sensitive information.
- Triggers corresponding actions, such as alerts.
- Stores the data in an archive.

A slow (cold) processing pipeline:

- Performs more complex analysis, potentially combining data from multiple sources and over an extended period of time.
- Generates artifacts such as reports or machine learning models.

## What is the role of Azure services in implementing IoT pipelines?

IoT systems ingest telemetry generated by a wide range of devices, process and analyze streaming data to derive near real-time insights, and archive data to cold storage for batch analytics. The data path starts with telemetry generated by IoT devices being sent for initial processing to Azure IoT Hub or Azure IoT Central. Both Azure IoT Hub and Azure IoT Central store collected data for a configurable amount of time.

:::image type="content" source="../media/5-iot-central-cloud-integration.png" alt-text="The options for integrating Azure IoT Central with cloud-native applications and Azure services.":::

Azure IoT Hub supports partitioning and message routing, which allow you to designate specific messages for processing, alerting, and remediation tasks by Azure Logic Apps and Azure Functions. The equivalent functionality is available in Azure IoT Central and is based on its custom-configured rules that trigger actions through webhooks. The webhooks can point to Azure Functions, Azure Logic Apps, Microsoft Flow, or your own custom apps. Azure IoT Hub routes also allow you to forward telemetry to an Azure function for initial processing and then forward it to Azure Cosmos DB. Examples of such processing include format conversion or constructing a synthetic partition key. Another potential use of Azure IoT Hub routes involves copying incoming data into Azure Blob Storage or Azure Data Lake. This method provides a low-cost archiving option, with convenient access for batch processing, including Azure Machine Learning data science tasks.

Azure IoT Central offers continuous data export to Azure Event Hubs, Azure Service Bus, and custom webhooks. It's also possible to configure interval-based data export to Azure Blob storage. Azure Functions support bindings for Azure Event Hubs and Azure Service Bus, which you can use to integrate them with Azure Cosmos DB.

With Azure IoT Central, you can provide near real-time insights with its built-in analytics capabilities. For more advanced analytics needs or when using Azure IoT Hub, you can funnel data into Azure Stream Analytics. Azure Stream Analytics supports Azure Cosmos DB SQL API as its output, writing stream processing results as JSON-formatted items into Azure Cosmos DB containers. This implements data archiving and allows for low-latency, ad-hoc queries on unstructured JSON data. The change feed functionality automatically detects new data and changes to existing data. You can process this data by connecting Azure Cosmos DB to Azure Synapse Analytics. After the processing completes, you can load it back to Azure Cosmos DB for more in-depth reporting. It's also possible to use Azure Databricks with Apache Spark streaming to:

- Load data from Azure IoT Hub.
- Process it to deliver real-time analytics.
- Archive it for long-term retention and more reporting to Azure services such as Azure Cosmos DB, Azure Blob storage, or Azure Data Lake.
