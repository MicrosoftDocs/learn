We've covered ROC curves in some depth. We learned they graph how often we mistakenly assign a true label against how often we correctly assign a true label. Each point on the graph represents one threshold that was applied.

We learned how we can use ROC curves to tune our decision threshold in the final model. We also saw how AUC can give us an idea as to how reliant our model is to having the perfect decision threshold. It's also a handy measure to compare two models to one another.

Congratulations on getting so far! As always, now that you have a new technique under your belt, the best you can do for your learning is practice using it on data you care about. By doing so, you'll gain experience and understand nuances that we haven't had time or space to cover here. Good luck!
