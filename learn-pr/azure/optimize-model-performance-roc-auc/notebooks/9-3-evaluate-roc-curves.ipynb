{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b794793",
   "metadata": {},
   "source": [
    "# Exercise: Good and Bad ROC curves\n",
    "\n",
    "In this exercise, we'll make some ROC curves to explain what good and bad ROC curves might look like.\n",
    "\n",
    "The goal of our models is to identify whether each item detected on the mountain is a hiker (`true`) or a tree (`false`). Let's take a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "!pip install statsmodels\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/hiker_or_tree.csv\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection\n",
    "\n",
    "# Load our data from disk\n",
    "df = pandas.read_csv(\"hiker_or_tree.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Split into train and test\n",
    "train, test =  sklearn.model_selection.train_test_split(df, test_size=0.5, random_state=1)\n",
    "\n",
    "# Define a helper function to plot histograms by class\n",
    "def plot_histogram_by_group(data, column, group_column, bins=12):\n",
    "    groups = data[group_column].unique()\n",
    "    for group in groups:\n",
    "        subset = data[data[group_column] == group]\n",
    "        plt.hist(subset[column], bins=bins, alpha=0.5, label=f'{group_column}={group}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'{column} by {group_column}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Histogram for \"height\"\n",
    "plt.hist(test[\"height\"], bins=12, color='skyblue', edgecolor='black')\n",
    "plt.xlabel(\"height\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Height Distribution\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Multiple histograms for \"motion\" and \"texture\" by \"is_hiker\"\n",
    "plot_histogram_by_group(test, \"motion\", \"is_hiker\", bins=12)\n",
    "plot_histogram_by_group(test, \"texture\", \"is_hiker\", bins=12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three visual features: `height`, `motion`, and `texture`. Our goal here is not to optimize a model, but to explore ROC curves, so we'll work with just one at a time.\n",
    "\n",
    "Before diving in, take a look at the preceding distributions. We can see that we should be able to use height to separate hikers from trees quite easily. Motion will be slightly more difficult, presumably because trees blow in the wind, and some hikers are found sitting down. Texture seems much the same for hikers and trees.\n",
    "\n",
    "## A perfect model\n",
    "\n",
    "What would a perfect ROC look like? If we had a perfectly designed model, it would calculate \"0% chance of hiker\" when it saw any tree and \"100% of hiker\" when it saw any hiker. This means that, so long as the decision threshold was > 0% and < 100%, it would have perfect performance. Between these thresholds, the true positive rate would always be 1, and the false positive rate would always be 0. \n",
    "\n",
    "Don't worry about the thresholds of exactly 0 and 1 (100%). At 0, we're forcing a model to return a _False_ value and at 1, we're forcing it to return _True_.\n",
    "\n",
    "It's almost impossible to train a model that is so perfect, but for the sake of learning, let's pretend we've done so, predicting the `is_hiker` label based on `height`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api\n",
    "\n",
    "# Create a fake model that is perfect at predicting labels\n",
    "class PerfectModel:\n",
    "    def predict(self, x):\n",
    "        # The perfect model believes that hikers are all\n",
    "        # under 4m tall\n",
    "        return 1 / (1 + np.exp(80*(x - 4)))\n",
    "    \n",
    "model = PerfectModel()\n",
    "\n",
    "# Plot the model\n",
    "plt.scatter(test[\"height\"], test[\"is_hiker\"], alpha=0.6, label=\"Data\", edgecolor='k')\n",
    "\n",
    "# Create a range of height values for plotting the model's prediction\n",
    "x_vals = np.linspace(test[\"height\"].min(), test[\"height\"].max(), 200)\n",
    "y_vals = model.predict(x_vals)\n",
    "\n",
    "# Plot the model's prediction (trendline)\n",
    "plt.plot(x_vals, y_vals, color='red', label=\"Perfect Model\")\n",
    "\n",
    "plt.xlabel(\"height\")\n",
    "plt.ylabel(\"is_hiker\")\n",
    "plt.title(\"Scatter Plot with Model Trendline\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our red line is our model, and our blue dots are our datapoints. On the y-axis, `0` means tree and `1` means hiker. We can see our perfect model is passing through every single point.\n",
    "\n",
    "Now we want to make an ROC curve for this perfect model. There are automated ways to do this, but we're here to learn! It's not so hard to do ourselves. We just need to break it down into steps.\n",
    "\n",
    "Remember than an ROC curve plots the _true positive rate_ (TPR) against the _false positive rate_ (FPR). Let's make a function that can calculate these for us. If you're rusty on what these terms mean, pay attention to the code comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tpr_fpr(prediction, actual):\n",
    "    '''\n",
    "    Calculates true positive rate and false positive rate\n",
    "\n",
    "    prediction: the labels predicted by the model\n",
    "    actual:     the correct labels we hope the model predicts\n",
    "    '''\n",
    "\n",
    "    # To calculate the true positive rate and true negative rate we need to know\n",
    "    # TP - how many true positives (where the model predicts hiker, and it is a hiker)\n",
    "    # TN - how many true negatives (where the model predicts tree, and it is a tree)\n",
    "    # FP - how many false positives (where the model predicts hiker, but it was a tree)\n",
    "    # FN - how many false negatives (where the model predicts tree, but it was a hiker)\n",
    "\n",
    "    # First, make a note of which predictions were 'true' and which were 'false'\n",
    "    prediction_true = np.equal(prediction, 1)\n",
    "    prediction_false= np.equal(prediction, 0)\n",
    "\n",
    "    # Now, make a note of which correct results were 'true' and which were 'false'\n",
    "    actual_true = np.equal(actual, 1)\n",
    "    actual_false = np.equal(actual, 0)\n",
    "\n",
    "    # Calculate TP, TN, FP, and FN\n",
    "    # The combination of sum and '&' counts the overlap\n",
    "    # For example, TP calculates how many 'true' predictions \n",
    "    # overlapped with 'true' labels (correct answers)\n",
    "    TP = np.sum(prediction_true  & actual_true)\n",
    "    TN = np.sum(prediction_false & actual_false)\n",
    "    FP = np.sum(prediction_true  & actual_false)\n",
    "    FN = np.sum(prediction_false & actual_true)\n",
    "\n",
    "    # Calculate the true positive rate\n",
    "    # This is the proportion of 'hiker' labels that are identified as hikers\n",
    "    tpr = TP / (TP + FN)\n",
    "\n",
    "    # Calculate the false positive rate \n",
    "    # This is the proportion of 'tree' labels that are identified as hikers\n",
    "    fpr = FP / (FP + TN)\n",
    "\n",
    "    # Return both rates\n",
    "    return tpr, fpr\n",
    "\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that to make an ROC curve, we calculate TPR and FPR for a wide range of thresholds. We then plot the TPR on the y-axis and the FPR on the x-axis. \n",
    "\n",
    "First, let's make a short method that can calculate the TPR and FPR for just one decision threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model(model_predict, feature_name, threshold):\n",
    "    '''\n",
    "    Calculates the true positive rate and false positive rate of the model\n",
    "    at a particular decision threshold\n",
    "\n",
    "    model_predict: the model's predict function\n",
    "    feature_name: the feature the model is expecting\n",
    "    threshold: the decision threshold to use \n",
    "    '''\n",
    "\n",
    "    # Make model predictions for every sample in the test set\n",
    "    # What we get back is a probability that the sample is a hiker\n",
    "    # For example, if we had two samples in the test set, we might\n",
    "    # get 0.45 and 0.65, meaning the model says there is a 45% chance\n",
    "    # the first sample is a hiker, and 65% chance the second is a \n",
    "    # hiker\n",
    "    probability_of_hiker = model_predict(test[feature_name])\n",
    "    \n",
    "    # See which predictions at this threshold would say hiker\n",
    "    predicted_is_hiker = probability_of_hiker > threshold\n",
    "\n",
    "    # calculate the true and false positives rates using our\n",
    "    # handy method\n",
    "    return calculate_tpr_fpr(predicted_is_hiker, test.is_hiker)\n",
    "\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use it in a loop to create an ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_roc_curve(model_predict, feature=\"height\"):\n",
    "    '''\n",
    "    This function creates a ROC curve for a given model by testing it\n",
    "    on the test set for a range of decision thresholds. An ROC curve has\n",
    "    the True Positive rate on the x-axis and False Positive rate on the \n",
    "    y-axis\n",
    "\n",
    "    model_predict: The model's predict function\n",
    "    feature: The feature to provide the model's predict function\n",
    "    '''\n",
    "\n",
    "    # Calculate what the true positive and false positive rate would be if\n",
    "    # we had used different thresholds. \n",
    "\n",
    "    #  Make a list of thresholds to try\n",
    "    thresholds = np.linspace(0,1,101)\n",
    "\n",
    "    false_positive_rates = []\n",
    "    true_positive_rates = []\n",
    "\n",
    "    # Loop through all thresholds\n",
    "    for threshold in thresholds:\n",
    "        # calculate the true and false positives rates using our\n",
    "        # handy method\n",
    "        tpr, fpr = assess_model(model_predict, feature, threshold)\n",
    "\n",
    "        # save the results\n",
    "        true_positive_rates.append(tpr)\n",
    "        false_positive_rates.append(fpr)\n",
    "\n",
    "\n",
    "    # Graph the result\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(false_positive_rates, true_positive_rates, color='blue', label='ROC Curve')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')  # Optional: make the plot square\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create an roc curve for our model\n",
    "create_roc_curve(model.predict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are we seeing here?\n",
    "\n",
    "Except for at a threshold of 0, the model always has a true positive rate of 1. It also always has a false positive rate of 0, unless the threshold has been set to 1. Note that because we've drawn a line, it appears that there are intermediate values (such as a FPR of 0.5) but the line is simply deceiving. If you'd like, comment out `fig.update_traces(mode='lines')` in the above cell and rerun to see points rather than lines. \n",
    "\n",
    "Think about it; our model is perfect. Using it, we will always gets all correct answers, putting all points in the top-left corner (unless the threshold is 0 or 1, which effectively mean that we're discarding the model results completely).  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worse than chance\n",
    "\n",
    "As a counter example to understand the ROC curve, lets consider a model that's worse than chance. In fact, this model gets every single answer wrong.\n",
    "\n",
    "This doesn't happen often in the real world, so again, we'll have to fake this model as well. Let's plot this fake model against our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fake model that gets every single answer incorrect\n",
    "class VeryBadModel:\n",
    "    def predict(self, x):\n",
    "        # This model thinks that all people are over 4m tall \n",
    "        # and all trees are shorter\n",
    "        return 1 / (1 + np.exp(-80*(x - 4)))\n",
    "\n",
    "model = VeryBadModel()\n",
    "\n",
    "# Plot the model\n",
    "plt.scatter(test[\"height\"], test[\"is_hiker\"], alpha=0.6, label=\"Data\", edgecolor='k')\n",
    "\n",
    "# Create a range of height values and get the model's predictions\n",
    "x_vals = np.linspace(test[\"height\"].min(), test[\"height\"].max(), 200)\n",
    "y_vals = model.predict(x_vals)\n",
    "\n",
    "# Plot the model's prediction trendline\n",
    "plt.plot(x_vals, y_vals, color='red', label=\"Very Bad Model\")\n",
    "\n",
    "plt.xlabel(\"height\")\n",
    "plt.ylabel(\"is_hiker\")\n",
    "plt.title(\"Scatter Plot with Very Bad Model Trendline\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the red line (model) goes the wrong direction! How will this look on an ROC curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run our code to create the ROC curve\n",
    "create_roc_curve(model.predict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the opposite of the perfect model. Rather than the line reaching the top left of the graph, it reaches the bottom right. This means that the TPR is always 0; it gets nothing right at all. In this particular example, it also always has a false positive rate of 1, so long as the threshold is less than 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A model no better than chance\n",
    "\n",
    "The previous two models we've seen are very unusual. We've learned, though, that we'd like the curve to be as close to the top left of the graph as possible.\n",
    "\n",
    "What would a model look like that does no better than chance? Let's have a look by trying to fit a model to our texture feature. We know this won't work well, because we've seen that hikers and trees have the same range of image textures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api\n",
    "\n",
    "# This is a helper method that reformats the data to be compatible\n",
    "# with this particular logistic regression model \n",
    "prep_data = lambda x:  np.column_stack((np.full(x.shape, 1), x))\n",
    "\n",
    "# Train a logistic regression model to predict hiker based on texture\n",
    "model = statsmodels.api.Logit(train.is_hiker, prep_data(train.texture)).fit()\n",
    "\n",
    "# Plot the model\n",
    "plt.scatter(test[\"texture\"], test[\"is_hiker\"], alpha=0.6, label=\"Data\", edgecolor='k')\n",
    "\n",
    "# Create a smooth range of texture values for the trendline\n",
    "x_vals = np.linspace(test[\"texture\"].min(), test[\"texture\"].max(), 200)\n",
    "y_vals = model.predict(prep_data(x_vals))\n",
    "\n",
    "# Plot the logistic regression model's predicted probabilities\n",
    "plt.plot(x_vals, y_vals, color='red', label=\"Logistic Regression Model\")\n",
    "\n",
    "plt.xlabel(\"texture\")\n",
    "plt.ylabel(\"is_hiker\")\n",
    "plt.title(\"Scatter Plot with Logistic Regression Trendline\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model isn't very good. It doesn't pass through a single data point and probably will do no better than chance. This seems extreme, but when we work with more complicated problems, sometimes it can be hard to find any real pattern in the data. What does this look like on an ROC curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run our code to create the ROC curve\n",
    "create_roc_curve(lambda x: model.predict(prep_data(x)), \"texture\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a diagonal line! Why?\n",
    "\n",
    "Remember that the model couldn't find a way to reliably predict the label from the feature. It is making a range of predictions, but they're essentially guesswork. \n",
    "\n",
    "If we have a threshold of 0.5, about half of our probabilities will be above the threshold, meaning that about half of our predictions are `hiker`. Half of the labels are also hiker, but there's no correlation between the two. This means we'll get about half the predicted `hiker` labels correct (TPR = 0.5). We'll also get about half the predicted `hiker` labels wrong (FPR = 0.5).\n",
    "\n",
    "If we increased the threshold to 0.8, it would predict _hiker_ 80% of the time. Again, because this is random, it would identify about 80% of the hikers correctly (by chance), and but also 80% of the trees as hikers.\n",
    "\n",
    "If we continued this approach for all thresholds, we'd achieve a diagonal line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A realistic model\n",
    "\n",
    "In the real world, we typically achieve models that perform somewhere between between pure chance (a diagonal line) and perfectly (a line that touches the top left corner).\n",
    "\n",
    "Let's finally build a more realistic model. We'll try to predict whether a sample is a hiker or not based on motion. Our model should do OK, but it won't be perfect. This is because hikers sometimes sit still (like trees) and trees sometimes blow in the wind (moving, like a hiker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api\n",
    "\n",
    "# Train a logistic regression model to predict hiker based on motion\n",
    "model = statsmodels.api.Logit(train.is_hiker, prep_data(train.motion), add_constant=True).fit()\n",
    "\n",
    "# Plot the model\n",
    "plt.scatter(test[\"motion\"], test[\"is_hiker\"], alpha=0.6, label=\"Data\", edgecolor='k')\n",
    "\n",
    "# Generate smooth range of motion values for plotting the trendline\n",
    "x_vals = np.linspace(test[\"motion\"].min(), test[\"motion\"].max(), 200)\n",
    "\n",
    "# Predict probabilities using the trained model\n",
    "y_vals = model.predict(prep_data(x_vals))\n",
    "\n",
    "# Plot the logistic regression trendline\n",
    "plt.plot(x_vals, y_vals, color='red', label=\"Logistic Regression Model\")\n",
    "\n",
    "plt.xlabel(\"motion\")\n",
    "plt.ylabel(\"is_hiker\")\n",
    "plt.title(\"Scatter Plot with Logistic Regression Trendline\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model (red line) seems sensible, though we know sometimes it will get answers wrong.\n",
    "\n",
    "Now, let's look at the ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_roc_curve(lambda x: model.predict(prep_data(x)), \"motion\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the curve bulging toward the top left corner, meaning it's working much better than chance.\n",
    "\n",
    "This is a fairly typical ROC curve for an \"easy\" machine-learning problem like this. Harder problems often show the line much more similar to a diagonal line.\n",
    "\n",
    "By contrast, if we ever came across a line that bulged the opposite direction (to the bottom right), we'd know we're doing worse than chance, and something is deeply wrong.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "We got through it! ROC curves can seem difficult at first, particularly due to the terminology with respect to True and False positives. We built one from scratch here to get a feel for how they're working inside. If you found that tough, read through again slowly, and experiment with some of the functions we made. Don't fret; we normally can use existing libraries to do most of this work for us.\n",
    "\n",
    "The most important thing to remember with these curves is that we'd like our line to be as close to the top-left of the graph as possible. A model that can do this is correctly identifying the target (such as hikers) most of the time, without falsely identifying the target (labelling trees as hikers) very often."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda-env-azureml_py38-py"
  },
  "kernelspec": {
   "display_name": "py38_default",
   "language": "python",
   "name": "conda-env-azureml_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
