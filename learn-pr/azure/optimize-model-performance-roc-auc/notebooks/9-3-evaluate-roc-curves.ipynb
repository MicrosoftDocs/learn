{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b794793",
   "metadata": {},
   "source": [
    "# Exercise: Good and Bad ROC curves\n",
    "\n",
    "In this exercise, we will make some ROC curves to explain what good and bad ROC curves might look like.\n",
    "\n",
    "The goal of our models is to identify whether each item detected on the mountain is a hiker (`true`) or a tree (`false`). Let's take a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "!pip install statsmodels\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/hiker_or_tree.csv\n",
    "import graphing # custom graphing code. See our GitHub repo for details\n",
    "import sklearn.model_selection\n",
    "\n",
    "# Load our data from disk\n",
    "df = pandas.read_csv(\"hiker_or_tree.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Split into train and test\n",
    "train, test =  sklearn.model_selection.train_test_split(df, test_size=0.5, random_state=1)\n",
    "\n",
    "# Graph our three features\n",
    "graphing.histogram(test, label_x=\"height\", label_colour=\"is_hiker\", show=True)\n",
    "graphing.multiple_histogram(test, label_x=\"motion\", label_group=\"is_hiker\", nbins=12, show=True)\n",
    "graphing.multiple_histogram(test, label_x=\"texture\", label_group=\"is_hiker\", nbins=12)"
   ]
  },
  {
   "source": [
    "We have three visual features - `height`, `motion`, and `texture`. Our goal here is not to optimize a model, but to explore ROC curves, so we'll work with just one at a time.\n",
    "\n",
    "Before diving into it, take a look at the distributions above. We can see that we should be able to use height to separate hikers from trees quite easily. Motion will be slightly more difficult, presumably because trees blow in the wind, and some hikers are found sitting down. Texture seems much the same for hikers and trees.\n",
    "\n",
    "## A perfect model\n",
    "What would a perfect ROC look like? If we had a perfectly designed model, it would calculate \"0% chance of hiker\" when it saw any tree and \"100% of hiker\" when it saw any hiker. This means that, so long as the decision threshold was > 0% and < 100%, it would have perfect performance. Between these thresholds, the true positive rate would always be 1, and the false positive rate would always be 0. \n",
    "\n",
    "Don't worry about the thresholds of exactly 0 and 1 (100%). At 0 we are forcing a model to return a False value and at 1 we are forcing it to return True.\n",
    "\n",
    "It's almost impossible to train a model that is so perfect, but for the sake of learning, let's pretend we have done so, predicting the `is_hiker` label based on `height`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api\n",
    "\n",
    "# Create a fake model that is perfect at predicting labels\n",
    "class PerfectModel:\n",
    "    def predict(self, x):\n",
    "        # The perfect model believes that hikers are all\n",
    "        # under 4m tall\n",
    "        return 1 / (1 + numpy.exp(80*(x - 4)))\n",
    "    \n",
    "model = PerfectModel()\n",
    "\n",
    "# Plot the model\n",
    "import graphing\n",
    "graphing.scatter_2D(test, trendline=model.predict)"
   ]
  },
  {
   "source": [
    "Our red line is our model, and our blue dots are our datapoints. On the y-axis `0` means tree, and `1` means hiker. We can see our perfect model is passing through every single point.\n",
    "\n",
    "Now we want to make an ROC curve for this perfect model. There are automated ways to do this, but we're here to learn! It's not so hard to do ourselves. We just need to break it down into steps.\n",
    "\n",
    "Remember than an ROC curve plots the _true positive rate_ (TPR) against the _false positive rate_ (FPR). Let's make a function that can calculate these for us. If you're rusty on what these terms mean, pay attention to the code comments:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tpr_fpr(prediction, actual):\n",
    "    '''\n",
    "    Calculates true positive rate and false positive rate\n",
    "\n",
    "    prediction: the labels predicted by the model\n",
    "    actual:     the correct labels we hope the model predicts\n",
    "    '''\n",
    "\n",
    "    # To calculate the true positive rate and true negative rate we need to know\n",
    "    # TP - how many true positives (where the model predicts hiker, and it is a hiker)\n",
    "    # TN - how many true negatives (where the model predicts tree, and it is a tree)\n",
    "    # FP - how many false positives (where the model predicts hiker, but it was a tree)\n",
    "    # FN - how many false negatives (where the model predicts tree, but it was a hiker)\n",
    "\n",
    "    # First, make a note of which predictions were 'true' and which were 'false'\n",
    "    prediction_true = numpy.equal(prediction, 1)\n",
    "    prediction_false= numpy.equal(prediction, 0)\n",
    "\n",
    "    # Now, make a note of which correct results were 'true' and which were 'false'\n",
    "    actual_true = numpy.equal(actual, 1)\n",
    "    actual_false = numpy.equal(actual, 0)\n",
    "\n",
    "    # Calculate TP, TN, FP, and FN\n",
    "    # The combination of sum and '&' counts the overlap\n",
    "    # For example, TP calculates how many 'true' predictions \n",
    "    # overlapped with 'true' labels (correct answers)\n",
    "    TP = numpy.sum(prediction_true  & actual_true)\n",
    "    TN = numpy.sum(prediction_false & actual_false)\n",
    "    FP = numpy.sum(prediction_true  & actual_false)\n",
    "    FN = numpy.sum(prediction_false & actual_true)\n",
    "\n",
    "    # Calculate the true positive rate\n",
    "    # This is the proportion of 'hiker' labels that are identified as hikers\n",
    "    tpr = TP / (TP + FN)\n",
    "\n",
    "    # Calculate the false positive rate \n",
    "    # This is the proportion of 'tree' labels that are identified as hikers\n",
    "    fpr = FP / (FP + TN)\n",
    "\n",
    "    # Return both rates\n",
    "    return tpr, fpr\n",
    "\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "source": [
    "Now remember that to make an ROC curve, we calculate TPR and FPR for a wide range of thresholds. We then plot the TPR on the y-axis and the FPR on the x-axis. \n",
    "\n",
    "First, lets make a short method that can calculate the TPR and FPR for just one decision threshold."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model(model_predict, feature_name, threshold):\n",
    "    '''\n",
    "    Calculates the true positive rate and false positive rate of the model\n",
    "    at a particular decision threshold\n",
    "\n",
    "    model_predict: the model's predict function\n",
    "    feature_name: the feature the model is expecting\n",
    "    threshold: the decision threshold to use \n",
    "    '''\n",
    "\n",
    "    # Make model predictions for every sample in the test set\n",
    "    # What we get back is a probability that the sample is a hiker\n",
    "    # For example, if we had two samples in the test set, we might\n",
    "    # get 0.45 and 0.65, meaning the model says there is a 45% chance\n",
    "    # the first sample is a hiker, and 65% chance the second is a \n",
    "    # hiker\n",
    "    probability_of_hiker = model_predict(test[feature_name])\n",
    "    \n",
    "    # See which predictions at this threshold would say hiker\n",
    "    predicted_is_hiker = probability_of_hiker > threshold\n",
    "\n",
    "    # calculate the true and false positives rates using our\n",
    "    # handy method\n",
    "    return calculate_tpr_fpr(predicted_is_hiker, test.is_hiker)\n",
    "\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "source": [
    "Now we can use it in a loop to create an ROC curve:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_roc_curve(model_predict, feature=\"height\"):\n",
    "    '''\n",
    "    This function creates a ROC curve for a given model by testing it\n",
    "    on the test set for a range of decision thresholds. An ROC curve has\n",
    "    the True Positive rate on the x-axis and False Positive rate on the \n",
    "    y-axis\n",
    "\n",
    "    model_predict: The model's predict function\n",
    "    feature: The feature to provide the model's predict function\n",
    "    '''\n",
    "\n",
    "    # Calculate what the true positive and false positive rate would be if\n",
    "    # we had used different thresholds. \n",
    "\n",
    "    #  Make a list of thresholds to try\n",
    "    thresholds = numpy.linspace(0,1,101)\n",
    "\n",
    "    false_positive_rates = []\n",
    "    true_positive_rates = []\n",
    "\n",
    "    # Loop through all thresholds\n",
    "    for threshold in thresholds:\n",
    "        # calculate the true and false positives rates using our\n",
    "        # handy method\n",
    "        tpr, fpr = assess_model(model_predict, feature, threshold)\n",
    "\n",
    "        # save the results\n",
    "        true_positive_rates.append(tpr)\n",
    "        false_positive_rates.append(fpr)\n",
    "\n",
    "\n",
    "    # Graph the result\n",
    "    # You don't need to understand this code, but essentially we are plotting\n",
    "    # TPR versus FPR as a line plot\n",
    "    # -- Prepare a dataframe, required by our graphing code\n",
    "    df_for_graphing = pandas.DataFrame(dict(fpr=false_positive_rates, tpr=true_positive_rates, threshold=thresholds))\n",
    "    # -- Generate the plot\n",
    "    fig = graphing.scatter_2D(df_for_graphing, x_range=[-0.05,1.05])\n",
    "    fig.update_traces(mode='lines') # Comment our this line if you would like to see points rather than lines\n",
    "    fig.update_yaxes(range=[-0.05, 1.05])\n",
    "\n",
    "    # Display the graph\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Create an roc curve for our model\n",
    "create_roc_curve(model.predict)"
   ]
  },
  {
   "source": [
    "What are we seeing here?\n",
    "\n",
    "Except for at a threshold of 0, the model always has a true positive rate of 1. It also always has a false positive rate of 0, unless the threshold has been set to 1. Note that because we've drawn a line, it appears that there are intermediate values (such as a FPR of 0.5) but the line is simply deceiving. If you would like, comment out `fig.update_traces(mode='lines')` in the above cell and re-run to see points, rather than lines. \n",
    "\n",
    "Think about it - our model is perfect. Using it, we will always gets all answers correct, putting all points are in top left corner (unless the threshold is 0 or 1, which effectively mean that we are discarding the model results completely).  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Worse-than-chance\n",
    "As a counter example to understand the ROC curve, lets consider a model that is worse than chance. In fact, this model gets every single answer wrong.\n",
    "\n",
    "This doesn't happen often in the real world, so again, we will have to fake this model as well. Let's plot this fake model against our data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fake model that gets every single answer incorrect\n",
    "class VeryBadModel:\n",
    "    def predict(self, x):\n",
    "        # This model thinks that all people are over 4m tall \n",
    "        # and all trees are shorter\n",
    "        return 1 / (1 + numpy.exp(-80*(x - 4)))\n",
    "\n",
    "model = VeryBadModel()\n",
    "\n",
    "# Plot the model\n",
    "graphing.scatter_2D(test, trendline=model.predict)"
   ]
  },
  {
   "source": [
    "As you can see, the red line (model) goes the wrong direction! How will this look on an ROC curve?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run our code to create the ROC curve\n",
    "create_roc_curve(model.predict)"
   ]
  },
  {
   "source": [
    "It is the opposite of the perfect model. Rather than the line reaching the top left of the graph, it reaches the bottom right. This means that the TPR is always 0 - it gets nothing right at all. In this particular example, it also always has a false positive rate of 1, so long as the threshold is less than 1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## A model no better than chance\n",
    "\n",
    "The previous two models we've seen are very unusual. We've learned though, that we would like the curve to be as close to the top left of the graph as possible.\n",
    "\n",
    "What would a model be like that does no better than chance? Let's have a look by trying to fit a model to our texture feature. We know this won't work well, because we've seen that hikers and trees have the same range of image textures. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api\n",
    "\n",
    "# This is a helper method that reformats the data to be compatible\n",
    "# with this particular logistic regression model \n",
    "prep_data = lambda x:  numpy.column_stack((numpy.full(x.shape, 1), x))\n",
    "\n",
    "# Train a logistic regression model to predict hiker based on texture\n",
    "model = statsmodels.api.Logit(train.is_hiker, prep_data(train.texture)).fit()\n",
    "\n",
    "# Plot the model\n",
    "graphing.scatter_2D(test, label_x=\"texture\", label_y=\"is_hiker\", trendline=lambda x: model.predict(prep_data(x)))"
   ]
  },
  {
   "source": [
    "Our model is not very good - it doesn't pass through a single data point and probably will do no better than chance. This seems extreme but when we work with more complicated problems, sometimes it can be hard to find any real pattern in the data. What does this look like on an ROC curve?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run our code to create the ROC curve\n",
    "create_roc_curve(lambda x: model.predict(prep_data(x)), \"texture\")"
   ]
  },
  {
   "source": [
    "It's a diagonal line! Why?\n",
    "\n",
    "Remember that the model could not find a way to reliably predict the label from the feature. It is making a range of predictions but they are essentially guesswork. \n",
    "\n",
    "If we have a threshold of 0.5, about half of our probabilities will be above the threshold, meaning that about half of our predictions are `hiker`. Half of the labels are also hiker, but there is no correlation between the two. This means we'll get about half the predicted `hiker` labels correct (TPR = 0.5). We will also get about half the predicted `hiker` labels wrong (FPR = 0.5).\n",
    "\n",
    "If we increased the threshold to 0.8, it would predict _hiker_ 80% of the time. Again, because this is random, it would identify about 80% of the hikers correctly (by chance), and but also 80% of the trees as hikers.\n",
    "\n",
    "If we continued this approach for all thresholds, we would achieve a diagonal line."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## A realistic model\n",
    "\n",
    "In the real world, we typically achieve models that perform somewhere between between pure chance (a diagonal line) and perfectly (a line that touches the top left corner).\n",
    "\n",
    "Let's finally build a more realistic model. We'll try to predict whether a sample is a hiker or not based on motion. Our model should do OK, but it won't be perfect. This is because hikers sometimes sit still (like trees) and trees sometimes blow in the wind (moving, like a hiker)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api\n",
    "\n",
    "# Train a logistic regression model to predict hiker based on motion\n",
    "model = statsmodels.api.Logit(train.is_hiker, prep_data(train.motion), add_constant=True).fit()\n",
    "\n",
    "# Plot the model\n",
    "graphing.scatter_2D(test, label_x=\"motion\", label_y=\"is_hiker\", trendline=lambda x: model.predict(prep_data(x)))"
   ]
  },
  {
   "source": [
    "The model (red line) seems sensible, though we know sometimes it will get answers wrong.\n",
    "\n",
    "Now let's look at the ROC curve:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_roc_curve(lambda x: model.predict(prep_data(x)), \"motion\")"
   ]
  },
  {
   "source": [
    "We can see the curve bulging toward the top left corner, meaning it is working much better than chance.\n",
    "\n",
    "This is a fairly typical ROC curve for an 'easy' machine-learning problem like this. Harder problems often see the line much more similar to a diagonal line.\n",
    "\n",
    "By contrast, if we ever came across a line that bulged the opposite direction - to the bottom right - we would know we're doing worse than chance, and something is deeply wrong.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "We got through it! ROC curves can seem difficult at first, particularly due to the terminology with respect to True and False positives. We built one from scratch though, here to get a feel for how they are working inside. If you found that tough, read through again slowly, and experiment with some of the functions we made. Don't fret - we normally can use existing libraries to do most of this work for us.\n",
    "\n",
    "The mode important thing to remember with these curves is that we would like our line to be as close to the top left of the graph as possible. A model that can do this is correctly identifying the target (such as hikers) most of the time, without falsely identifying the target (labelling trees as hikers) very often."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
    "kernelspec": {
      "name": "conda-env-azureml_py38-py",
      "language": "python",
      "display_name": "py38_default"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "conda-env-azureml_py38-py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
  }