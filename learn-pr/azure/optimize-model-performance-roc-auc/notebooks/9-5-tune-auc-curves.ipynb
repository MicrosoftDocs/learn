{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c227fd28",
   "metadata": {},
   "source": [
    "# Exercise: Tune the area under the curve\n",
    "\n",
    "In this exercise, we will make and compare two models, using ROC curves, and tune one using the area under the curve (AUC).\n",
    "\n",
    "The goal of our models is to identify whether each item detected on the mountain is a hiker (`true`) or a tree (`false`). We will work with our `motion` feature here. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5383dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "!pip install statsmodels\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/hiker_or_tree.csv\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/m2d_make_roc.py\n",
    "import graphing # custom graphing code. See our GitHub repo for details\n",
    "import sklearn.model_selection\n",
    "\n",
    "# Load our data from disk\n",
    "df = pandas.read_csv(\"hiker_or_tree.csv\", delimiter=\"\\\\t\")\n",
    "\n",
    "# Remove features we no longer want\n",
    "del df[\"height\"]\n",
    "del df[\"texture\"]\n",
    "\n",
    "# Split into train and test\n",
    "train, test =  sklearn.model_selection.train_test_split(df, test_size=0.5, random_state=1)\n",
    "\n",
    "# Graph our feature\n",
    "graphing.multiple_histogram(test, label_x=\"motion\", label_group=\"is_hiker\", nbins=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39e4a3",
   "metadata": {},
   "source": [
    "Motion seems associated with hikers more than trees, but not perfectly. Presumably this is because trees blow in the wind, and some hikers are found sitting down.\n",
    "\n",
    "## A logistic regression model and a random forest\n",
    "\n",
    "Let's train the same logistic regression model we used in the previous exercise, as well as a random forest model. Both will try to predict which objects are hikers.\n",
    "\n",
    "First the logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fcb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# This is a helper method that reformats the data to be compatible\n",
    "# with this particular logistic regression model \n",
    "prep_data = lambda x:  numpy.column_stack((numpy.full(x.shape, 1), x))\n",
    "\n",
    "# Train a logistic regression model to predict hiker based on motion\n",
    "lr_model = statsmodels.api.Logit(train.is_hiker, prep_data(train.motion), add_constant=True).fit()\n",
    "\n",
    "# Assess its performance\n",
    "# -- Train\n",
    "predictions = lr_model.predict(prep_data(train.motion)) > 0.5\n",
    "train_accuracy = accuracy_score(train.is_hiker, predictions)\n",
    "\n",
    "# -- Test\n",
    "predictions = lr_model.predict(prep_data(test.motion)) > 0.5\n",
    "test_accuracy = accuracy_score(test.is_hiker, predictions)\n",
    "\n",
    "print(\"Train accuracy\", train_accuracy)\n",
    "print(\"Test accuracy\", test_accuracy)\n",
    "\n",
    "# Plot the model\n",
    "predict_with_logistic_regression = lambda x: lr_model.predict(prep_data(x))\n",
    "graphing.scatter_2D(test, label_x=\"motion\", label_y=\"is_hiker\", title=\"Logistic Regression\", trendline=predict_with_logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc16730",
   "metadata": {},
   "source": [
    "Now our random forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec93912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a random forest model with 50 trees\n",
    "random_forest = RandomForestClassifier(random_state=2,\n",
    "                                       verbose=False)\n",
    "\n",
    "# Train the model\n",
    "random_forest.fit(train[[\"motion\"]], train.is_hiker)\n",
    "\n",
    "# Assess its performance\n",
    "# -- Train\n",
    "predictions = random_forest.predict(train[[\"motion\"]])\n",
    "train_accuracy = accuracy_score(train.is_hiker, predictions)\n",
    "\n",
    "# -- Test\n",
    "predictions = random_forest.predict(test[[\"motion\"]])\n",
    "test_accuracy = accuracy_score(test.is_hiker, predictions)\n",
    "\n",
    "\n",
    "# Train and test the model\n",
    "print(\"Random Forest Performance:\")\n",
    "print(\"Train accuracy\", train_accuracy)\n",
    "print(\"Test accuracy\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a18cb",
   "metadata": {},
   "source": [
    "These models have similar, but not identical, performance on the test set in terms of accuracy.\n",
    "\n",
    "## Create ROC plots\n",
    "\n",
    "Let's create ROC curves for these models. To do this, we will simply import code from the last exercises so that we can focus on what we would like to learn here. If you need a refresher on how these were made, re-read the last exercise.\n",
    "\n",
    "Note that we've made a slight change. Now our method produces both a graph, and the table of numbers we used to create the graph.\n",
    "\n",
    "First let's look at the logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b1526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from m2d_make_roc import create_roc_curve # import our previous ROC code\n",
    "\n",
    "fig, thresholds_lr = create_roc_curve(predict_with_logistic_regression, test, \"motion\")\n",
    "\n",
    "# Uncomment the line below if you would like to see the area under the curve\n",
    "#fig.update_traces(fill=\"tozeroy\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Show the table of results\n",
    "thresholds_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb79b5",
   "metadata": {},
   "source": [
    "We can see our model does better than chance (it is not a diagonal line). Our table shows the false positive rate (_fpr_) and true positive rate (_tpr_) for each threshold.\n",
    "\n",
    "Let's repeat this for our random forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3809a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry about this lambda function. It simply reorganizes \n",
    "# the data into the shape expected by the random forest model, \n",
    "# and calls predict_proba, which gives us predicted probabilities\n",
    "# that the label is 'hiker'\n",
    "predict_with_random_forest = lambda x: random_forest.predict_proba(numpy.array(x).reshape(-1, 1))[:,1]\n",
    "\n",
    "# Create the ROC curve\n",
    "fig, thresholds_rf = create_roc_curve(predict_with_random_forest, test, \"motion\")\n",
    "\n",
    "# Uncomment the line below if you would like to see the area under the curve\n",
    "#fig.update_traces(fill=\"tozeroy\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Show the table of results\n",
    "thresholds_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675807c4",
   "metadata": {},
   "source": [
    "## Area under the curve\n",
    "\n",
    "Our models look quite similar. Which model do we think is best? Let's use _area under the curve_ (AUC) to compare them. We should expect a number larger than 0.5, because these models are both better than chance, but smaller than 1, because they are not perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Logistic regression\n",
    "print(\"Logistic Regression AUC:\", roc_auc_score(test.is_hiker, predict_with_logistic_regression(test.motion)))\n",
    "\n",
    "# Random Forest\n",
    "print(\"Random Forest AUC:\", roc_auc_score(test.is_hiker, predict_with_random_forest(test.motion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef40dc9",
   "metadata": {},
   "source": [
    "By a very thin margin, the logistic regression model comes out on top.\n",
    "\n",
    "Remember, this doesn't mean the logistic regression model will always do a better job than the random forest. It means that the logistic regression model is a slightly better choice for this kind of data, and probably is marginally less reliant on having the perfect decision thresholds chosen.\n",
    "\n",
    "## Decision Threshold Tuning\n",
    "\n",
    "We can also use our ROC information to find the best thresholds to use. We'll just work with our random forest model for this part.\n",
    "\n",
    "First, let's take a look at the rate of True and False positives with the default threshold of 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d7527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out its expected performance at the default threshold of 0.5\n",
    "# We previously obtained this information when we created our graphs\n",
    "row_of_0point5 = thresholds_rf[thresholds_rf.threshold == 0.5]\n",
    "print(\"TPR at threshold of 0.5:\", row_of_0point5.tpr.values[0])\n",
    "print(\"FPR at threshold of 0.5:\", row_of_0point5.fpr.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f89d230",
   "metadata": {},
   "source": [
    "We can expect that, when real hikers are seen, we have an 86% chance of identifying them. When trees or hikers are seen, we have a 16% chance of identifying them as a hiker.\n",
    "\n",
    "Let's say that for our particular situation, we consider obtaining true positive just as important as avoiding a false positive. We don't want to ignore hikers on the mountain, but we also don't want to send our team out into dangerous conditions for no reason. \n",
    "\n",
    "We can find the best threshold by making our own scoring system, and seeing which threshold would get the best result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a05ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how good each threshold is from our TPR and FPR. \n",
    "# Our criteria is that the TPR is as high as possible and \n",
    "# the FPR is as low as possible. We consider them equally important\n",
    "scores = thresholds_rf.tpr - thresholds_rf.fpr\n",
    "\n",
    "# Find the entry with the lowest score according to our criteria\n",
    "index_of_best_score = numpy.argmax(scores)\n",
    "best_threshold = thresholds_rf.threshold[index_of_best_score]\n",
    "print(\"Best threshold:\", best_threshold)\n",
    "\n",
    "# Print out its expected performance\n",
    "print(\"TPR at this threshold:\", thresholds_rf.tpr[index_of_best_score])\n",
    "print(\"FPR at this threshold:\", thresholds_rf.fpr[index_of_best_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d995a0",
   "metadata": {},
   "source": [
    "Our best threshold, with this criteria, is 0.74, not 0.5! This would have us still identify 83% of hikers properly - a slight decrease from 86% - but only mis-identify 3.6% of trees as hikers.\n",
    "\n",
    "If you would like, play with how we are calculating our scores here, and see how the threshold is adjusted.\n",
    "\n",
    "## Summary\n",
    "\n",
    "That's it! Here we've created ROC curves for two different models, using code we wrote in the previous exercise.\n",
    "\n",
    "Visually, they were quite similar, and when we compared them using the area-under-the-curve metric we found that the logistic regression model was marginally better performing.\n",
    "\n",
    "We then used the ROC curve to tune our random forest model, based on criteria specific to our circumstances. Our very simple criteria of `TPR - FPR` let us pick a threshold that was right for us."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda-env-azureml_py38-py"
  },
  "kernelspec": {
   "display_name": "py38_default",
   "language": "python",
   "name": "conda-env-azureml_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
