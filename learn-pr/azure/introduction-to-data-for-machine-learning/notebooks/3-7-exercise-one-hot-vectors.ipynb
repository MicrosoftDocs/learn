{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise: Titanic Dataset - One-Hot Vectors\n",
        "\n",
        "In this unit, we'll build a model to predict who survived the Titanic disaster. We'll practice transforming data between numerical and categorical types, including use of one-hot vectors.\n",
        "\n",
        "## Data prepartion\n",
        "\n",
        "First, we'll open and quickly clean our dataset, like we did in the last unit:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677538087990
        }
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/m0c_logistic_regression.py\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/titanic.csv\n",
        "\n",
        "# Open our dataset from file\n",
        "dataset = pandas.read_csv(\"titanic.csv\", index_col=False, sep=\",\", header=0)\n",
        "\n",
        "# Fill missing cabin information with 'Unknown'\n",
        "dataset[\"Cabin\"].fillna(\"Unknown\", inplace=True)\n",
        "\n",
        "# Remove rows missing Age information\n",
        "dataset.dropna(subset=[\"Age\"], inplace=True)\n",
        "\n",
        "# Remove the Name, PassengerId, and Ticket fields\n",
        "# This is optional; it makes it easier to read our print-outs\n",
        "dataset.drop([\"PassengerId\", \"Name\", \"Ticket\"], axis=1, inplace=True)\n",
        "\n",
        "dataset.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## About Our Model\n",
        "\n",
        "We'll use a model training type known as Logistic Regression, which predicts who survived the Titanic disaster.\n",
        "\n",
        "For this exercise, you don't need to understand logistic regression. We placed the implementation outside this notebook in a method called `train_logistic_regression`. If you're curious, you can read this method in our GitHub repository.\n",
        "\n",
        "The `train_logistic_regression` method\n",
        "\n",
        "1. Accepts our data frame, and a list of features, to include in the model. \n",
        "2. Trains the model.\n",
        "3. Returns a number that states how well the model performs as it predicts passenger survival. **Smaller numbers are better.**\n",
        "\n",
        "## Numerical Only\n",
        "\n",
        "Let's create a model that uses only the numerical features.\n",
        "\n",
        "First, we'll use `Pclass` as an ordinal feature, rather than a one-hot encoded categorical feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677538996452
        }
      },
      "outputs": [],
      "source": [
        "from m0c_logistic_regression import train_logistic_regression\n",
        "\n",
        "features = [\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\"] \n",
        "loss_numerical_only = train_logistic_regression(dataset, features)\n",
        "\n",
        "print(f\"Numerical-Only, Log-Loss (cost): {loss_numerical_only}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have our starting point. Let's see if categorical features will improve the model.\n",
        "\n",
        "## Binary Categorical Features\n",
        "\n",
        "Categorical features with only two potential values - for example, `0` or `1` - can be encoded in a single column.\n",
        "\n",
        "We'll convert `Sex` values into `IsFemale` - a `0` for male and `1` for female - and include that in our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677538422533
        }
      },
      "outputs": [],
      "source": [
        "# Swap male / female with numerical values\n",
        "# We can do this because there are only two categories\n",
        "dataset[\"IsFemale\"] = dataset.Sex.replace({'male':0, 'female':1})\n",
        "\n",
        "# Print out the first few rows of the dataset\n",
        "print(dataset.head())\n",
        "\n",
        "# Run and test the model, also using IsFemale this time\n",
        "features = [\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\", \"IsFemale\"] \n",
        "loss_binary_categoricals = train_logistic_regression(dataset, features)\n",
        "\n",
        "print(f\"\\nNumerical + Sex, Log-Loss (cost): {loss_binary_categoricals}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our loss (error) decreased! This model performs better than the previous model.\n",
        "\n",
        "## One-Hot Encoding\n",
        "\n",
        "Ticket class (`Pclass`) is an Ordinal feature. Its potential values (1, 2 & 3) have an order, and they have equal spacing. However, this even spacing might be incorrect - in stories about the Titanic, the third-class passengers were treated much worse than those in 1st and 2nd class.\n",
        "\n",
        "Let's convert `Pclass` into a categorical feature, with one-hot encoding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677538566508
        }
      },
      "outputs": [],
      "source": [
        "# Get all possible categories for the \"PClass\" column\n",
        "print(f\"Possible values for PClass: {dataset['Pclass'].unique()}\")\n",
        "\n",
        "# Use Pandas to One-Hot encode the PClass category\n",
        "dataset_with_one_hot = pandas.get_dummies(dataset, columns=[\"Pclass\"], drop_first=False)\n",
        "\n",
        "# Add back in the old Pclass column, for learning purposes\n",
        "dataset_with_one_hot[\"Pclass\"] = dataset.Pclass\n",
        "\n",
        "# Print out the first few rows\n",
        "dataset_with_one_hot.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the original `Pclass` data became three values: `Pclass_1`, `Pclass_2` and `Pclass_3`.\n",
        "\n",
        "Rows with `Pclass` values of 1 have a value in the `Pclass_1` column. We see the same pattern for values of 2 and 3.\n",
        "\n",
        "Now, re-run the model, and treat `Pclass` values as categorical values, instead of ordinal values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677538662588
        }
      },
      "outputs": [],
      "source": [
        "# Run and test the model, also using Pclass as a categorical feature this time\n",
        "features = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"IsFemale\",\n",
        "            \"Pclass_1\", \"Pclass_2\", \"Pclass_3\"]\n",
        "\n",
        "loss_pclass_categorical = train_logistic_regression(dataset_with_one_hot, features)\n",
        "\n",
        "print(f\"\\nNumerical, Sex, Categorical Pclass, Log-Loss (cost): {loss_pclass_categorical}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This seems to have made things slightly worse!\n",
        "\n",
        "Let's move on.\n",
        "\n",
        "## Including Cabin\n",
        "\n",
        "Recall that many passengers had `Cabin` information. `Cabin` is a categorical feature and should be a good predictor of survival, because people in lower cabins probably had little time to escape during the sinking.\n",
        "\n",
        "Let's encode cabin using one-hot vectors, and include it in a model. This time, there are so many cabins that we won't print them all out. To practice printing them out, feel free to edit the code for practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677538735962
        }
      },
      "outputs": [],
      "source": [
        "# Use Pandas to One-Hot encode the Cabin and Pclass categories\n",
        "dataset_with_one_hot = pandas.get_dummies(dataset, columns=[\"Pclass\", \"Cabin\"], drop_first=False)\n",
        "\n",
        "# Find cabin column names\n",
        "cabin_column_names = list(c for c in dataset_with_one_hot.columns if c.startswith(\"Cabin_\"))\n",
        "\n",
        "# Print out how many cabins there were\n",
        "print(len(cabin_column_names), \"cabins found\")\n",
        "\n",
        "# Make a list of features\n",
        "features = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"IsFemale\",\n",
        "            \"Pclass_1\", \"Pclass_2\", \"Pclass_3\"] + \\\n",
        "            cabin_column_names\n",
        "\n",
        "# Run the model and print the result\n",
        "loss_cabin_categorical = train_logistic_regression(dataset_with_one_hot, features)\n",
        "\n",
        "print(f\"\\nNumerical, Sex, Categorical Pclass, Cabin, Log-Loss (cost): {loss_cabin_categorical}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's our best result so far!\n",
        "\n",
        "## Improving Power\n",
        "\n",
        "Including very large numbers of categorical classes - for example, 135 cabins - is often not the best way to train a model, because the model has only a few examples of each category class to learn from.\n",
        "\n",
        "Sometimes, we can improve models if we simplify features. `Cabin` probably helped because it indicated the Titanic deck where people were probably located: those in lower decks would have had their quarters flooded first. \n",
        "\n",
        "It might become simpler to use deck information, instead of categorizing people into Cabins. \n",
        "\n",
        "Let's simplify what we have run, replacing the 135 `Cabin` categories with a simpler `Deck` category that has only 9 values: A - G, T, and U (Unknown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677538902120
        }
      },
      "outputs": [],
      "source": [
        "# We have cabin names, like A31, G45. The letter refers to the deck that\n",
        "# the cabin was on. Extract just the deck and save it to a column. \n",
        "dataset[\"Deck\"] = [c[0] for c in dataset.Cabin]\n",
        "\n",
        "print(\"Decks: \", sorted(dataset.Deck.unique()))\n",
        "\n",
        "# Create one-hot vectors for:\n",
        "# Pclass - the class of ticket. (This could be treated as ordinal or categorical)\n",
        "# Deck - the deck that the cabin was on\n",
        "dataset_with_one_hot = pandas.get_dummies(dataset, columns=[\"Pclass\", \"Deck\"], drop_first=False)\n",
        "\n",
        "# Find the deck names\n",
        "deck_of_cabin_column_names = list(c for c in dataset_with_one_hot.columns if c.startswith(\"Deck_\"))\n",
        " \n",
        "features = [\"Age\", \"IsFemale\", \"SibSp\", \"Parch\", \"Fare\", \n",
        "            \"Pclass_1\", \"Pclass_2\", \"Pclass_3\",\n",
        "            \"Deck_A\", \"Deck_B\", \"Deck_C\", \"Deck_D\", \n",
        "            \"Deck_E\", \"Deck_F\", \"Deck_G\", \"Deck_U\", \"Deck_T\"]\n",
        "\n",
        "loss_deck = train_logistic_regression(dataset_with_one_hot, features)\n",
        "\n",
        "print(f\"\\nSimplifying Cabin Into Deck, Log-Loss (cost): {loss_deck}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing Models\n",
        "\n",
        "Let's compare the `loss` for these models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677538909633
        }
      },
      "outputs": [],
      "source": [
        "# Use a dataframe to create a comparison table of metrics\n",
        "# Copy metrics from previous Unit\n",
        "\n",
        "l =[[\"Numeric Features Only\", loss_numerical_only],\n",
        "    [\"Adding Sex as Binary\", loss_binary_categoricals],\n",
        "    [\"Treating Pclass as Categorical\", loss_pclass_categorical],\n",
        "    [\"Using Cabin as Categorical\", loss_cabin_categorical],\n",
        "    [\"Using Deck rather than Cabin\", loss_deck]]\n",
        "\n",
        "pandas.DataFrame(l, columns=[\"Dataset\", \"Log-Loss (Low is better)\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we include categorical features, we can both improve and harm how well a model works. Often, experimentation is the best way to find the best model. \n",
        "\n",
        "## Summary\n",
        "\n",
        "In this unit you learned how to use One-Hot encoding to address categorical data.\n",
        "\n",
        "We also explored how critical thinking about the problem at hand can sometimes improve a solution more than simply including all possible features in a model."
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "conda-env-py38_default-py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
