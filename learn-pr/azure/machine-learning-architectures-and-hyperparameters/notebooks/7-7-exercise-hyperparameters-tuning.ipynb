{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edd76bd1",
   "metadata": {},
   "source": [
    "# Exercise: Random forests and hyperparameters\n",
    "\n",
    "The goal of this unit is to explore how hyperparameters change training, and thus model performance. The line between model architecture and hyperparameters is a bit blurry for random forests, because training itself actually changes the model's architecture by adding or removing branches. \n",
    "\n",
    "We'll again pursue our goal of predicting which crimes in San Francisco will be resolved.\n",
    "\n",
    "## Data and Training Preparation\n",
    "\n",
    "Let's load our data, split it, and prepare for training. This is the same code you've seen in the previous exercises. If you haven't done those, go back and do them now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec3f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This code is exactly the same as what we used in the previous exercises. You don't need to read it again.\n",
    "\n",
    "import pandas\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/san_fran_crime.csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import graphing # custom graphing code. See our GitHub repo for details\n",
    "\n",
    "#Import the data from the .csv file\n",
    "dataset = pandas.read_csv('san_fran_crime.csv', delimiter=\"\\t\")\n",
    "\n",
    "# One-hot encode features\n",
    "dataset = pandas.get_dummies(dataset, columns=[\"Category\", \"PdDistrict\"], drop_first=False)\n",
    "\n",
    "features = [c for c in dataset.columns if c != \"Resolution\"]\n",
    "\n",
    "# Make a utility method that we can re-use throughout this exercise\n",
    "# To easily fit and test out model\n",
    "def fit_and_test_model(model):\n",
    "    '''\n",
    "    Trains a model and tests it against both train and test sets\n",
    "    '''  \n",
    "    global features\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(train[features], train.Resolution)\n",
    "\n",
    "    # Assess its performance\n",
    "    # -- Train\n",
    "    predictions = model.predict(train[features])\n",
    "    train_accuracy = balanced_accuracy_score(train.Resolution, predictions)\n",
    "\n",
    "    # -- Test\n",
    "    predictions = model.predict(test[features])\n",
    "    test_accuracy = balanced_accuracy_score(test.Resolution, predictions)\n",
    "\n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "\n",
    "print(\"Ready!\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not forget to split our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset in an 90/10 train/test ratio. \n",
    "train, test = train_test_split(dataset, test_size=0.1, random_state=2, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria to split on\n",
    "\n",
    "The first hyperparameter with which we'll work is the criterion. This is essentially a kind of cost function that is used to determine whether a node should be split or not. We have two options available in the package that we are using: `gini` and `entropy`. Let's try them both: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Shrink the training set temporarily to explore this\n",
    "# setting with a more normal sample size\n",
    "sample_size = 1000\n",
    "full_trainset = train\n",
    "train = full_trainset[:sample_size]\n",
    "\n",
    "\n",
    "# Prepare the model \n",
    "rf = RandomForestClassifier(n_estimators=10,\n",
    "                            # max_depth=12,\n",
    "                            # max_features=cur_max_features,\n",
    "                            random_state=2,\n",
    "                            criterion=\"gini\", \n",
    "                            verbose=False)\n",
    "# Train and test the result\n",
    "train_accuracy, test_accuracy = fit_and_test_model(rf)\n",
    "# Train and test the result\n",
    "print(train_accuracy, test_accuracy)\n",
    "\n",
    "# Prepare the model \n",
    "rf = RandomForestClassifier(n_estimators=10,\n",
    "                            random_state=2,\n",
    "                            criterion=\"entropy\", \n",
    "                            verbose=False)\n",
    "# Train and test the result\n",
    "train_accuracy, test_accuracy = fit_and_test_model(rf)\n",
    "# Train and test the result\n",
    "print(train_accuracy, test_accuracy)\n",
    "\n",
    "# Roll back the train dataset to the full train set\n",
    "train = full_trainset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are subtly different, and usually only subtly as both criteria use a similar way to assess performance. We suggest you try different sample sizes, such as 50 and 50000, to see how this changes with larger or smaller samples. \n",
    "\n",
    "## Minimum impurity decrease\n",
    "\n",
    "The minimum impurity decrease is another criterion that's used to assess whether a node should be split. It's used by the `gini` or `entropy` algorithms we used previously. If minimum impurity decrease is high, then splitting a node must result in substantial performance improvement. If it's very low, then nodes can be split even if they offer very little to no performance improvements on the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Shrink the training set temporarily to explore this\n",
    "# setting with a more normal sample size\n",
    "full_trainset = train\n",
    "train = full_trainset[:1000] # limit to 1000 samples\n",
    "\n",
    "min_impurity_decreases = np.linspace(0, 0.0005, num=100)\n",
    "\n",
    "# Train our models and report their performance\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(\"Working...\")\n",
    "for min_impurity_decrease in min_impurity_decreases:\n",
    "\n",
    "    # Prepare the model \n",
    "    rf = RandomForestClassifier(n_estimators=10,\n",
    "                                min_impurity_decrease=min_impurity_decrease,\n",
    "                                random_state=2, \n",
    "                                verbose=False)\n",
    "    \n",
    "    # Train and test the result\n",
    "    train_accuracy, test_accuracy = fit_and_test_model(rf)\n",
    "\n",
    "    # Save the results\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "\n",
    "# Plot results\n",
    "graphing.line_2D(dict(Train=train_accuracies, Test=test_accuracies), \n",
    "                    min_impurity_decreases,\n",
    "                    label_x=\"Minimum impurity decreases (min_impurity_decrease)\",\n",
    "                    label_y=\"Accuracy\",\n",
    "                    title=\"Performance\", show=True)\n",
    "\n",
    "# Roll back the train dataset to the full train set\n",
    "train = full_trainset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that _train_ performance drastically reduces as we get more scrict about when a node can be split. This is because the higher the minimum impurity decrease, the more strict we are about growing our tree. The smaller the tree, the less overfitting we'll see.\n",
    "\n",
    "Changes in _test_ performance are more subtle. A small increase above zero appears to increase test performance. Further increases begin to hurt test performance only subtly. \n",
    "\n",
    "This is similar to what we saw in the previous exercise about model size; more complex models (those with more nodes) can fit the training data better, but once they exceed a certain complexity, they begin to overfit.\n",
    "\n",
    "\n",
    "## Maximum number of features\n",
    "\n",
    "When trees are created, they are provided with a subset of the data. This not only means they see a certain collection of rows (samples), but also a certain collection of columns (features). The more features are provided, the more likely a given tree is going to overfit. Let's see what happens when we restrict the maximum number of features that can be provided to each tree in the forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shrink the training set temporarily to explore this\n",
    "# setting with a more normal sample size\n",
    "full_trainset = train\n",
    "train = full_trainset[:1000] # limit to 1000 samples\n",
    "\n",
    "max_features = range(10, len(features) +1)\n",
    "\n",
    "# Train our models and report their performance\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(\"Working...\")\n",
    "for cur_max_features in max_features:\n",
    "    # Prepare the model \n",
    "    rf = RandomForestClassifier(n_estimators=50,\n",
    "                                max_depth=12,\n",
    "                                max_features=cur_max_features,\n",
    "                                random_state=2, \n",
    "                                verbose=False)\n",
    "    \n",
    "    # Train and test the result\n",
    "    train_accuracy, test_accuracy = fit_and_test_model(rf)\n",
    "\n",
    "    # Save the results\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "\n",
    "# Plot results\n",
    "graphing.line_2D(dict(Train=train_accuracies, Test=test_accuracies), \n",
    "                    max_features,\n",
    "                    label_x=\"Maximum number of features (max_features)\",\n",
    "                    label_y=\"Accuracy\",\n",
    "                    title=\"Performance\", show=True)\n",
    "\n",
    "# Roll back the trainset to the full set\n",
    "train = full_trainset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more features we have, the worse the overfitting (gap between blue and red lines). Initial increases from 10-20 provide a minute improvement in test performance, after which it begins to hurt test performance very slightly. As features increase, training performance continues to grow unmatched by test performance, indicating overfitting. An optimal value here would be around 20 features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeding\n",
    "\n",
    "Finally, we come to seeding. When trees are initially made, there's a degree of randomness used to decide which features and samples are provided to which trees. Changing the random state (seed) value changes this initial state.\n",
    "\n",
    "The random seed is not a parameter to be tuned, but we shouldn't forget its effects on our models, particularly when there isn't much data to work with. Let's see how our model behaves with different random states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shrink the training set temporarily to explore this\n",
    "# setting with a more normal sample size\n",
    "sample_size = 1000\n",
    "full_trainset = train\n",
    "train = full_trainset[:sample_size] \n",
    "\n",
    "\n",
    "seeds = range(0,101)\n",
    "\n",
    "# Train our models and report their performance\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for seed in seeds:\n",
    "    # Prepare the model \n",
    "    rf = RandomForestClassifier(n_estimators=10,\n",
    "                                random_state=seed, \n",
    "                                verbose=False)\n",
    "    \n",
    "    # Train and test the result\n",
    "    train_accuracy, test_accuracy = fit_and_test_model(rf)\n",
    "\n",
    "    # Save the results\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "\n",
    "# Plot results\n",
    "graphing.line_2D(dict(Train=train_accuracies, Test=test_accuracies), \n",
    "                    seeds,\n",
    "                    label_x=\"Seed value (random_state)\",\n",
    "                    label_y=\"Accuracy\",\n",
    "                    title=\"Performance\", show=True)\n",
    "\n",
    "# Roll back the trainset to the full set\n",
    "train = full_trainset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance, particularly on the test set, is variable, and thus some part of performance is blind luck. This is not only because the initial state of the model can be different, but also that we split our training and test data differently. Whether this would apply to a holdout set is not easy to tell without trying it.\n",
    "\n",
    "There's no correlation between high or low seed values and performance; seed is not something to \"tune.\" The seed is a random factor and it can help or hinder depending on the model at play. Generally speaking, when we work with small amounts of data, we're more likely to be affected by different seed values. More complex models can also be affected more by the seed, but not always.\n",
    "\n",
    "Try changing the sample size and/or number of trees in the preceding model and watch how the variability in performance changes. Think about why this might be.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Complex models typically have associated hyperparameters that can affect training. The extent to which these matter, and how they affect the result, depends on the data at hand and complexity of the model being used. We usually need to experiment somewhat with these in order to achieve optimum performance for the data that we have."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda-env-azureml_py38-py"
  },
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "conda-env-azureml_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
