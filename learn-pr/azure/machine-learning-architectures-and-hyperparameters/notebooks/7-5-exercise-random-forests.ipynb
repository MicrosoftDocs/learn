{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd76bd1",
   "metadata": {},
   "source": [
    "# Exercise: Random forests and model architecture\n",
    "\n",
    "In the previous exercise, we used decision trees to predict whether a crime would be solved in San Francisco.\n",
    "\n",
    "Recall that decision trees did a reasonable job, but they have a tendency to *overfit*, meaning that the results would degrade considerably when using the *test* set or any *unseen data*.\n",
    "\n",
    "This time, we'll use *random forests* to address that overfit tendency.\n",
    "\n",
    "We'll also look at how the *model's architecture* can influence its performance.\n",
    "\n",
    "## Data visualization and preparation\n",
    "\n",
    "As usual, let's take another quick look at the crime dataset, then split it into *train* and *test* sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec3f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/san_fran_crime.csv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import graphing # custom graphing code. See our GitHub repo for details\n",
    "\n",
    "# Import the data from the .csv file\n",
    "dataset = pandas.read_csv('san_fran_crime.csv', delimiter=\"\\t\")\n",
    "\n",
    "# Remember to one-hot encode our crime and PdDistrict variables \n",
    "categorical_features = [\"Category\", \"PdDistrict\"]\n",
    "dataset = pandas.get_dummies(dataset, columns=categorical_features, drop_first=False)\n",
    "\n",
    "# Split the dataset in an 90/10 train/test ratio. \n",
    "# Recall that our dataset is very large so we can afford to do this\n",
    "# with only 10% entering the test set\n",
    "train, test = train_test_split(dataset, test_size=0.1, random_state=2, shuffle=True)\n",
    "\n",
    "# Let's have a look at the data and the relationship we are going to model\n",
    "print(dataset.head())\n",
    "print(\"train shape:\", train.shape)\n",
    "print(\"test shape:\", test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, this looks familiar to you! If not, jump back and go through the previous exercise on decision trees.\n",
    "\n",
    "## Model assessment code\n",
    "\n",
    "We'll use the same model assessment code as we did in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Make a utility method that we can re-use throughout this exercise\n",
    "# To easily fit and test out model\n",
    "\n",
    "features = [c for c in dataset.columns if c != \"Resolution\"]\n",
    "\n",
    "def fit_and_test_model(model):\n",
    "    '''\n",
    "    Trains a model and tests it against both train and test sets\n",
    "    '''  \n",
    "    global features\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(train[features], train.Resolution)\n",
    "\n",
    "    # Assess its performance\n",
    "    # -- Train\n",
    "    predictions = model.predict(train[features])\n",
    "    train_accuracy = balanced_accuracy_score(train.Resolution, predictions)\n",
    "\n",
    "    # -- Test\n",
    "    predictions = model.predict(test[features])\n",
    "    test_accuracy = balanced_accuracy_score(test.Resolution, predictions)\n",
    "\n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "\n",
    "print(\"Ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "\n",
    "Let's quickly train a reasonably well-tuned decision tree to remind ourselves of its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn.tree\n",
    "# re-fit our last decision tree to print out its performance\n",
    "model = sklearn.tree.DecisionTreeClassifier(random_state=1, max_depth=10) \n",
    "\n",
    "dt_train_accuracy, dt_test_accuracy = fit_and_test_model(model)\n",
    "\n",
    "print(\"Decision Tree Performance:\")\n",
    "print(\"Train accuracy\", dt_train_accuracy)\n",
    "print(\"Test accuracy\", dt_test_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "A random forest is a collection of decision trees that work together to calculate the label for a sample.\n",
    "\n",
    "Trees in a random forest are trained independently, on different partitions of data, and thus develop different biases; but when combined, they're less likely to overfit the data.\n",
    "\n",
    "Let's build a very simple forest with two trees and the *default* parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a random forest model with two trees\n",
    "random_forest = RandomForestClassifier( n_estimators=2,\n",
    "                                        random_state=2,\n",
    "                                        verbose=False)\n",
    "\n",
    "# Train and test the model\n",
    "train_accuracy, test_accuracy = fit_and_test_model(random_forest)\n",
    "print(\"Random Forest Performance:\")\n",
    "print(\"Train accuracy\", train_accuracy)\n",
    "print(\"Test accuracy\", test_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our two-tree forest has done more poorly than the single tree on the test set, though it's done a better job on the train set. \n",
    "\n",
    "To some extent, we should expect this. Random forests usually work with many more trees. Simply having two allowed it to overfit the training data much better than the original decision tree.\n",
    "\n",
    "## Altering the number of trees\n",
    "\n",
    "Let's then build several forest models, each with a different number of trees, and see how they perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphing\n",
    "\n",
    "# n_estimators states how many trees to put in the model\n",
    "# We will make one model for every entry in this list\n",
    "# and see how well each model performs \n",
    "n_estimators = [2, 5, 10, 20, 50]\n",
    "\n",
    "# Train our models and report their performance\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for n_estimator in n_estimators:\n",
    "    print(\"Preparing a model with\", n_estimator, \"trees...\")\n",
    "\n",
    "    # Prepare the model \n",
    "    rf = RandomForestClassifier(n_estimators=n_estimator, \n",
    "                                random_state=2, \n",
    "                                verbose=False)\n",
    "    \n",
    "    # Train and test the result\n",
    "    train_accuracy, test_accuracy = fit_and_test_model(rf)\n",
    "\n",
    "    # Save the results\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "\n",
    "# Plot results\n",
    "graphing.line_2D(dict(Train=train_accuracies, Test=test_accuracies), \n",
    "                    n_estimators,\n",
    "                    label_x=\"Numer of trees (n_estimators)\",\n",
    "                    label_y=\"Accuracy\",\n",
    "                    title=\"Performance X number of trees\", show=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics look great for the *training* set, but not so much for the *test* set. More trees tended to help both, but only up to a point.\n",
    "\n",
    "We might have expected the number of trees to resolve our overfitting problem, but this wasn't the case! Chances are that the model is simply too complex relative to the data, allowing it to overfit the training set.\n",
    "\n",
    "## Altering the minimum number of samples for split parameter\n",
    "\n",
    "Recall that decision trees have a *root node*, *internal nodes*, and *leaf nodes*, and that the first two can be split into newer nodes with subsets of data.\n",
    "\n",
    "If we let our model split and create too many nodes, it can become increasingly complex and start to overfit.\n",
    "\n",
    "One way to limit that complexity is to tell the model that each node needs to have __at least__ a certain number of samples, otherwise it can't split into subnodes. \n",
    "\n",
    "In other words, we can set the model's `min_samples_split` parameter to the least number of samples required so that a node can be split.\n",
    "\n",
    "Our default value for `min_samples_split` is only `2`, so models will quickly become too complex if that parameter is left untouched.\n",
    "\n",
    "We'll now use the best-performing model we generated, then try it with different `min_samples_split` values, and compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shrink the training set temporarily to explore this\n",
    "# setting with a more normal sample size\n",
    "full_trainset = train\n",
    "train = full_trainset[:1000] # limit to 1000 samples\n",
    "\n",
    "min_samples_split = [2, 10, 20, 50, 100, 500]\n",
    "\n",
    "# Train our models and report their performance\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for min_samples in min_samples_split:\n",
    "    print(\"Preparing a model with min_samples_split = \", min_samples)\n",
    "\n",
    "    # Prepare the model \n",
    "    rf = RandomForestClassifier(n_estimators=20,\n",
    "                                min_samples_split=min_samples,\n",
    "                                random_state=2, \n",
    "                                verbose=False)\n",
    "    \n",
    "    # Train and test the result\n",
    "    train_accuracy, test_accuracy = fit_and_test_model(rf)\n",
    "\n",
    "    # Save the results\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "\n",
    "# Plot results\n",
    "graphing.line_2D(dict(Train=train_accuracies, Test=test_accuracies), \n",
    "                    min_samples_split,\n",
    "                    label_x=\"Minimum samples split (min_samples_split)\",\n",
    "                    label_y=\"Accuracy\",\n",
    "                    title=\"Performance\", show=True)\n",
    "\n",
    "# Rol back the trainset to the full set\n",
    "train = full_trainset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you'll notice in the preceding data, small restrictions on the model's complexity by limiting its ability to split nodes reduce the gap between training and test performance. If this is subtle, it does so without damaging test performance at all.\n",
    "\n",
    "By limiting the model complexity, we address `overfitting`, thus improving its ability to generalize and make accurate predictions on *unseen* data.\n",
    "\n",
    "Notice that using `min_samples_split=20` gave us the best result for the *test* set, and that higher values worsened outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altering the model depth\n",
    "\n",
    "A related method to limit the trees is restricting `max_depth`. This is equivalent to `max_depth` we used for our decision tree, earlier. Its default value is `None`, which means nodes can be expanded until all leaves are *pure* (all samples in it have the same label) or have less samples than the value set for `min_samples_split`.\n",
    "\n",
    "Whether `max_depth` or `min_samples_split` is more appropriate depends on the nature of your dataset, including its size. Usually, we need to experiment to find the best settings. Let's investigate `max_depth` as though we only had 500 crime samples available for our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shrink the training set temporarily to explore this\n",
    "# setting with a more normal sample size\n",
    "full_trainset = train\n",
    "train = full_trainset[:500] # limit to 500 samples\n",
    "\n",
    "max_depths = [2, 4, 6, 8, 10, 15, 20, 50, 100]\n",
    "\n",
    "# Train our models and report their performance\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    print(\"Preparing a model with max_depth = \", max_depth)\n",
    "\n",
    "    # Prepare the model \n",
    "    rf = RandomForestClassifier(n_estimators=20,\n",
    "                                max_depth=max_depth,\n",
    "                                random_state=2, \n",
    "                                verbose=False)\n",
    "    \n",
    "    # Train and test the result\n",
    "    train_accuracy, test_accuracy = fit_and_test_model(rf)\n",
    "\n",
    "    # Save the results\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "\n",
    "# Plot results\n",
    "graphing.line_2D(dict(Train=train_accuracies, Test=test_accuracies),\n",
    "                    max_depths,\n",
    "                    label_x=\"Maximum depth (max_depths)\",\n",
    "                    label_y=\"Accuracy\",\n",
    "                    title=\"Performance\", show=True)\n",
    "\n",
    "# Rol back the trainset to the full set\n",
    "train = full_trainset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding plot tells us that our model actually __benefits__ from a higher value for `max_depth`, up to the limit of `15`.\n",
    "\n",
    "Increasing depth beyond this point begins to harm test performance, because it constrains the model too much for it to generalize.\n",
    "\n",
    "As usual, it's important to evaluate different values when setting model parameters and defining its architecture.\n",
    "\n",
    "## An optimized model\n",
    "\n",
    "Properly optimizing a model on a dataset this large can take many hours, which is more than you need to commit to this exercise just to learn. If you'd like to run a model that's already been optimized for the full dataset, you can run the following code and compare its performance to everything we have seen so far.\n",
    "\n",
    "This is optional, so just note that the model might take a minute or two to train due to its size and the sheer volume of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model \n",
    "rf = RandomForestClassifier(n_estimators=200,\n",
    "                            max_depth=128,\n",
    "                            max_features=25,\n",
    "                            min_samples_split=2,\n",
    "                            random_state=2, \n",
    "                            verbose=False)\n",
    "\n",
    "# Train and test the result\n",
    "print(\"Training model. This may take 1 - 2 minutes\")\n",
    "train_accuracy, test_accuracy = fit_and_test_model(rf)\n",
    "\n",
    "# Print out results, compared to the decision tree\n",
    "data = {\"Model\": [\"Decision tree\",\"Final random forest\"],\n",
    "        \"Train sensitivity\": [dt_train_accuracy, train_accuracy],\n",
    "        \"Test sensitivity\": [dt_test_accuracy, test_accuracy]\n",
    "        }\n",
    "\n",
    "pandas.DataFrame(data, columns = [\"Model\", \"Train sensitivity\", \"Test sensitivity\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, fine-tuning the model's parameters resulted in a significant improvement in the *test* set results.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, we covered the following topics:\n",
    "\n",
    "- Random forest models and how they differ from decision trees\n",
    "- How we can change a model's architecture by setting different parameters and changing their values\n",
    "- The importance of trying several combinations of parameters and evaluate these changes to improve performance\n",
    "\n",
    "In the future, you'll see that different models have architectures where you can fine tune the parameters. Experimentation is needed to achieve the best possible results."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda-env-azureml_py38-py"
  },
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "conda-env-azureml_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
