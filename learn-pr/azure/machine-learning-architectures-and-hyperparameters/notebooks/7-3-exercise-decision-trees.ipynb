{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd76bd1",
   "metadata": {},
   "source": [
    "# Exercise: Decision trees and model architecture\n",
    "\n",
    "Our goal in this exercise is to use a decision tree classifier to predict whether an individual crime will be resolved, based on simple information such as where it took place and what kind of crime it was.\n",
    "\n",
    "## Data visualization\n",
    "\n",
    "As usual, let's begin by loading in and having a look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec3f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/san_fran_crime.csv\n",
    "\n",
    "#Import the data from the .csv file\n",
    "dataset = pandas.read_csv('san_fran_crime.csv', delimiter=\"\\t\")\n",
    "\n",
    "#Let's have a look at the data and the relationship we are going to model\n",
    "print(dataset.head())\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7325d5f",
   "metadata": {},
   "source": [
    "Our data looks to be a mix of _categorical_ variables like Crime `Category` or `PdDistrict`, and _numerical_ variables like the `day_of_year` (1-365) and `time_in_hours` (time of day, converted to a float). We also have `X` and `Y` which refer to GPS coordinates, and `Resolution` which is our label.\n",
    "\n",
    "Let's visualize our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed522b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import graphing # custom graphing code. See our GitHub repo for details\n",
    "import numpy as np\n",
    "\n",
    "# Crime category\n",
    "graphing.multiple_histogram(dataset, label_x='Category', label_group=\"Resolution\", histfunc='sum', show=True)\n",
    "\n",
    "# District\n",
    "graphing.multiple_histogram(dataset, label_group=\"Resolution\", label_x=\"PdDistrict\", show=True)\n",
    "\n",
    "# Map of crimes\n",
    "graphing.scatter_2D(dataset, label_x=\"X\", label_y=\"Y\", label_colour=\"Resolution\", title=\"GPS Coordinates\", size_multiplier=0.8, show=True)\n",
    "\n",
    "# Day of the week\n",
    "graphing.multiple_histogram(dataset, label_group=\"Resolution\", label_x=\"DayOfWeek\", show=True)\n",
    "\n",
    "# day of the year\n",
    "# For graphing we simplify this to week or the graph becomes overwhelmed with bars\n",
    "dataset[\"week_of_year\"] = np.round(dataset.day_of_year / 7.0)\n",
    "graphing.multiple_histogram(dataset, \n",
    "                    label_x='week_of_year',\n",
    "                    label_group='Resolution',\n",
    "                    histfunc='sum', show=True)\n",
    "del dataset[\"week_of_year\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3463e7a1",
   "metadata": {},
   "source": [
    "It always pays to inspect your data before diving in. What we can see here is that:\n",
    "\n",
    "* Most reported crimes were not resolved in 2016.\n",
    "* Different police districts reported different volumes of crime.\n",
    "* Different police districts reported different success rates resolving crimes.\n",
    "* Friday and Saturday typically had more crimes than other days.\n",
    "* Larceny/Theft was overwhelmingly the most common crime reported.\n",
    "\n",
    "## Finalizing Data preparation\n",
    "\n",
    "Let's finalize our data preparation by one-hot encoding our categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical features\n",
    "dataset = pandas.get_dummies(dataset, columns=[\"Category\", \"PdDistrict\"], drop_first=False)\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to make a training and test set.\n",
    "\n",
    "Did you notice how much data we were working with before? If not, recheck the printouts from above.\n",
    "\n",
    "We have more than 150,000 samples to work with. That's a very large amount of data. Due to the sheer size, we can afford to have a larger proportion in the training set that we would normally have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset in an 90/10 train/test ratio. \n",
    "# We can afford to do this here because our dataset is very very large\n",
    "# Normally we would choose a more even ratio\n",
    "train, test = train_test_split(dataset, test_size=0.1, random_state=2, shuffle=True)\n",
    "\n",
    "print(\"Data shape:\")\n",
    "print(\"train\", train.shape)\n",
    "print(\"test\", test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model assessment code\n",
    "\n",
    "We'll fit several models here, so to maximize code reuse, we'll make a dedicated method that trains a model and then tests it.\n",
    "\n",
    "Our test stage uses a metric called \"balanced accuracy,\" which we'll refer to as \"accuracy\" for short throughout this exercise. It's not critical that you understand this metric for these exercises, but in essence this is between `0` and `1`:\n",
    "* `0` means no answers were correct\n",
    "* `1` means all answers were correct\n",
    "\n",
    "Balanced accuracy takes into account that our data set has more unresolved than resolved crimes. We'll cover what this means in later learning material in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Make a utility method that we can re-use throughout this exercise\n",
    "# To easily fit and test out model\n",
    "\n",
    "features = [c for c in dataset.columns if c != \"Resolution\"]\n",
    "\n",
    "\n",
    "def fit_and_test_model(model):\n",
    "    '''\n",
    "    Trains a model and tests it against both train and test sets\n",
    "    '''  \n",
    "    global features\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(train[features], train.Resolution)\n",
    "\n",
    "    # Assess its performance\n",
    "    # -- Train\n",
    "    predictions = model.predict(train[features])\n",
    "    train_accuracy = balanced_accuracy_score(train.Resolution, predictions)\n",
    "\n",
    "    # -- Test\n",
    "    predictions = model.predict(test[features])\n",
    "    test_accuracy = balanced_accuracy_score(test.Resolution, predictions)\n",
    "\n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "\n",
    "print(\"Ready to go!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e73712e8",
   "metadata": {},
   "source": [
    "## Fitting a decision tree\n",
    "\n",
    "Let's use a decision tree to help us determine whether a not a crime will be resolved. Decision trees are categorization models that break decisions down into multiple steps. You can liken them to a flow chart, with a decision being made at each subsequent level of the tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e738484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree\n",
    "\n",
    "# fit a simple tree using only three levels\n",
    "model = sklearn.tree.DecisionTreeClassifier(random_state=2, max_depth=3) \n",
    "train_accuracy, test_accuracy = fit_and_test_model(model)\n",
    "\n",
    "print(\"Model trained!\")\n",
    "print(\"Train accuracy\", train_accuracy)\n",
    "print(\"Test accuracy\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df8161",
   "metadata": {},
   "source": [
    "That's not bad!\n",
    "Now that the model is trained, let's visualize it so we can get a better idea of how it works (and also see where it gets its **tree** moniker from!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------\n",
    "from sklearn.tree import plot_tree\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plot = plt.subplots(figsize = (4,4), dpi=300)[0]\n",
    "plot = plot_tree(model,\n",
    "                fontsize=3,\n",
    "                feature_names = features, \n",
    "                class_names = ['0','1'], # class_names in ascending numerical order \n",
    "                label=\"root\",\n",
    "                impurity=False,\n",
    "                filled=True) \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18adc25b",
   "metadata": {},
   "source": [
    "All of the blue-colored boxes correspond to prediction that a crime would be resolved. \n",
    "\n",
    "Take a look at the tree to learn what it thinks are important for predicting an outcome. Compare this to the graphs we made earlier. Can you see a relationship between the two?\n",
    "\n",
    "The score we have isn't bad, but the tree is pretty simple. Let's see if we can do better.\n",
    "\n",
    "## Improving performance through architecture\n",
    "\n",
    "We'll try and improve our model's performance by changing its architecture. Let's focus on the ``maximum_depth`` parameter. \n",
    "\n",
    "Our previous tree was relatively simple and shallow with a ``maximum_depth = 3``. Let's see what happens if we increase it to 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8387b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a very deep tree\n",
    "model = sklearn.tree.DecisionTreeClassifier(random_state=1, max_depth=100)\n",
    "\n",
    "train_accuracy, test_accuracy = fit_and_test_model(model)\n",
    "print(\"Train accuracy\", train_accuracy)\n",
    "print(\"Test accuracy\", test_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24dfb550",
   "metadata": {},
   "source": [
    "As you can imagine, a tree with a ``maximum_depth = 100`` is big. In fact, it's too big to visualize here, so let's jump straight into seeing how the new model works on our training data.\n",
    "\n",
    "Both the training and test accuracy have increased a lot. The training, however, has increased much more. While we're happy with the improvement in test accuracy, this is a clear sign of _overfitting_.\n",
    "\n",
    "Overfitting with decision trees becomes even more obvious when we have more typical (smaller) sized datasets. Let's rerun the previous exercise, but with only 100 training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b1010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily shrink the training set to something\n",
    "# more realistic\n",
    "full_training_set = train\n",
    "train = train[:100]\n",
    "\n",
    "# fit the same tree as before\n",
    "model = sklearn.tree.DecisionTreeClassifier(random_state=1, max_depth=100)\n",
    "\n",
    "# Assess on the same test set as before\n",
    "train_accuracy, test_accuracy = fit_and_test_model(model)\n",
    "print(\"Train accuracy\", train_accuracy)\n",
    "print(\"Test accuracy\", test_accuracy)\n",
    "\n",
    "# Roll the training set back to the full set\n",
    "train = full_training_set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a704ebbb",
   "metadata": {},
   "source": [
    "The model performs badly on the test data. With reasonable sized datasets, _decision trees_ are notoriously prone to _overfitting_. In other words, they tend to fit very well to the data on which they're trained, but generalize very poorly to unknown data. This gets worse the deeper the tree gets or the smaller the training set gets. Let's see if we can mitigate this.\n",
    "\n",
    "## Pruning a tree\n",
    "\n",
    "_Pruning_ is the process of simplifying a _decision tree_ so that it gives the best classification results while simultaneously reducing overfitting. There are two types of pruning: _pre-pruning_ and _post-pruning_. \n",
    "\n",
    "_Pre-pruning_ involves restricting the model during training so that it doesn't grow larger than is useful. We will cover this next.\n",
    "\n",
    "_Post-pruning_ is when we simplify the tree after training it. It doesn't involve making any design decisions ahead of time, but simply optimizing the existing model. This is a valid technique but is quite involved, and so we won't cover it here due time constraints.\n",
    "\n",
    "## Prepruning\n",
    "\n",
    "We can perform pre-pruning by generating many models, each with different ``max_depth`` parameters. For each, we're recording the _balanced accuracy_ for the _test set_. To show that this is important even with quite large datasets, we'll work with 10,000 samples here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e9f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily shrink the training set to 10000\n",
    "# for this exercise to see how pruning is important\n",
    "# even with moderately large datasets\n",
    "full_training_set = train\n",
    "train = train[:10000]\n",
    "\n",
    "\n",
    "# Loop through the values below and build a model\n",
    "# each time, setting the maximum depth to that value \n",
    "max_depth_range = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ,15, 20, 50, 100]\n",
    "accuracy_trainset = []\n",
    "accuracy_testset = []\n",
    "for depth in max_depth_range:\n",
    "    # Create and fit the model\n",
    "    prune_model = sklearn.tree.DecisionTreeClassifier(random_state=1, max_depth=depth)\n",
    "\n",
    "    # Calculate and record its sensitivity\n",
    "    train_accuracy, test_accuracy = fit_and_test_model(prune_model)\n",
    "    accuracy_trainset.append(train_accuracy)\n",
    "    accuracy_testset.append(test_accuracy)\n",
    "\n",
    "# Plot the sensitivity as a function of depth  \n",
    "pruned_plot = pandas.DataFrame(dict(max_depth=max_depth_range, accuracy=accuracy_trainset))\n",
    "\n",
    "fig = graphing.line_2D(dict(train=accuracy_trainset, test=accuracy_testset), x_range=max_depth_range, show=True)\n",
    "\n",
    "# Roll the training set back to the full thing\n",
    "train = full_training_set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9302f3d8",
   "metadata": {},
   "source": [
    "We can see from our plot that the best _accuracy_ is obtained for a ``max_depth`` of about 10. We're looking to simplify our tree, so we pick ``max_depth = 10`` for our final _pruned_ tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a30988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily shrink the training set to 10000\n",
    "# for this exercise to see how pruning is important\n",
    "# even with moderately large datasets\n",
    "full_training_set = train\n",
    "train = train[:10000]\n",
    "\n",
    "\n",
    "# Not-pruned\n",
    "model = sklearn.tree.DecisionTreeClassifier(random_state=1)\n",
    "train_accuracy, test_accuracy = fit_and_test_model(model)\n",
    "print(\"Unpruned Train accuracy\", train_accuracy)\n",
    "print(\"Unpruned Test accuracy\", test_accuracy)\n",
    "\n",
    "\n",
    "# re-fit our final tree to print out its performance\n",
    "model = sklearn.tree.DecisionTreeClassifier(random_state=1, max_depth=10)\n",
    "train_accuracy, test_accuracy = fit_and_test_model(model)\n",
    "print(\"Train accuracy\", train_accuracy)\n",
    "print(\"Test accuracy\", test_accuracy)\n",
    "\n",
    "# Roll the training set back to the full thing\n",
    "train = full_training_set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5afb65a",
   "metadata": {},
   "source": [
    "Our new and improved _pruned_ model shows a marginally better _balanced accuracy_ on the _test set_ and much worse performance on the _training set_ than the model that isn't pruned. This means our pruning has significantly reduced overfitting.\n",
    "\n",
    "If you'd like, go back and change the number of samples to 100 and notice how the optimal `max_depth` changes. Think about why this might be (hint: model complexity versus sample size) .\n",
    "\n",
    "Another option that you might like to play with is how many features are entered into the tree. You can observe similar patterns of overfitting by manipulating this. In fact, the number and type of the features provided to a decision tree can be even more important than its sheer size.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this unit, we covered the following topics:\n",
    " - Using visualization techniques to gain insights into our data\n",
    " - Building a simple _decision tree_ model\n",
    " - Using the trained model to predict labels\n",
    " - _Pruning_ a _decision tree_ to reduce the effects of overfitting"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda-env-azureml_py38-py"
  },
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "conda-env-azureml_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
