We completed our introduction to classification, so let's recap some key points.

We saw that classification has a lot in common with classic regression. For both, we can use supervised learning, a cost function, and use test and train datasets to estimate real-world performance. We focused here on logistic regression, which is almost a hybrid between these two types of model, and showed how thresholding the output gives us a categorical label, like `avalanche`/`no-avalanche`.

We discussed how assessing classification models can be slightly more difficult than with regression models, particularly because the costs functions involved are often unintuitive.

We also explored how adding and combining features could result in substantial model improvements. Importantly, we showed how really thinking about what your data means is the key to achieving the best result.

In this module, we worked with logistic regression. But keep in mind, that most of the subjects we covered here also apply to many other types of classification models, including models that try to predict more than two possible categories.
