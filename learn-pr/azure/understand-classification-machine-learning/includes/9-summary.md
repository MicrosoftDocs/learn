We’ve completed our introduction to classification. Let’s recap some key points.

We’ve seen that classification has a lot in common with classic regression. For both we can use supervised learning, a cost function, and use test and train datasets to estimate real-world performance. We focused here on logistic regression, which is almost a hybrid between these two types of model, and showed how thresholding the output gives us a categorical label, like ‘avalanche’/’no-avalanche’.

We discussed how assessing classification models can be slightly more difficult than with regression models, particularly because the costs functions involved are often unintuitive.

We also explored how adding and combining features could result in substantial model improvements. Importantly, we showed how really thinking about what your data mean is the key to achieving the best result.

In this module, we worked with logistic regression. But keep in mind, that most of the subjects we covered here apply to many other types of classification models as well. Including models that try to predict more than two possible categories.