{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd76bd1",
   "metadata": {},
   "source": [
    "# Exercise: Assessing a logistic regression model\n",
    "\n",
    "In the previous exercise, we fit a simple logistic regression model to predict the chance of an avalanche. This time, we'll create the same model and take a deeper look at how to best understand the mistakes that it makes.\n",
    "\n",
    "## Data visualization\n",
    "\n",
    "Let's remind ourselves of our data. Remember we're planning to train a model that can predict avalanches based on the number of weak layers of snow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec3f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "!pip install statsmodels\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/avalanche.csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import the data from the .csv file\n",
    "dataset = pandas.read_csv('avalanche.csv', delimiter=\"\\t\")\n",
    "\n",
    "#Let's have a look at the data and the relationship we're going to model\n",
    "print(dataset.head())\n",
    "\n",
    "plt.boxplot([dataset[\"avalanche\"], dataset[\"weak_layers\"]])\n",
    "\n",
    "plt.xlabel(\"Avalanches, Weak Layers\")\n",
    "plt.ylabel(\"Values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04600b",
   "metadata": {},
   "source": [
    "It seems that avalanches are associated with having more weak layers of snow. That said, some days many weak layers have been recorded, but no avalanche occurred. This means our model will have difficulty being extremely accurate using only this feature. Let's continue though, and come back to this in a future exercise.  \n",
    "\n",
    "Before we begin, we need to split our dataset into training and test sets. We will train on the _training_ set, and test on (you guessed it) the _test_ set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dbac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset in an 75/25 train/test ratio. \n",
    "train, test = train_test_split(dataset, test_size=0.25, random_state=10)\n",
    "\n",
    "print(\"Train size:\", train.shape[0])\n",
    "print(\"Test size:\", test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48accb9c",
   "metadata": {},
   "source": [
    "## Fitting a model\n",
    "\n",
    "Let's fit a simple logistic regression model using *log-loss* as a cost function. This is a very standard way to fit a classification model - so standard, in fact that we don't need to specify it at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33121fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Perform logistic regression.\n",
    "model = smf.logit(\"avalanche ~ weak_layers\", train).fit()\n",
    "\n",
    "print(\"Model trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc175c",
   "metadata": {},
   "source": [
    "## Assessing the model with summary information\n",
    "\n",
    "If we use `statsmodels`, we can get a detailed summary about the model by simply calling `summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c47291",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "502ca355",
   "metadata": {},
   "source": [
    "This summary provides some detailed information. \n",
    "\n",
    "Two useful pieces of information are in the bottom row. The coef for `weak_layers` is positive, which means that as `weak_layers` increases, so does the probability of an avalanche. The `P` value column is less than 0.05, which means the model is confident that `weak_layers` is a useful predictor of avalanches. \n",
    "\n",
    "The rest of this table, however, is difficult to understand for beginners and so it's not clear how well our model is working. Let's try another way. \n",
    "\n",
    "## Assessing model visually\n",
    "\n",
    "Sometimes, but not always, we can visually assess a logistic regression mode. Let's plot our model against the actual data in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def predict(weak_layers):\n",
    "    return model.predict(pd.DataFrame({\"weak_layers\": weak_layers}))\n",
    "\n",
    "plt.scatter(test['weak_layers'], test['avalanche'])\n",
    "\n",
    "trendline_values = predict(test['weak_layers'])\n",
    "plt.plot(test['weak_layers'], trendline_values, color='red', label=\"Trendline\")\n",
    "\n",
    "plt.xlabel(\"weak_layers\")\n",
    "plt.ylabel(\"avalanche\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1720fcdd",
   "metadata": {},
   "source": [
    "It's hard to see the s-shape of the trendline, because the number of weak layers of snow, and the likelihood of an avalanche, are only weakly related. If we zoom out, we can get a slightly better view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8606dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(weak_layers):\n",
    "    return model.predict(pd.DataFrame({\"weak_layers\": weak_layers}))\n",
    "\n",
    "\n",
    "plt.scatter(test['weak_layers'], test['avalanche'])\n",
    "trendline_values = predict(test['weak_layers'])\n",
    "plt.plot(test['weak_layers'], trendline_values, color='red', label=\"Trendline\")\n",
    "plt.xlim(-20, 20)\n",
    "plt.xlabel(\"weak_layers\")\n",
    "plt.ylabel(\"avalanche\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654dc417",
   "metadata": {},
   "source": [
    "Checking the earlier graph, we can see that our model will predict an avalanche when the number of weak layers of snow is greater than 5. We can tell this because the value of the line is `0.5` at `x=5` (remember that in the previous unit we defined a *classifier threshold*, so that probabilities over `0.5` would be classified as `True`).\n",
    "\n",
    "How this relates with points is hard to tell - the points overlap and so it is difficult to see how many points are at 0 or at 1. How else can we assess the model?\n",
    "\n",
    "## Assess with cost function\n",
    "\n",
    "Let's assess our model with a log-loss cost function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d49ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Make predictions from the test set\n",
    "predictions = model.predict(test)\n",
    "\n",
    "# Calculate log loss\n",
    "print(\"Log loss\", log_loss(test.avalanche, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a6048b0",
   "metadata": {},
   "source": [
    "0.68 - what does that mean? This could be useful to compare two different models, but it's hard to get a grasp on exactly what this means for real-world performance. \n",
    "\n",
    "## Assess accuracy\n",
    "\n",
    "Let's instead assess _accuracy_. Accuracy refers to the _proportion of predictions the model got correct_, after predictions are converted from probabilities to `avalanche` or `no-avalanche`.\n",
    "\n",
    "The first thing to do is convert probabilities to hard predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Print a few predictions before we convert them to categories\n",
    "print(f\"First three predictions (probabilities): {predictions.iloc[0]}, {predictions.iloc[1]}, {predictions.iloc[2]}\")\n",
    "\n",
    "# convert to absolute values\n",
    "avalanche_predicted = predictions >= 0.5\n",
    "\n",
    "# Print a few predictions converted into categories\n",
    "print(f\"First three predictions (categories): {avalanche_predicted.iloc[0]}, {avalanche_predicted.iloc[1]}, {avalanche_predicted.iloc[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f39f1e",
   "metadata": {},
   "source": [
    "Now we can calculate accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ef4a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate what proportion were predicted correctly\n",
    "guess_was_correct = test.avalanche == avalanche_predicted\n",
    "accuracy = numpy.average(guess_was_correct)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy for whole test dataset:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc68253",
   "metadata": {},
   "source": [
    "It looks like it's predicting the correct answer 61% of the time. This is helpful information. What kind of mistakes is it making, though? Let's take a look at whether it is guessing avalanche when there are none (false positives), or failing to guess 'avalanche' when one actually occurs (false negative):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc3552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Negative: calculate how often it guessed no avalanche when one actually occurred\n",
    "false_negative = numpy.average(numpy.logical_not(guess_was_correct) & test.avalanche)\n",
    "\n",
    "# False positive: calculate how often it guessed avalanche, when none actually happened\n",
    "false_positive = numpy.average(numpy.logical_not(guess_was_correct) & numpy.logical_not(test.avalanche))\n",
    "\n",
    "\n",
    "print(f\"Wrongly predicted an avalanche {false_positive * 100}% of the time\")\n",
    "print(f\"Failed to predict avalanches {false_negative * 100}% of the time\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c90d1851",
   "metadata": {},
   "source": [
    "I think we can agree that's a lot more understandable than reading the cost function or the graph!\n",
    "\n",
    "## Summary\n",
    "\n",
    "We've walked through different ways to assess a logistic regression model. We've seen that detailed summaries can provide rich information, but this can be difficult to digest. Metrics for these kinds of models also aren't necessarily intuitive or detailed enough to understand the model. With a little extra work, we can look at actual predictions versus actual outcomes and get an intuition for how the model might work in the real world."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda-env-azureml_py38-py"
  },
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "conda-env-azureml_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
