The Mixed Reality Toolkit allows you to consume inputs from various input sources, such as controllers, articulated hands, or speech. Choosing the interaction model best suited for your mixed-reality experience requires you to identify your user, identify their goals, and consider any situational or environmental factors that might impact their experience.

Suppose you're designing a game that requires a lot of interactions. The users are a bit concerned about the method of interaction, as it'll be tiring to use the controllers over a long period. Therefore, you need to come up with alternative methods of input for your application's success.

This module shows you how to incorporate eye tracking and voice commands with MRTK using NASA's Mars Curiosity Rover hologram model. Here, you'll:

* How to enable eye tracking for HoloLens 2.
* Learn how to use eye tracking to trigger action.
* How to create speech commands.
* Learn how to control speech commands globally and locally.

By the end of the module, you can add eye tracking and voice commands to your project using MRTK.

You can find a [completed example of this tutorial here](https://github.com/microsoft/MixedRealityLearning/tree/development/MRTK3%20Tutorials).

