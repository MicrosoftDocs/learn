{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a9f521",
   "metadata": {},
   "source": [
    "# Training And Test Sets\n",
    "\n",
    "We have seen previously how to fit a model to a dataset. In this exercise, we'll be looking at how to check and confirm the validity and performance of our models by using training and testing sets.\n",
    "As usual, we begin by loading in and having a look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa02a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "!pip install statsmodels\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/dog-training.csv\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/dog-training-switzerland.csv\n",
    "\n",
    "data = pandas.read_csv(\"dog-training.csv\", delimiter=\"\\t\")\n",
    "\n",
    "print(data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87b080",
   "metadata": {},
   "source": [
    "We are interested in the relationship between a dog's weight and the amount of rescues it performed in the previous year. Let's begin by plotting `rescues_last_year` as a function of `weight_last_year`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5403b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import graphing\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# First, we define our formula using a special syntax\n",
    "# This says that rescues_last_year is explained by weight_last_year\n",
    "formula = \"rescues_last_year ~ weight_last_year\"\n",
    "\n",
    "model = smf.ols(formula = formula, data = data).fit()\n",
    "\n",
    "graphing.scatter_2D(data, \"weight_last_year\", \"rescues_last_year\", trendline = lambda x: model.params[1] * x + model.params[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8729bbba",
   "metadata": {},
   "source": [
    "There seems to be a pretty clear relationship between a dog's weight and the number of rescues it's performed. \n",
    "That seems pretty reasonable, as we would expect heavier dogs to be bigger and stronger and thus better at saving lives!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789bfb1b",
   "metadata": {},
   "source": [
    "# Train/test split\n",
    "\n",
    "This time, instead of fitting a model to the entirety of our dataset, we're going to separate our dataset into two smaller partitions: a _training set_ and a _test set_.  \n",
    "The _training set_ is the largest of the two, usually made up of between 70-80% of the overall dataset, with the rest of the dataset making up the _test set_.\n",
    "By splitting our data, we're able to gauge the performance of our model when confronted with previously unseen data. \n",
    "\n",
    "Notice that data on the _test set_ is never used in training. For that reason it's commonly referred to as *unseen data* or data that is *unknown by the model*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b26083f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Obtain the label and feature from the original data\n",
    "dataset = data[['rescues_last_year','weight_last_year']]\n",
    "\n",
    "# Split the dataset in an 70/30 train/test ratio. We also obtain the respective corresponding indices from the original dataset.\n",
    "train, test = train_test_split(dataset, train_size=0.7, random_state=21)\n",
    "\n",
    "print(\"Train\")\n",
    "print(train.head())\n",
    "print(train.shape)\n",
    "\n",
    "print(\"Test\")\n",
    "print(test.head())\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d8423",
   "metadata": {},
   "source": [
    "We can see that these sets are different, and that the _training set_ and _test set_ contain 70% and 30% of the overall data respectively.\n",
    "\n",
    "Let's have a look at how the _training set_ and _test set_ are separated out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16458b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't need to understand this code well\n",
    "# It's just used to create a scatter plot\n",
    "\n",
    "# concatenate training and test so they can be graphed\n",
    "plot_set = pandas.concat([train,test])\n",
    "plot_set[\"Dataset\"] = [\"train\"] * len(train) + [\"test\"] * len(test)\n",
    "\n",
    "# Create graph\n",
    "graphing.scatter_2D(plot_set, \"weight_last_year\", \"rescues_last_year\", \"Dataset\", trendline = lambda x: model.params[1] * x + model.params[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54dccc7",
   "metadata": {},
   "source": [
    "# Training Set\n",
    "\n",
    "We begin by training our model using the _training set_, and testing its performance with the same _training set_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48980f28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "# First, we define our formula using a special syntax\n",
    "# This says that rescues_last_year is explained by weight_last_year\n",
    "formula = \"rescues_last_year ~ weight_last_year\"\n",
    "\n",
    "# Create and train the model\n",
    "model = smf.ols(formula = formula, data = train).fit()\n",
    "\n",
    "# Graph the result against the data\n",
    "graphing.scatter_2D(train, \"weight_last_year\", \"rescues_last_year\", trendline = lambda x: model.params[1] * x + model.params[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6320f95e",
   "metadata": {},
   "source": [
    "We can gauge our model's performance by calculating the _mean squared error_ (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e0e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the in-buit sklearn function to calculate the MSE\n",
    "correct_labels = train['rescues_last_year']\n",
    "predicted = model.predict(train['weight_last_year'])\n",
    "\n",
    "MSE = mse(correct_labels, predicted)\n",
    "print('MSE = %f ' % MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02703ba2",
   "metadata": {},
   "source": [
    "# Test Set\n",
    "\n",
    "Next, we test the same model's performance using the _test set_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphing.scatter_2D(test, \"weight_last_year\", \"rescues_last_year\", trendline = lambda x: model.params[1] * x + model.params[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67c1c94",
   "metadata": {},
   "source": [
    "Let's have a look at the MSE again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd4053",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_labels = test['rescues_last_year']\n",
    "predicted = model.predict(test['weight_last_year'])\n",
    "\n",
    "MSE = mse(correct_labels, predicted)\n",
    "print('MSE = %f ' % MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295d65cc",
   "metadata": {},
   "source": [
    "We can see that the model performs much better on the known _training data_ than on the unseen _test data_ (remember that higher MSE values are worse).  \n",
    "\n",
    "This can be down to a number of factors but first and foremost is _overfitting_, which is when a model matches the data in the _training set_ too closely. This means that it will perform very well on the _training set_, but will not _generalize_ well. (i.e., work well with other datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aaab18",
   "metadata": {},
   "source": [
    "# New Dataset\n",
    "\n",
    "To illustrate our point further, let's have a look at how our model performs when confronted with a completely new, unseen, and larger dataset. For our scenario, we'll use data provided by the avalanche rescue charity's European branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c8d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an alternative dataset from the charity's European branch\n",
    "new_data = pandas.read_csv(\"dog-training-switzerland.csv\", delimiter=\"\\t\")\n",
    "\n",
    "print(new_data.shape)\n",
    "new_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b9a5d",
   "metadata": {},
   "source": [
    "The features are the same, but we have much more data this time. Let's see how our model does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b578927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the fitted model against this new dataset. \n",
    "\n",
    "graphing.scatter_2D(new_data, \"weight_last_year\", \"rescues_last_year\", trendline = lambda x: model.params[1] * x + model.params[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e8b4e",
   "metadata": {},
   "source": [
    "And now the MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9276c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_labels = new_data['rescues_last_year']\n",
    "predicted = model.predict(new_data['weight_last_year'])\n",
    "\n",
    "MSE = mse(correct_labels, predicted)\n",
    "print('MSE = %f ' % MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c582048",
   "metadata": {},
   "source": [
    "As expected, the model performs better on the training dataset as it does on the unseen dataset. This is simply due to overfitting, as we saw previously.\n",
    "\n",
    "Interestingly, the model performs better on this unseen dataset than it does on the _test set_. This is because our previous test set was quite small, and thus not a very good representation of 'real world' data. By contrast, this unseen dataset is large and a much better representation of data we'll find outside of the lab. In essence, this shows us that part of performance difference we see between training and test is due to model overfitting, and part of the error is due to the test set not being perfect. In the next exercises, we'll explore the trade-off we have to make between training and test dataset sizes. \n",
    "\n",
    "# Summary\n",
    "\n",
    "In this exercise we covered the following concepts:\n",
    "\n",
    " - Splitting a dataset into a _training set_ and a _test set_.\n",
    " - Training a model using the _training set_ and testing its performance on the _training set_, _test set_, and on a new, unseen dataset.\n",
    " - Compared the respective MSEs to highlight the effects and dangers of _overfitting_.\n"
   ]
  }
 ],
 "metadata": {
    "kernelspec": {
      "name": "conda-env-azureml_py38-py",
      "language": "python",
      "display_name": "azureml_py38"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "conda-env-azureml_py38-py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
  }