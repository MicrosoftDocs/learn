{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d991c31d",
   "metadata": {},
   "source": [
    "# Test sets in depth\n",
    "\n",
    "In the previous exercise, we looked at how to split our data into training and test sets to evaluate model performance.\n",
    "\n",
    "We'll now repeat the last exercise, but this time we'll look at what happens when we split the data in different ways and ratios.\n",
    "\n",
    "First, let's recall what's in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "!pip install statsmodels\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/dog-training.csv\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/dog-training-switzerland.csv\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "\n",
    "data = pandas.read_csv(\"dog-training.csv\", delimiter=\"\\t\")\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d57fb2d",
   "metadata": {},
   "source": [
    "Let's take a quick look at what the distribution of our data looks like (remember that we were using `weight_last_year` to predict the value of `rescues_last_year`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d47ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obtain the label and feature from the original data\n",
    "dataset = data[['rescues_last_year','weight_last_year']]\n",
    "\n",
    "# Print the number of observations\n",
    "print(\"No. observations:\", dataset.shape[0])\n",
    "\n",
    "# Scatter plot of the full (unsplit) dataset\n",
    "plt.scatter(dataset['rescues_last_year'], dataset['weight_last_year'], alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"Rescues Last Year\")\n",
    "plt.ylabel(\"Weight Last Year\")\n",
    "plt.title(\"Distribution of Rescues vs Weight (Full Dataset)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e02b8c4",
   "metadata": {},
   "source": [
    "Notice we have 50 observations plotted (which corresponds to the number of samples in the dataset).\n",
    "\n",
    "## Dataset split ratio comparison\n",
    "\n",
    "We'll now split our dataset using different ratios to understand how these can affect our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3522e103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split Dataset using different ratios 50:50, 60:40, 70:30, 80:20\n",
    "train_5050, test_5050 = train_test_split(dataset, test_size=0.5, random_state=2)\n",
    "train_6040, test_6040 = train_test_split(dataset, test_size=0.4, random_state=2)\n",
    "train_7030, test_7030 = train_test_split(dataset, test_size=0.3, random_state=2)\n",
    "train_8020, test_8020 = train_test_split(dataset, test_size=0.2, random_state=2)\n",
    "\n",
    "# Add a column to each set to identify if a datapoint belongs to \"train\" or \"test\"\n",
    "train_5050, test_5050 = train_5050.assign(Set=\"train\"), test_5050.assign(Set=\"test\")\n",
    "train_6040, test_6040 = train_6040.assign(Set=\"train\"), test_6040.assign(Set=\"test\")\n",
    "train_7030, test_7030 = train_7030.assign(Set=\"train\"), test_7030.assign(Set=\"test\")\n",
    "train_8020, test_8020 = train_8020.assign(Set=\"train\"), test_8020.assign(Set=\"test\")\n",
    "\n",
    "# Concatenate the \"train\" and \"test\" sets for each split so we can plot them on the same chart\n",
    "df_5050 = pandas.concat([train_5050, test_5050], axis=0)\n",
    "df_6040 = pandas.concat([train_6040, test_6040], axis=0)\n",
    "df_7030 = pandas.concat([train_7030, test_7030], axis=0)\n",
    "df_8020 = pandas.concat([train_8020, test_8020], axis=0)\n",
    "\n",
    "# Function to plot each split\n",
    "def plot_split(df, title):\n",
    "    colors = {'train': 'blue', 'test': 'orange'}\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for label in df[\"Set\"].unique():\n",
    "        subset = df[df[\"Set\"] == label]\n",
    "        plt.scatter(subset[\"weight_last_year\"], subset[\"rescues_last_year\"],\n",
    "                    label=label, alpha=0.6, c=colors[label])\n",
    "    plt.xlabel(\"Weight Last Year\")\n",
    "    plt.ylabel(\"Rescues Last Year\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot each distribution for comparison\n",
    "plot_split(df_5050, \"50:50 Split\")\n",
    "plot_split(df_6040, \"60:40 Split\")\n",
    "plot_split(df_7030, \"70:30 Split\")\n",
    "plot_split(df_8020, \"80:20 Split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c059172f",
   "metadata": {},
   "source": [
    "Notice how the first splits leave us with relatively small _training_ datasets. On the 50:50 split, we have only 25 samples to train with.\n",
    "\n",
    "On the other hand, the latter ones leave us much more data for training, but relatively little for testing. The 80:20 split leaves us with only 10 samples in the _test_ set!\n",
    "\n",
    "\n",
    "Let's take a look at the distributions of _training_ data for each split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5a64f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for each \"train\" set that identifies the split used\n",
    "train_8020 = train_8020.assign(Split=\"80:20\")\n",
    "train_7030 = train_7030.assign(Split=\"70:30\")\n",
    "train_6040 = train_6040.assign(Split=\"60:40\")\n",
    "train_5050 = train_5050.assign(Split=\"50:50\")\n",
    "\n",
    "# Concatenate training sets so we can plot them on the same chart\n",
    "split_df = pandas.concat([train_5050, train_6040, train_7030, train_8020], axis=0)\n",
    "\n",
    "# Plot a histogram of data distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "splits = split_df[\"Split\"].unique()\n",
    "for split in splits:\n",
    "    subset = split_df[split_df[\"Split\"] == split]\n",
    "    plt.hist(subset[\"rescues_last_year\"], bins=15, alpha=0.5, label=split)\n",
    "\n",
    "plt.xlabel(\"Rescues Last Year\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Training Data by Split\")\n",
    "plt.legend(title=\"Split\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c0ff59e",
   "metadata": {},
   "source": [
    "We can draw a couple of conclusions from the plot above:\n",
    "\n",
    "1. The `train_test_split()` function we used does a fairly good job of keeping a fair distribution of data across different ratios. It attempts to keep different values represented in the same proportion.\n",
    "\n",
    "2. When we use a `50:50` ratio, however, we have to leave so much data out of the _train_ set that some values aren't present anymore! (Can you spot where blue bars are missing?)\n",
    "\n",
    "Point `2` is especially concerning, because if that data isn't available for training, our model can't learn from it, and therefore won't make accurate predictions. In other words, using a `50:50` split doesn't look like a good idea for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a94fe",
   "metadata": {},
   "source": [
    "## Fitting and evaluating models with different split ratios\n",
    "\n",
    "Let's fit models using different splits, then see how they appear to perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9527b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "def train_and_test_model(name, train, test):\n",
    "    '''\n",
    "    This method creates a model, trains it on the provided data, and assesses \n",
    "    it against the test data\n",
    "    '''\n",
    "    # This formula says that rescues_last_year is explained by weight_last_year\n",
    "    formula = \"rescues_last_year ~ weight_last_year\"\n",
    "\n",
    "    # Create and train a linear regression model using the training data\n",
    "    model = smf.ols(formula = formula, data = train).fit()\n",
    "\n",
    "    # Now evaluate the model (by calculating the Mean Squared Error) using the \n",
    "    # corresponding \"test\" set for that split\n",
    "    correct_answers = test['rescues_last_year']\n",
    "    predictions = model.predict(test['weight_last_year'])\n",
    "    MSE = mse(correct_answers, predictions)\n",
    "    print(name + ' MSE = %f ' % MSE)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Train a model and test it for each kind of split\n",
    "print(\"Mean Squared Error values (smaller is better)\")\n",
    "model_5050 = train_and_test_model(\"50:50\", train_5050, test_5050)\n",
    "model_6040 = train_and_test_model(\"60:40\", train_6040, test_6040)\n",
    "model_7030 = train_and_test_model(\"70:30\", train_7030, test_7030)\n",
    "model_8020 = train_and_test_model(\"80:20\", train_8020, test_8020)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1025fe8a",
   "metadata": {},
   "source": [
    "The preceding code trains one model for each ratio, using the corresponding _train_ set for that ratio. It then calculates the MSE (Mean Squared Error) for each model, using its corresponding _test_ set.\n",
    "\n",
    "The results seem mixed. The `80:20` ratio was best, but there's no clear pattern as to whether growing or shrinking the training set is helpful.\n",
    "\n",
    "Two things influence our results. First, the larger the _test_ set, the more we can trust the test results. Secondly, models usually will train better with larger training sets. \n",
    "\n",
    "These influences are at odds with one another, and how influential they are is exaggerated here because our dataset it very small. In this particular situation, it's hard to assess whether the `60:40` split is truly better than the `70:30` split, for example. This is because the `70:30 split` might just give the appearance of being worse because it was tested against a less-representative (smaller) test set.\n",
    "\n",
    "## Model evaluation\n",
    "\n",
    "Let's take a look now at what happens when these models are used against a much larger dataset on which it wasn't trained or tested. This can happen in the real world because we choose to hold back data in the beginning, or simply because we collect data after training our model. In our current scenario, this is new data given to us by our avalanche-rescue charity's European arm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d041f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "# Load some dog statistics from our charity's European arm\n",
    "swiss_data = pandas.read_csv(\"dog-training-switzerland.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Show show information about the data\n",
    "print(f\"The Swiss dataset contains {swiss_data.shape[0]} samples\")\n",
    "\n",
    "plt.scatter(swiss_data['rescues_last_year'], swiss_data['weight_last_year'], alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"Rescues Last Year\")\n",
    "plt.ylabel(\"Weight Last Year\")\n",
    "plt.title(\"Swiss Dog Data: Rescues vs Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7ad06d2",
   "metadata": {},
   "source": [
    "This is clearly a much larger sample of data. Let's see how our models perform. Note that we aren't retraining the models here; we're simply using them to make predictions on a new dataset to assess how well they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our models against the swiss data\n",
    "features = swiss_data['weight_last_year']\n",
    "correct_answers = swiss_data['rescues_last_year']\n",
    "\n",
    "# We will now assess our models. Notice we're not training them again.\n",
    "# We are simply testing them against some new data \n",
    "\n",
    "# Assess the model trained on a 50:50 split\n",
    "predictions = model_5050.predict(features)\n",
    "MSE = mse(correct_answers, predictions)\n",
    "print('50:50 MSE = %f ' % MSE)\n",
    "\n",
    "# Assess the model trained on a 60:40 split\n",
    "predictions = model_6040.predict(features)\n",
    "MSE = mse(correct_answers, predictions)\n",
    "print('60:40 MSE = %f ' % MSE)\n",
    "\n",
    "# Assess the model trained on a 70:30 split\n",
    "predictions = model_7030.predict(features)\n",
    "MSE = mse(correct_answers, predictions)\n",
    "print('70:30 MSE = %f ' % MSE)\n",
    "\n",
    "# Assess the model trained on a 80:20 split\n",
    "predictions = model_8020.predict(features)\n",
    "MSE = mse(correct_answers, predictions)\n",
    "print('80:20 MSE = %f ' % MSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec696ddc",
   "metadata": {},
   "source": [
    "With this larger dataset, the model using to `80:20` split yielded the lowest error, and thus the best performance. There's also a clear pattern: the larger the training dataset, the better the model could perform after training.\n",
    "\n",
    "Together, this shows that we should try and evaluate different train/test splits when building machine learning models, and that _generally_ splits that favor the _train_ set with more data will yield better results. \n",
    " \n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned the following concepts:\n",
    "\n",
    " - You can use different *ratios* when splitting your dataset into *train* and *test* sets.\n",
    " - Different ratios yield different distributions of variables in the resulting *train* and *test* sets.\n",
    " - When the train:test ratios are close, you're possibly leaving a lot of data out of the __training__ set, and that can have a negative impact on your model's performance.\n",
    " - When building models, it's important to test them using different train/test splits. Simply assigning more data to the *train* set doesn't always guarantee the best results.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda-env-azureml_py38-py"
  },
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "conda-env-azureml_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
