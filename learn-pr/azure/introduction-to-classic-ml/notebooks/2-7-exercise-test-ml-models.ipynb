{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Gradient Descent\n",
    "\n",
    "Previously we identified trends in winter temperatures by fitting a linear regression model to weather data.\n",
    "\n",
    "Here, we'll repeat this process focussing on the optimizer. Specifically, we'll work with batch gradient descent and explore how changing the learning rate can alter its behaviour.\n",
    "\n",
    "The model we'll be working with will be the same linear regression model we have used in other units. The principles we learn, however, apply to much more complex models too.\n",
    "\n",
    "## Loading data and preparing our model\n",
    "\n",
    "Let's load up our weather data from Seattle, filter to January temperatures, and make slight adjustments so that the dates are mathematically interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas\n",
    "!pip install statsmodels\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/seattleWeather_1948-2017.csv\n",
    "import graphing # custom graphing code. See our GitHub repository\n",
    "\n",
    "# Load a file containing weather data for Seattle\n",
    "data = pandas.read_csv('seattleWeather_1948-2017.csv', parse_dates=['date'])\n",
    "\n",
    "# Remove all dates after July 1st because we have to to plant onions before summer begins\n",
    "data = data[[d.month == 1 for d in data.date]].copy()\n",
    "\n",
    "\n",
    "# Convert the dates into numbers so we can use it in our models\n",
    "# We make a year column which can contain fractions. For example\n",
    "# 1948.5 is half way through the year 1948\n",
    "data[\"year\"] = [(d.year + d.timetuple().tm_yday / 365.25) for d in data.date]\n",
    "\n",
    "# Let's take a quick look at our data\n",
    "print(\"Visual Check:\")\n",
    "graphing.scatter_2D(data, \n",
    "                    label_x=\"year\", \n",
    "                    label_y=\"min_temperature\",\n",
    "                    title=\"Temperatures over time (°F)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a model automatically\n",
    "\n",
    "Let's fit a line to this data well using an existing library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Perform linear regression to fit a line to our data\n",
    "# NB OLS uses the sum or mean of squared differences as a cost function\n",
    "# which we're familiar with from our last exercise \n",
    "model = smf.ols(formula = \"min_temperature ~ year\", data = data).fit()\n",
    "\n",
    "# Print the model\n",
    "intercept = model.params[0]\n",
    "slope = model.params[1]\n",
    "\n",
    "print(f\"The model is: y = {slope:0.3f} * X + {intercept:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooh, some math! Don't let that bother you. It's quite common for labels and features to be referred to as `y` and `X`, respectively.\n",
    "Here:\n",
    "* `y` is temperature (°F)\n",
    "* `X` is year\n",
    "* -83 is a _model parameter_ that acts as the line offset  \n",
    "* 0.063 is a _model parameter_ that defines the line slope (in °F per year)\n",
    "\n",
    "So this little equation states that the model estimates temperature by multiplying the year by `0.063`, then subtracting `83`.\n",
    "\n",
    "How did the library calculate these values? Let's go through the process.\n",
    "\n",
    "## Model Selection\n",
    "\n",
    "The first step is always selecting a model. Let's re-use the model we used in previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Creates a new MyModel\n",
    "        '''\n",
    "        # Straight lines described by two parameters:\n",
    "        # The slop is the angle of the line\n",
    "        self.slope = 0\n",
    "        # The intercept moves the line up or down\n",
    "        self.intercept = 0\n",
    "\n",
    "    def predict(self, date):\n",
    "        '''\n",
    "        Estimates the temperature from the date\n",
    "        '''\n",
    "        return date * self.slope + self.intercept\n",
    "\n",
    "    def get_summary(self):\n",
    "        '''\n",
    "        Returns a string that summarises the model\n",
    "        '''\n",
    "        return f\"y = {self.slope} * x + {self.intercept}\"\n",
    "\n",
    "print(\"Model class ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting our model with gradient descent\n",
    "\n",
    "The automatic method used the _ordinary least squares_ (OLS) method, which is the standard way to fit a line. OLS uses the mean (or sum) of square differences as a cost function (recall our experimentation with the sum of squared differences in the last exercise). Let's replicate the above line fitting, and break down each step so we can watch it in action.\n",
    "\n",
    "Recall that for each iteration, our training conducts three steps: \n",
    "\n",
    "1) Estimation of `y` (temperature) from `X` (year)\n",
    "\n",
    "2) Calculation of the cost function and its slope\n",
    "\n",
    "3) Adjustment of our model according to this slope\n",
    "\n",
    "Let's implement this now to watch it in action. Note that, **to keep things simple, we'll focus on estimating one parameter (line slope) for now**.\n",
    "\n",
    "### Visualising the error function\n",
    "\n",
    "First, lets look at the error function for this data. Normally we don't know this in advance, but for learning purposes, let's calculate it now for different potential models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = data.year\n",
    "temperature_true = data.min_temperature\n",
    "\n",
    "# we'll use a pre-built method to show a 3D plot\n",
    "# This requires a range of x values, a range of y values,\n",
    "# and a way to calculate z\n",
    "# Here, we set:\n",
    "#   x to a range of potential model intercepts\n",
    "#   y to a range of potential model slopes\n",
    "#   z is the cost for that combination of model parameters   \n",
    "\n",
    "# Choose a range of intercepts and slopes values\n",
    "intercepts = np.linspace(-100,-70,10)\n",
    "slopes = np.linspace(0.060,0.07,10)\n",
    "\n",
    "\n",
    "# Set a cost function. This will be the mean of squared differences\n",
    "def cost_function(temperature_estimate):\n",
    "    \"\"\"\n",
    "    Calculates cost for a given temperature estimate\n",
    "    Our cost function is the mean of squared differences (aka mean squared error)\n",
    "    \"\"\"\n",
    "    # Note that with numpy to square each value we use **\n",
    "    return np.mean((temperature_true - temperature_estimate) ** 2)\n",
    "\n",
    "def predict_and_calc_cost(intercept, slope):\n",
    "    '''\n",
    "    Uses the model to make a prediction, then calculates the cost \n",
    "    '''\n",
    "\n",
    "    # Predict temperature using these model parameters\n",
    "    temperature_estimate = x * slope + intercept\n",
    "\n",
    "    # Calculate cost\n",
    "    return cost_function(temperature_estimate)\n",
    "\n",
    "# Call the graphing method. This will use our cost function\n",
    "# which is above. If you would like to view this code in detail\n",
    "# then see this project's GitHub repository\n",
    "graphing.surface(x_values=intercepts, \n",
    "                y_values=slopes, \n",
    "                calc_z=predict_and_calc_cost, \n",
    "                title=\"Cost for Different Model Parameters\",\n",
    "                axis_title_x=\"Model intercept\",\n",
    "                axis_title_y=\"Model slope\",\n",
    "                axis_title_z=\"Cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above is interactive. Try clicking and dragging the mouse to rotate it.\n",
    "\n",
    "Notice how cost changes with both intercept and with line slope. This is because our model has a slope and an intercept, which both will affect how well the line fits the data. A consequence is that the gradient of the cost function must also be described by two numbers - one for intercept, and one for line slope.\n",
    "\n",
    "Our lowest point on the graph is the location of the best line equation for our data: a slope of 0.063 and intercept of -83. Let's try to train a model to find this point.\n",
    "\n",
    "### Implementing Gradient Descent\n",
    "\n",
    "To implement gradient descent, we need a method that can calculate the gradient of the above curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(temperature_estimate):\n",
    "    \"\"\"\n",
    "    This calculates gradient for a linear regession \n",
    "    using the Mean Squared Error cost function\n",
    "    \"\"\"\n",
    "\n",
    "    # The partial derivatives of MSE are as follows\n",
    "    # You don't need to be able to do this just yet but\n",
    "    # it's important to note these give you the two gradients\n",
    "    # that we need to train our model\n",
    "    error = temperature_estimate - temperature_true\n",
    "    grad_intercept = np.mean(error) * 2\n",
    "    grad_slope = (x * error).mean() * 2\n",
    "\n",
    "    return grad_intercept, grad_slope\n",
    "\n",
    "print(\"Function is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we need is a starting guess, and a loop that will update this guess with each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent(learning_rate, number_of_iterations):\n",
    "    \"\"\"\n",
    "    Performs gradient descent for a one-variable function. \n",
    "\n",
    "    learning_rate: Larger numbers follow the gradient more aggressively\n",
    "    number_of_iterations: The maximum number of iterations to perform\n",
    "    \"\"\"\n",
    "\n",
    "    # Our starting guess is y = 0 * x - 83\n",
    "    # we're going to start with the correct intercept so that \n",
    "    # only the line's slope is estimated. This is just to keep\n",
    "    # things simple for this exercise\n",
    "    model = MyModel()\n",
    "    model.intercept = -83\n",
    "    model.slope = 0\n",
    "\n",
    "    for i in range(number_of_iterations):\n",
    "        # Calculate the predicted values\n",
    "        predicted_temperature = model.predict(x)\n",
    "\n",
    "        # == OPTIMISER ===\n",
    "        # Calculate the gradient\n",
    "        _, grad_slope = calculate_gradient(predicted_temperature)\n",
    "        # Update the estimation of the line\n",
    "        model.slope -= learning_rate * grad_slope\n",
    "\n",
    "        # Print the current estimation and cost every 100 iterations\n",
    "        if( i % 100 == 0):\n",
    "            estimate = model.predict(x)\n",
    "            cost = cost_function(estimate)\n",
    "            print(\"Next estimate:\", model.get_summary(), f\"Cost: {cost}\")\n",
    "\n",
    "    # Print the final model\n",
    "    print(f\"Final estimate:\", model.get_summary())\n",
    "\n",
    "# Run gradient descent\n",
    "gradient_descent(learning_rate=1E-9, number_of_iterations=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model found the correct answer, but it took a number of steps. Looking at the print out we can see how it progressively stepped toward the correct solution.\n",
    "\n",
    "Now, what happens if we make the learning rate faster? This means taking larger steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(learning_rate=1E-8, number_of_iterations=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears to have found the solution faster. If we increase the rate even faster, however, things don't go so well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(learning_rate=5E-7, number_of_iterations=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the cost is getting worse each time.\n",
    "\n",
    "This is because the steps it was taking were too large. While it would step towards the correct solution, it would step too far and actually get worse with each attempt.\n",
    "\n",
    "For each model, there is an ideal learning rate. This is something that requires experimentation.\n",
    "\n",
    "## Fitting multiple variables simultaneously\n",
    "\n",
    "We've just fit one variable here to keep things simple. Expanding this to fit multiple variables requires only a few small code changes:\n",
    "\n",
    "1) We need to update more than one variable in the gradient descent loop\n",
    "\n",
    "2) We need to do some pre-processing of the data, which we alluded to in an earlier exercise. we'll cover how and why in later learning material. \n",
    "\n",
    "## Summary\n",
    "\n",
    "Well done! In this unit we have:\n",
    "\n",
    "1) watched gradient descent in action\n",
    "\n",
    "2) seen how changing the learning rate can improve a model's training speed\n",
    "\n",
    "3) learned that changing the learning rate can also result in unstable models \n",
    "\n",
    "You might have noticed that where the cost function stopped and the optimiser began became a little blurred here. Don't let that bother you. This is happens commonly, simply because, while they are conceptually separate, their mathematics sometimes can become intertwined."
   ]
  }
 ],
"metadata": {
  "kernelspec": {
    "name": "conda-env-py37_default-py",
    "language": "python",
    "display_name": "py37_default"
  },
  "language_info": {
    "name": "python",
    "version": "3.7.9",
    "mimetype": "text/x-python",
    "codemirror_mode": {
      "name": "ipython",
      "version": 3
    },
    "pygments_lexer": "ipython3",
    "nbconvert_exporter": "python",
    "file_extension": ".py"
  },
  "kernel_info": {
    "name": "conda-env-py37_default-py"
  },
  "nteract": {
    "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
