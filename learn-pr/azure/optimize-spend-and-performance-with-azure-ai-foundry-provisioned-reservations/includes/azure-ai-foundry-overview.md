Azure AI Foundry Models (formerly known Azure AI model inference) provides access to the most powerful models available in the industry. The models come from Microsoft, OpenAI, DeepSeek, Hugging Face, Meta, and more. These models can be integrated with software solutions to deliver a wide range of tasks that include content generation, summarization, image understanding, semantic search, and code generation.

AI Foundry Models provides a way to **consume models as APIs without hosting them on your infrastructure**. Models are hosted in a Microsoft-managed infrastructure, which enables API-based access to the model provider's model. API-based access can dramatically reduce the cost of accessing a model and simplify the provisioning experience.

AI Foundry Models is part of Azure AI Foundry, and users can access the service through [REST APIs](/rest/api/aifoundry/modelinference/), [SDKs in several languages](/azure/ai-foundry/foundry-models/supported-languages) such as Python, C#, JavaScript, and Java. You can also use AI Foundry Models from Azure AI Foundry by [configuring a connection](/azure/ai-foundry/foundry-models/how-to/configure-project-connection?pivots=ai-foundry-portal).

## How to start with Azure AI Foundry Models

Similar to other Azure products, you can start with Azure AI Foundry by creating a resource or an instance of the service in your Azure subscription. After you create an Azure AI Foundry resource, you must deploy a model before you can start making API calls and generating text. You can do this by using deployment APIs, which allow you to specify the model you wish to use. In the Azure AI Foundry, you can build AI models and deploy them for public consumption in software applications.

## Summary

In this unit, you learned about Azure AI Foundry Models, how it works, and the main characteristics and features of this service.
