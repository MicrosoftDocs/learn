### YamlMime:ModuleUnit
uid: learn.azure.introduction-azure-hpc-cache.knowledge-check
title: Knowledge check
metadata:
  unitType: knowledge_check
  title: Knowledge check
  description: A knowledge check that reviews the core concepts of the module.
  ms.date: 06/21/2021
  author: ronhogue
  ms.author: rohogue
  ms.topic: interactive-tutorial
  ms.service: azure
durationInMinutes: 2
content: |
quiz:
  questions:
  - content: "Which file sharing protocol is ideal for Azure HPC Cache to accelerate?"
    choices:
    - content: "NFS"
      isCorrect: true
      explanation: "Correct. Azure HPC Cache was designed to accelerate NFS workloads."
    - content: "NTFS"
      isCorrect: false
      explanation: "NTFS isn't a supported protocol for Azure HPC Cache."
    - content: "SFTP"
      isCorrect: false
      explanation: "SFTP isn't a supported protocol for Azure HPC Cache."
  - content: "Ideal workloads for Azure HPC Cache are heavy in which operation?"
    choices:
    - content: "Directory operations (listing folder content)"
      isCorrect: false
      explanation: "Directory operations aren't ideal workloads for Azure HPC Cache."
    - content: "Read operations (requesting file content)"
      isCorrect: true
      explanation: "Correct. Read operations are ideal workloads for Azure HPC Cache."
    - content: "Write operations (creating or modifying content)"
      isCorrect: false
      explanation: "Write operations aren't ideal workloads for Azure HPC Cache."
  - content: "Azure HPC Cache is designed to run best when the data is what size?"
    choices:
    - content: "Kilobyte files in a megabyte working set"
      isCorrect: false
      explanation: "Smaller files and datasets aren't ideal for Azure HPC Cache."
    - content: "Megabyte files in a gigabyte working set"
      isCorrect: false
      explanation: "Azure HPC Cache can accelerate these workloads, but they aren't typically cost effective."
    - content: "Gigabyte files in a terabyte working set"
      isCorrect: true
      explanation: "Correct. Gigabyte-sized files are ideal in a working set that's terabytes in size."
  - content: "Which of the following reasons best illustrates why Azure HPC Cache isn't a good fit for accelerating users' personal data files?"
    choices:
    - content: "Personal data files aren't shared with others because they're private."
      isCorrect: false
      explanation: "Personal data files might or might not be private. Privacy status doesn't affect how the cache works."
    - content: "A single client accesses personal data files."
      isCorrect: true
      explanation: "Correct. A cache is most efficient when several clients are accessing it at the same time."
    - content: "Personal data files aren't sensitive and don't need to be encrypted."
      isCorrect: false
      explanation: "All files are encrypted in Azure HPC Cache. File sensitivity and encryption don't make the cache a good fit or bad fit."
  - content: "Suppose a genetic research company is considering Azure HPC Cache for a workload that compares gigabyte DNA files to the same reference DNA file. The company uses hundreds of cloud clients running an NFS workload. What would make this good fit even better?"
    choices:
    - content: "If the company installed a load balancer to distribute the workload"
      isCorrect: false
      explanation: "A load balancer wouldn't affect Azure HPC Cache."
    - content: "If the company installed a proxy server to handle network traffic"
      isCorrect: false
      explanation: "Proxy servers wouldn't affect Azure HPC Cache."
    - content: "If the company installed Azure ExpressRoute to maximize bandwidth"
      isCorrect: true
      explanation: "Correct. ExpressRoute maximizes bandwidth between the company's clients in the cloud and it's data on-premises."