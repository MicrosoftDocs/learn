{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70d0b5f9",
   "metadata": {},
   "source": [
    "# Exercise: More metrics derived from confusion matrices\n",
    "\n",
    "In this exercise we will learn about different metrics, using them to explain the results obtained from the *binary classification model* we built in the previous unit.\n",
    "\n",
    "## Data visualization\n",
    "\n",
    "We will use the dataset with different classes of objects found on the mountain one more time:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec3f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects.csv\n",
    "\n",
    "#Import the data from the .csv file\n",
    "dataset = pandas.read_csv('snow_objects.csv', delimiter=\"\\t\")\n",
    "\n",
    "#Let's have a look at the data\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f314baf6",
   "metadata": {},
   "source": [
    "Recall that to use the dataset above for *binary classification*, we need to add another column to the dataset, and set it to `True` where the original label is `hiker`, and `False` where it's not.\n",
    "\n",
    "Let's then add that label, split the dataset and train the model again:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add a new label with true/false values to our dataset\n",
    "dataset[\"is_hiker\"] = dataset.label == \"hiker\"\n",
    "\n",
    "# Split the dataset in an 70/30 train/test ratio. \n",
    "train, test = train_test_split(dataset, test_size=0.3, random_state=1, shuffle=True)\n",
    "\n",
    "# define a random forest model\n",
    "model = RandomForestClassifier(n_estimators=1, random_state=1, verbose=False)\n",
    "\n",
    "# Define which features are to be used \n",
    "features = [\"size\", \"roughness\", \"motion\"]\n",
    "\n",
    "# Train the model using the binary label\n",
    "model.fit(train[features], train.is_hiker)\n",
    "\n",
    "print(\"Model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb18e1ae",
   "metadata": {},
   "source": [
    "We can now use this model to predict whether objects in the snow are hikers or not.\n",
    "\n",
    "Let's plot its *confusion matrix*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn has a very convenient utility to build confusion matrices\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Calculate the model's accuracy on the TEST set\n",
    "actual = test.is_hiker\n",
    "predictions = model.predict(test[features])\n",
    "\n",
    "# Build and print our confusion matrix, using the actual values and predictions \n",
    "# from the test set, calculated in previous cells\n",
    "cm = confusion_matrix(actual, predictions, normalize=None)\n",
    "\n",
    "# Create the list of unique labels in the test set, to use in our plot\n",
    "# I.e., ['True', 'False',]\n",
    "unique_targets = sorted(list(test[\"is_hiker\"].unique()))\n",
    "\n",
    "# Convert values to lower case so the plot code can count the outcomes\n",
    "x = y = [str(s).lower() for s in unique_targets]\n",
    "\n",
    "# Plot the matrix above as a heatmap with annotations (values) in its cells\n",
    "fig = ff.create_annotated_heatmap(cm, x, y)\n",
    "\n",
    "# Set titles and ordering\n",
    "fig.update_layout(  title_text=\"<b>Confusion matrix</b>\", \n",
    "                    yaxis = dict(categoryorder = \"category descending\"))\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=0.5,\n",
    "                        y=-0.15,\n",
    "                        showarrow=False,\n",
    "                        text=\"Predicted label\",\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=-0.15,\n",
    "                        y=0.5,\n",
    "                        showarrow=False,\n",
    "                        text=\"Actual label\",\n",
    "                        textangle=-90,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "# We need margins so the titles fit\n",
    "fig.update_layout(margin=dict(t=80, r=20, l=120, b=50))\n",
    "fig['data'][0]['showscale'] = True\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also calculate some values that will be used throughout this exercise\n",
    "# We already have actual values and corresponding predictions, defined above\n",
    "correct = actual == predictions\n",
    "tp = numpy.sum(correct & actual)\n",
    "tn = numpy.sum(correct & numpy.logical_not(actual))\n",
    "fp = numpy.sum(numpy.logical_not(correct) & actual)\n",
    "fn = numpy.sum(numpy.logical_not(correct) & numpy.logical_not(actual))\n",
    "\n",
    "print(\"TP - True Positives: \", tp)\n",
    "print(\"TN - True Negatives: \", tn)\n",
    "print(\"FP - False positives: \", fp)\n",
    "print(\"FN - False negatives: \", fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727351a",
   "metadata": {},
   "source": [
    "We can use the values and matrix above to help us understand other metrics.\n",
    "\n",
    "\n",
    "## Calculating metrics\n",
    "\n",
    "From here on we will take a closer look at each at the following metrics, how they are calculated and how they can help explain our current model. \n",
    "\n",
    "* Accuracy\n",
    "* Sensitivity/Recall\n",
    "* Specificity\n",
    "* Precision\n",
    "* False positive rate\n",
    "\n",
    "Let's first recall some useful terms:\n",
    "\n",
    "* TP = True positives: a positive label is correctly predicted\n",
    "* TN = True negatives: a negative label is correctly predicted\n",
    "* FP = False positives: a negative label is predicted as a positive\n",
    "* FN = False negatives: a positive label is predicted as a negative\n",
    "\n",
    "\n",
    "### Accuracy\n",
    "Accuracy is the number of correct predictions divided by the total number of predictions:\n",
    "\n",
    "```\n",
    "    accuracy = (TP+TN) / number of samples\n",
    "```\n",
    "\n",
    "It's possibly the most basic metric used but, as we've seen, it's not the most reliable when *imbalanced datasets* are used.\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982af0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "# len(actual) is the number of samples in the set that generated TP and TN\n",
    "accuracy = (tp+tn) / len(actual) \n",
    "\n",
    "# print result as a percentage\n",
    "print(f\"Model accuracy is {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bfcdb3",
   "metadata": {},
   "source": [
    "### Sensitivity/Recall\n",
    "\n",
    "*Sensitivity* and *Recall* are interchangeable names for the same metric, which expresses the fraction of samples __correctly__ predicted by a model:\n",
    "\n",
    "\n",
    "```\n",
    "    sensitivity = recall = TP / (TP + FN)\n",
    "```\n",
    "\n",
    "This is an important metric, that tells us how out of all the *actually* __positive__ samples, how many are __correctly__ predicted as positive.\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3242b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for sensitivity/recall\n",
    "sensitivity = recall = tp / (tp + fn)\n",
    "\n",
    "# print result as a percentage\n",
    "print(f\"Model sensitivity/recall is {sensitivity:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b92514",
   "metadata": {},
   "source": [
    "### Specificity\n",
    "Specificity expresses the fraction of __negative__ labels correctly predicted over the total number of existing negative samples:\n",
    "\n",
    "```\n",
    "    specificity = TN / (TN + FP)\n",
    "```\n",
    "\n",
    "Tells us how out of all the *actually* __negative__ samples, how many are __correctly__ predicted as negative\n",
    "\n",
    "It can be calculated using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0698213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for specificity\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# print result as a percentage\n",
    "print(f\"Model specificity is {specificity:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fffd4b",
   "metadata": {},
   "source": [
    "### Precision\n",
    "Precision expresses the proportion of __correctly__ predicted positive samples over all positive predictions:\n",
    "\n",
    "```\n",
    "    precision = TP / (TP + FP)\n",
    "```\n",
    "In other words, it indicates how out of all positive predictions, how many are truly positive labels.\n",
    "\n",
    "It can be calculated using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07eba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for precision\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "\n",
    "# print result as a percentage\n",
    "print(f\"Model precision is {precision:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ddf306",
   "metadata": {},
   "source": [
    "### False positive rate\n",
    "False positive rate or FPR, is the number of __incorrect__ positive predictions divided by the total number of negative samples:\n",
    "\n",
    "```\n",
    "    false_positive_rate = FP / (FP + TN)\n",
    "```\n",
    "\n",
    "Out of all the actual negatives, how many were misclassified as positives?\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46b42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for false positive rate\n",
    "false_positive_rate = fp / (fp + tn)\n",
    "\n",
    "# print result as a percentage\n",
    "print(f\"Model false positive rate is {false_positive_rate:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eef1b4",
   "metadata": {},
   "source": [
    "Notice that the sum of `specificity` and `false positive rate` should always be equal to `1`.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "There are several different metrics that can help us evaluate the performance of a model, in the context of the quality of its predictions.\n",
    "\n",
    "The choice of the most adequate metrics, however, is primarily a funciton of the data and the problem we are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e1d6e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We covered the following topics in this unit:\n",
    "\n",
    "* How to calculate the very basic measurements used in the evaluation of classification models: TP, FP, TN, FN.\n",
    "* How to use the measurement aboves to calculate more meaningful metrics, such as:\n",
    "    * Accuracy\n",
    "    * Sensitivity/Recall\n",
    "    * Specificity\n",
    "    * Precision\n",
    "    * False positive rate\n",
    "* How the choice of metrics depends on the dataset and the problem we are trying to solve.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda-env-azureml_py38-py"
  },
  "kernelspec": {
   "display_name": "py38_default",
   "language": "python",
   "name": "conda-env-azureml_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
