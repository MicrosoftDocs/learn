### YamlMime:ModuleUnit
uid: learn.wwl.perform-model-selection-with-hyperparameter-tuning.knowledge-check
title: Knowledge check
metadata:
  title: Knowledge check
  description: "Knowledge check"
  ms.date: 05/22/2020
  author: b-dstrod
  ms.author: b-dstrodtman
  ms.topic: interactive-tutorial
  ms.prod: learning-azure
azureSandbox: false
durationInMinutes: 5
quiz:
  questions:
  - content: "What is the likely cause when a supervised model makes predictions perfectly against all the training data, but fails against new data?"
    choices:
    - content: "Overfitting"
      isCorrect: true
      explanation: "Overfitting occurs when the model has effectively memorized the answers for the data it has seen, but has not \"learned\" to make predictions against unseen data."
    - content: "Underfitting"
      isCorrect: false
      explanation: "Underfitting would result in bad performance against both training and unseen validation data."
    - content: "Regularization"
      isCorrect: false
      explanation: "Regularization is a technique in machine learning to avoid overfitting."
  - content: "Which of the following statements best describes how the k-fold cross-validation splits the training data into the various folds?"
    choices:
    - content: "In k-fold cross-validation, the validation subset is always the same across all folds."
      isCorrect: false
      explanation: "In k-fold cross-validation, the original training dataset is randomly divided into k folds or groups of approximately equal size. In each of the k training passes, k-1 folds are used as training subset, whereas 1-fold is used as the validation subset such that each fold is used only once as a validation subset across all k training passes."
    - content: "In k-fold cross-validation, the training and validation subsets are created with random sampling of the training data with replacement."
      isCorrect: false
      explanation: "In k-fold cross-validation, the original training dataset is randomly divided into k folds or groups of approximately equal size. In each of the k training passes, k-1 folds are used as training subset, whereas 1-fold is used as the validation subset such that each fold is used only once as a validation subset across all k training passes."
    - content: "In k-fold cross-validation, each observation from the original training dataset has a chance to appear in both the training subset and the validation subset."
      isCorrect: true
      explanation: "In k-fold cross-validation, the original training dataset is randomly divided into k folds or groups of approximately equal size. In each of the k training passes, k-1 folds are used as training subset, whereas 1-fold is used as the validation subset such that each fold is used only once as a validation subset across all k training passes."
  - content: "Which of the following is always true when using k-fold cross-validation?"
    choices:
    - content: "In k-fold cross-validation, the best performing fold is the selected model pipeline."
      isCorrect: false
      explanation: "The best performing fold only identifies the winning pipeline. You discard all the trained models during the k-fold procedure and train the winning pipeline with all available training data to produce the selected model pipeline."
    - content: "In k-fold cross-validation, the best performing fold only identifies the winning pipeline."
      isCorrect: true
      explanation: "The best performing fold only identifies the winning pipeline. You discard all the trained models during the k-fold procedure and train the winning pipeline with all available training data to produce the selected model pipeline."
    - content: "The k-fold cross-validation technique is used to fine-tune the machine learning algorithmâ€™s hyperparameters."
      isCorrect: false
      explanation: "While the k-fold cross-validation technique is often used for hyperparameter tuning, it can be also used for selecting the preprocessing pipelines, and machine learning algorithms."