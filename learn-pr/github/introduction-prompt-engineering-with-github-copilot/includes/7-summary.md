In this module, we unveiled the intricacies of optimizing GitHub Copilot through effective prompting. Harnessing the tool's maximum potential lies in the art and science of prompt engineering. Now, you're equipped with refined skills and insights to elevate your coding experience and output.
With the completion of this module, you have learned:

Prompt engineering principles, best practices, and how GitHub Copilot learns from your prompts to provide context-aware responses.
The underlying flow of how GitHub Copilot processes user prompts to generate responses or code suggestions.
The data flow for code suggestions and chat in GitHub Copilot.
LLMs (Large Language Models) and their role in GitHub Copilot and prompting.
How to craft effective prompts that optimize GitHub Copilot's performance, ensuring precision and relevance in every code suggestion.
The intricate relationship between prompts and Copilot's responses.
How Copilot handles data from prompts in different situations, including secure transmission and content filtering.


## References

- [Inside GitHub: Working with the Large Language Models (LLMs) behind GitHub Copilot - The GitHub Blog](https://github.blog/2023-05-17-inside-github-working-with-the-llms-behind-github-copilot/)
- [How to use GitHub Copilot: Prompts, tips, and use cases - The GitHub Blog](https://github.blog/2023-06-20-how-to-write-better-prompts-for-github-copilot/)
- [How GitHub copilot handles data](https://resources.github.com/learn/pathways/copilot/essentials/how-github-copilot-handles-data/)

## Provide feedback

[Submit content feedback or suggested changes](https://github.com/githubpartners/microsoft-learn/issues/new/choose) for this Microsoft Learn module. GitHub maintains the content and a team member will triage the request. Thank you for taking the time to improve our content!

